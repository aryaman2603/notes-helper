[
  {
    "id": 0,
    "source": "Module 1.txt",
    "text": "Operating System BCSE303L Module 1 Dr. Naveenkumar Jayakumar Associate Professor Department of Computational Intelligence PRP 217-4 naveenkumar.jk@vit.ac.in Operating System - Definition \u2751 An Operating System (OS) is system software that acts as an intermediary between computer hardware and the user. \u2751 It manages hardware resources. \u2751 It provides an environment in which application programs can run efficiently. The OS makes the computer usable by controlling the hardware and providing services for programs. Operating System - Definition Aspect 1- Resource Aspect 2 - Providing Services Management The OS offers common The OS allocates and functionalities like file controls the use of management, security, and hardware components like a user interface (UI) for memory, storage, and applications to run processors. smoothly. Operating System - Definition It is the first program loaded by a computer at startup (boot loader loads the OS kernel). It provides an interface between users and computer hardware. It acts as a control program that prevents misuse of hardware resources. The OS is a resource allocator, deciding which program gets which resource and for how long. Operating System - Goals \u2022 Make interaction with the computer user- Convenience friendly. \u2022 Provide GUIs, virtual desktops, file explorers. \u2022 Optimize system performance \u2014 maximize CPU, disk, and I/O utilization. Efficiency \u2022 Minimize response time, turnaround time, waiting time. \u2022 Modular design so parts can be upgraded. Ability to Evolve \u2022 Support new hardware easily. Operating System - Goals \u2022 Ensure fair resource allocation among Fairness multiple users/programs. Robustness & \u2022 Keep the system stable even under Reliability unexpected failures. \u2022 Handle growth in number of users, tasks, Scalability or resources. Operating System - Functionality Process Memory File System Management Management Management Device Security and Networking Management Protection Error Command Detection User Interface Interpretation and Recovery Operating System - Functionality Process Management \u2751 Create, schedule, suspend, and terminate processes. \u2751 Context switching between processes. \u2751 Synchronization and communication between processes (e.g., semaphores, pipes). Memory Management \u2751 Keeps track of each byte of memory. \u2751 Allocates memory to processes when they need it and deallocates when done. \u2751 Implements techniques like paging, segmentation, and virtual memory. \u2751 Handles swapping between main memory and disk. Operating System - Functionality File System Management \u2751Creates, deletes, reads, writes files/directories. \u2751Manages permissions and access control lists (ACLs). \u2751Maintains file metadata (size, timestamps, permissions). Device Management \u2751Manages input/output devices using device drivers. \u2751Handles buffering, spooling, and caching of data. \u2751Provides uniform interface to devices (device independence). Operating System - Functionality Security and Protection \u2751User authentication (login, passwords, biometrics). \u2751Authorization and access control. \u2751Protects user data and system resources from malware and unauthorized access. Networking \u2751Manages network connections (TCP/IP stack). \u2751Supports distributed computing and network file systems. \u2751Provides protocols for secure data exchange. Operating System - Functionality Command Interpretation \u2751Shell interprets user commands and executes them. \u2751Provides scripting capabilities for automation. Error Detection and Recovery \u2751Monitors hardware for failures. \u2751Reports errors and takes corrective actions. \u2751Logs system events for diagnostics. Operating System \u2013 What it Does? Process Memory File"
  },
  {
    "id": 1,
    "source": "Module 1.txt",
    "text": "data. \u2751Provides uniform interface to devices (device independence). Operating System - Functionality Security and Protection \u2751User authentication (login, passwords, biometrics). \u2751Authorization and access control. \u2751Protects user data and system resources from malware and unauthorized access. Networking \u2751Manages network connections (TCP/IP stack). \u2751Supports distributed computing and network file systems. \u2751Provides protocols for secure data exchange. Operating System - Functionality Command Interpretation \u2751Shell interprets user commands and executes them. \u2751Provides scripting capabilities for automation. Error Detection and Recovery \u2751Monitors hardware for failures. \u2751Reports errors and takes corrective actions. \u2751Logs system events for diagnostics. Operating System \u2013 What it Does? Process Memory File System Device Management Management Management Management \u2022 Process \u2022 Allocation \u2022 File \u2022 Device Scheduling and Organization Drivers Deallocation \u2022 Process \u2022 Access \u2022 I/O Creation and \u2022 Paging and Control Operations Termination Segmentatio \u2022 File n \u2022 Concurrency Operations and \u2022 Virtual Synchronizat Memory ion Operating System \u2013 What it Does? Security and Access User Interface Networking Control \u2022 Authentication \u2022 Graphical user \u2022 Network interface (GUI) Communication \u2022 Authorization \u2022 Command Line \u2022 Resource Sharing \u2022 Encryption Interface (CLI) Operating System \u2013 What it Does? System Performance Error Handling and Multitasking and and Monitoring Recovery Multithreading \u2022 Performance \u2022 Error Detection \u2022 Multitasking Optimization \u2022 Recovery \u2022 Multithreading \u2022 Monitoring Tools Mechanisms Operating System \u2013 Design Issues An operating system's The OS must decide which of the many running processes gets to use the CPU and for design is a complex how long process that involves addressing a multitude of The operating system is responsible for allocating and deallocating memory space to challenges and processes. It must keep track of which parts of memory are currently being used and by making critical trade- whom, and it needs to ensure that processes do not interfere with each other's memory. offs to ensure a computer system is The OS manages communication with hardware devices such as disk drives, printers, and both usable and network adapters. It provides a consistent interface for applications to interact with these efficient. devices, hiding the complexities of the underlying hardware. This involves using device drivers to translate high-level requests into low-level device- specific commands. When multiple processes need to access shared resources, the OS must ensure that their operations are coordinated to avoid conflicts and maintain data consistency. Operating System \u2013 Design Issues An operating A deadlock is a situation where two or more processes are blocked forever, system's design is each waiting for a resource held by another. The OS must have strategies to a complex prevent, detect, and recover from deadlocks. process that involves The OS must define how files are stored on secondary storage devices, addressing a including their structure and naming conventions multitude of challenges and making critical trade-offs to The OS needs to enforce permissions to control which users and processes ensure a can access which files and what operations they can perform (read, write, computer system execute). is both usable and efficient. The file system must ensure that data"
  },
  {
    "id": 2,
    "source": "Module 1.txt",
    "text": "are blocked forever, system's design is each waiting for a resource held by another. The OS must have strategies to a complex prevent, detect, and recover from deadlocks. process that involves The OS must define how files are stored on secondary storage devices, addressing a including their structure and naming conventions multitude of challenges and making critical trade-offs to The OS needs to enforce permissions to control which users and processes ensure a can access which files and what operations they can perform (read, write, computer system execute). is both usable and efficient. The file system must ensure that data is not corrupted due to system crashes or hardware failures. Operating System \u2013 Design Issues An operating Verifying the identity of users before granting them access to the system. system's design is a complex process that involves Restricting the access of processes and users to system resources based on addressing a a defined policy. multitude of challenges and making critical trade-offs to Implementing mechanisms to defend against viruses, malware, and other ensure a security threats. computer system is both usable and efficient. An operating system should be designed to be efficient and provide good performance. This involves minimizing the overhead of the OS itself and making optimal use of the available hardware resources. Operating System \u2013 Abstract View Hardware Kernel SHELL & System Libraries and Utilities User Applications User Interface Operating System \u2013 Abstract View Hardware Layer: \u2022 CPU \u2022 Memory (RAM) \u2022 Storage (HDD/SSD) \u2022 I/O Devices Kernel Layer: \u2022 Process Management \u2022 Memory Management \u2022 Device Drivers \u2022 System Calls System Libraries and Utilities Layer: \u2022 Standard Libraries (e.g., C Standard Library) \u2022 Utility Programs (e.g., file managers, system monitors) User Applications Layer: \u2022 Application Software (e.g., browsers, office suites) User Interface Layer: \u2022 Graphical User Interface (GUI) \u2022 Command-Line Interface (CLI) Operating System \u2013 Structuring \u2022 Objective of Structuring a Operating system is to balance performance, maintainability, security, and flexibility Primary Structuring Monolithic Microkernel Layered Hybrid Modular Exokernel Structure Structure Systems Structure Loadable Kernel Modules Operating System \u2013 Structuring \u2022 The structuring of an Operating System (OS) is strongly related to how it divides its components and services between Kernel Space and User Space. \u2022 The privileged area of memory where the OS kernel runs. \u2022 Has full access to hardware resources. Kernel Space \u2022 Runs in privileged mode (Ring 0 in x86 architecture). \u2022 Handles critical tasks like CPU scheduling, interrupt handling, device drivers, memory management. \u2022 The area of memory where user applications and some system services run. \u2022 Runs in unprivileged mode (Ring 3 in x86). User Space \u2022 Has restricted access \u2014 must use system calls to request services from the kernel. \u2022 Examples: your web browser, word processor, or user-level services like print spoolers (in some architectures). Operating System \u2013 Monolithic Structure Monolithic Kernel \u2022 All OS services (process management, memory management, device drivers, file systems) run in a single, unified kernel space with full hardware access. \u2022 No separation between components;"
  },
  {
    "id": 3,
    "source": "Module 1.txt",
    "text": "tasks like CPU scheduling, interrupt handling, device drivers, memory management. \u2022 The area of memory where user applications and some system services run. \u2022 Runs in unprivileged mode (Ring 3 in x86). User Space \u2022 Has restricted access \u2014 must use system calls to request services from the kernel. \u2022 Examples: your web browser, word processor, or user-level services like print spoolers (in some architectures). Operating System \u2013 Monolithic Structure Monolithic Kernel \u2022 All OS services (process management, memory management, device drivers, file systems) run in a single, unified kernel space with full hardware access. \u2022 No separation between components; tightly integrated. \u2022 Direct hardware access for speed. \u2022 Examples: Traditional Unix systems (e.g., BSD, Linux in its core design). Advantages \u2022 High Performance : Minimal overhead due to direct communication between components. \u2022 Simple design for developers. Disadvantages : \u2022 Complexity : Large codebase makes debugging and updates challenging. \u2022 Instability : A bug in any component can crash the entire system. \u2022 Security risks due to lack of isolation. Operating System \u2013 Microkernel Structure Microkernel \u2022 Only essential services (IPC, scheduling, memory management) run in kernel space. Non-essential services (device drivers, file systems) operate in user space . Key Characteristics \u2022 Minimalist core with modular user-space services. \u2022 Examples: QNX, L4, Mach (basis for macOS/iOS). Advantages \u2022 Reliability : Failures in user-space services don\u2019t crash the kernel. \u2022 Security : Isolation reduces attack surfaces. \u2022 Easier to update components. Disadvantages \u2022 Performance Overhead : Frequent context switches between user/kernel modes. \u2022 Complex IPC mechanisms can slow down operations. Operating System \u2013 Modular Structure Modular Kernel \u2022 A hybrid approach where the core is monolithic but supports loadable kernel modules (LKMs) for extending functionality at runtime. Key Characteristics \u2022 Core services (e.g., process scheduling) are static. \u2022 Modules (e.g., drivers, filesystems) can be added/removed dynamically. \u2022 Example: Linux kernel. Advantages \u2022 Flexibility : Add/remove features without rebooting. \u2022 Balances performance (core services in kernel) and modularity. Disadvantages \u2022 Modules still run in kernel space; a faulty module can crash the system. \u2022 Slightly higher complexity than pure monolithic kernels. Operating System \u2013 Layered Structure Layered Kernel \u2022 OS components are organized into hierarchical layers , each providing abstracted services to the layer above. \u2022 Example: Hardware \u2192 Kernel \u2192 Device Drivers \u2192 File Systems \u2192 User Apps. Key Characteristics \u2022 Strict abstraction (e.g., lower layers handle hardware, upper layers handle applications). \u2022 Example: MINIX (educational OS by Andrew Tanenbaum). Advantages \u2022 Simplicity : Each layer has a well-defined role. \u2022 Easier debugging and testing due to isolation. Disadvantages \u2022 Latency : Layered communication adds overhead. \u2022 Reduced performance compared to monolithic designs. Operating System \u2013 Structuring Hybrid Kernel \u2022 Combines a microkernel core with monolithic elements . Critical services run in kernel space for speed, while others run in user space. Key Characteristics \u2022 Core services (e.g., hardware abstraction, threading) in kernel space. \u2022 Higher-level services (e.g., drivers) may run in user space or kernel space. \u2022 Examples: Windows"
  },
  {
    "id": 4,
    "source": "Module 1.txt",
    "text": "handle applications). \u2022 Example: MINIX (educational OS by Andrew Tanenbaum). Advantages \u2022 Simplicity : Each layer has a well-defined role. \u2022 Easier debugging and testing due to isolation. Disadvantages \u2022 Latency : Layered communication adds overhead. \u2022 Reduced performance compared to monolithic designs. Operating System \u2013 Structuring Hybrid Kernel \u2022 Combines a microkernel core with monolithic elements . Critical services run in kernel space for speed, while others run in user space. Key Characteristics \u2022 Core services (e.g., hardware abstraction, threading) in kernel space. \u2022 Higher-level services (e.g., drivers) may run in user space or kernel space. \u2022 Examples: Windows NT, macOS (XNU kernel), iOS. Advantages \u2022 Balanced Performance : Critical tasks are optimized; non-critical tasks are modular. \u2022 Improved stability over pure monolithic kernels. Disadvantages \u2022 Complexity in design and maintenance. \u2022 Less isolation than pure microkernels. Structuring Kernel - ? Kernel User Space Other User Notepad Paint Terminal Applications \u2751All OS services run in the same address space (kernel space) \u2751Everything is tightly integrated. \u2751 No separation between subsystems Kernel Space Process Memory Device File Systems System Calls Scheduling Management Drivers Structuring Kernel - ? Kernel User Space \u2751 Minimal kernel functionality in kernel space IDE User Device Drivers File Systems Environments Applications \u2751 Most services (like file systems, device drivers) run in user space. \u2751Only essential services are in the kernel; other services are outside communicating via IPC Kernel Space (Inter-Process Communication). Inter Process Memory Hardware Scheduling Communication Management Management Structuring Kernel - ? Kernel User Space \u2751 Combines aspects of monolithic and microkernel. Kernel Space \u2751 Core functions are in the kernel, but modules Memory Scheduler Manager can be loaded/unloaded dynamically. \u2026. \u2026.. \u2026.. File Device System Drivers Structuring Kernel - ? Kernel \u2751 The OS is divided into layers; each built on top of the lower layer. \u2751 Top layers depend on lower ones. \u2751 Each layer sits neatly above the previous, showing dependency from top to bottom. Structuring Kernel - ? Kernel User Space User \u2751Mix of microkernel and monolithic concepts. Optional OS Application User File System Services Device Applications \u2751Runs some services in user space, others in kernel Drivers space for performance. \u2751Looks similar to a microkernel, but with more services optionally included in the kernel for Kernel Space speed. Kernel Process Memory Some Basic Inter Process Scheduler Management Hardware Drivers Communication Operating System \u2013 Structuring Structure Type Key Trait Performance Stability Flexibility Example All services in Monolithic High Low Low Linux (core) kernel space Minimal core; Microkernel services in Low High High QNX user space Loadable modules + Linux Modular Medium-High Medium High monolithic (modules) core Strict Layered hierarchical Low Medium Low MINIX layers Microkernel core + Windows Hybrid Medium-High Medium-High Medium monolithic NT, macOS elements Abstraction in Kernels \u2751 Abstraction in Monolithic Kernel - Minimal \u2751All components (e.g., device drivers, file systems) run in the same address space (kernel space). \u2751 Applications interact with hardware through system calls , but the kernel itself doesn\u2019t abstract its internal components from each other."
  },
  {
    "id": 5,
    "source": "Module 1.txt",
    "text": "Example All services in Monolithic High Low Low Linux (core) kernel space Minimal core; Microkernel services in Low High High QNX user space Loadable modules + Linux Modular Medium-High Medium High monolithic (modules) core Strict Layered hierarchical Low Medium Low MINIX layers Microkernel core + Windows Hybrid Medium-High Medium-High Medium monolithic NT, macOS elements Abstraction in Kernels \u2751 Abstraction in Monolithic Kernel - Minimal \u2751All components (e.g., device drivers, file systems) run in the same address space (kernel space). \u2751 Applications interact with hardware through system calls , but the kernel itself doesn\u2019t abstract its internal components from each other. \u2751 Abstraction in Microkernel - High \u2751 Only core services (e.g., process scheduling, memory management) run in kernel space. \u2751 Non-essential services (e.g., device drivers, file systems) are abstracted into user-space servers that communicate via message passing . Abstraction in Kernels \u2751 Abstraction in Modular Kernel \u2013 Moderate \u2751 Core services (e.g., process management) are built into the kernel. \u2751 Optional components (e.g., drivers, file systems) are abstracted as loadable modules that can be added/removed dynamically. \u2751 Abstraction in Layered Kernel \u2013 High \u2751 The OS is split into hierarchical layers (e.g., hardware \u2192 CPU scheduling \u2192 memory management \u2192 file systems \u2192 user interface). \u2751 Each layer abstracts the layer below it, providing a simplified interface for the layer above. Abstraction in Kernels \u2751 Abstraction in Hybrid Kernel \u2013 Selective \u2751 Combines microkernel and monolithic ideas. Critical services (e.g., thread scheduling) run in kernel space, while others (e.g., drivers) may run in user space or kernel space. \u2751 Abstraction is applied selectively to balance performance and isolation. \u2751 Benefits of Abstraction \u2751Simplifies Complexity : Applications and developers don\u2019t need to manage hardware directly (e.g., a program just asks to \"read a file\" without knowing how the disk works). \u2751Improves Security : Isolating components (e.g., microkernel services) limits the damage from bugs or attacks. \u2751Enables Modularity : Abstraction allows features to be added/removed without rewriting the entire OS (e.g., Linux modules). \u2751Balances Trade-offs : Hybrid and layered kernels use abstraction to strike a balance between speed, stability, and flexibility. Abstraction in Kernels \u2751 Its primary role is to hide the complexity of hardware from the user and application programs. \u2751Instead of dealing with the intricate details of disk drives, memory chips, and CPU registers, we work with simpler, high-level representations. \u2751 For example, you interact with files and folders, not with the physical sectors and tracks on a storage drive. The OS abstracts the hardware details into this easy-to-use file system model. This makes software development drastically simpler and allows programs to be portable across different hardware configuration Process & Resources \u2751Processes \u2751A process is a program in execution. \u2751 It's the basic unit of work in an OS. \u2751 The OS's job is to manage numerous processes, switching between them rapidly to give the illusion of simultaneous execution (multitasking). \u2751 This management ensures that each process gets a fair share of the CPU and runs in isolation, preventing it"
  },
  {
    "id": 6,
    "source": "Module 1.txt",
    "text": "with the physical sectors and tracks on a storage drive. The OS abstracts the hardware details into this easy-to-use file system model. This makes software development drastically simpler and allows programs to be portable across different hardware configuration Process & Resources \u2751Processes \u2751A process is a program in execution. \u2751 It's the basic unit of work in an OS. \u2751 The OS's job is to manage numerous processes, switching between them rapidly to give the illusion of simultaneous execution (multitasking). \u2751 This management ensures that each process gets a fair share of the CPU and runs in isolation, preventing it from interfering with others. Process & Resources Processes System/Kernel Daemon User Process Process Process Process & Resources \u2751Resources \u2751 These processes need resources to do their work. \u2751Resources include: \u2751 CPU time: The processing power needed to execute instructions. \u2751 Memory: Space to store the program and its data. \u2751 I/O devices: Access to hardware like keyboards, displays, and network cards. \u2751 The OS acts as a resource manager, allocating these resources efficiently and fairly among all the competing processes. Process & Resources Physical Resources Virtual Resources Software Resources Abstract Resources \u2022 CPU (Central \u2022 Virtual Memory \u2022 Files and Directories \u2022 Processes (Threads, Processing Unit) (Swap Space or Page Tasks, or Jobs) \u2022 I/O Streams File) \u2022 Memory (Main (Input/Output \u2022 Threads (Lightweight Memory or RAM) \u2022 Virtual CPUs (in a Channels) Processes) virtualized \u2022 I/O Devices \u2022 Sockets (Network \u2022 Synchronization environment) (Keyboard, Mouse, Connections) Objects (Mutexes, Printer, etc.) Locks, etc.) \u2022 Semaphores \u2022 Storage Devices (Synchronization (Hard Disk, Solid Mechanisms) State Drive, etc.) Process & Resources \u2022 The Operating System manages and performs following operations with resources : Allocation Deallocation Scheduling Protection Sharing \u2022 Assigning \u2022 Releasing \u2022 Allocating \u2022 Ensuring \u2022 Allowing resources resources CPU time processes multiple to when no to access processes processes longer processes. only to access or needed. authorize shared programs. d resources. resources. Influence of Security \u2751Initially, OS security was minimal. Today, it is a foundational requirement. \u2751 With systems connected to networks and storing sensitive data, the OS must protect against \u2751unauthorized access, \u2751malware, and \u2751other threats. \u2751 It enforces security policies through mechanisms like \u2751user accounts, \u2751permissions, and \u2751firewalls, \u2751ensuring data integrity and user privacy Networking \u2751 The rise of the internet and local networks transformed computers from isolated machines into communication devices. \u2751 Consequently, networking became a core OS service. \u2751 The OS manages network connections, protocols (like TCP/IP), and interfaces. \u2751 It provides the foundation for everything from web Browse and email to distributed computing and cloud services. Networking \u2751 The demand for high-quality audio and video has significantly influenced OS design. \u2751 Multimedia applications require the OS to handle high-throughput data streams and meet real-time deadlines. \u2751 A slight delay in processing a video frame can ruin the user experience. \u2751 This led to the development of better scheduling algorithms and improved I/O techniques to ensure smooth, continuous playback and processing of multimedia content."
  },
  {
    "id": 7,
    "source": "Module 1.txt",
    "text": "OS service. \u2751 The OS manages network connections, protocols (like TCP/IP), and interfaces. \u2751 It provides the foundation for everything from web Browse and email to distributed computing and cloud services. Networking \u2751 The demand for high-quality audio and video has significantly influenced OS design. \u2751 Multimedia applications require the OS to handle high-throughput data streams and meet real-time deadlines. \u2751 A slight delay in processing a video frame can ruin the user experience. \u2751 This led to the development of better scheduling algorithms and improved I/O techniques to ensure smooth, continuous playback and processing of multimedia content."
  },
  {
    "id": 8,
    "source": "Module 3.txt",
    "text": "Operating System BCSE303L Module 3 - Scheduling Dr. Naveenkumar Jayakumar Associate Professor Department of Computational Intelligence PRP 217-4 naveenkumar.jk@vit.ac.in Scheduling - Definition \u2751 The activity of deciding which process gets to use the CPU at any given time. \u2751 It is the mechanism by which the process manager removes a currently running process from the CPU and selects another one to take its place, based on a specific strategy or algorithm. \u2751 In multiprogramming operating systems, which allow multiple processes to be loaded into memory simultaneously and share the CPU over time. \u2751 It is the mechanism of selecting a process from a ready queue and allotting CPU to this process for execution. Scheduling - Objectives \u2751The operating system schedules the processes in such a way that the CPU doesn\u2019t sit idle and keeps processing some or the other process. \u2751 The primary objective of process scheduling is to optimize system performance according to several key metrics: \u2751Maximize CPU Utilization Keep the CPU as busy as possible to prevent wasted cycles. \u2751Minimize Response Time Reduce the time it takes from a user's request to the start of the response. \u2751Minimize Waiting Time Decrease the amount of time a process spends in the ready queue waiting for the CPU. \u2751Fair Allocation Ensure each process gets a fair share of the CPU's time. Scheduling - Working Scheduling - Working \u2751 Scheduling is managed by system software called schedulers, which move processes between different states using scheduling queues. \u2751 The operating system maintains several queues to manage processes \u2751 Job Queue/Task Queue/Process Queue: Contains all processes in the system. \u2751 Ready Queue Holds processes that are in main memory and are ready and waiting to execute. \u2751 Device Queues/I/O Queue/Wait Queue: Contain processes that are blocked because they are waiting for an I/O device to become available Scheduling - Working \u2751 There are generally three types of schedulers that operate at different frequencies and for different purposes \u2751 Long-Term Scheduler (or Job Scheduler) This scheduler selects processes from the job queue and loads them into memory for execution. It controls the degree of multiprogramming (the number of processes in memory) and aims to create a balanced mix of CPU-bound and I/O- bound jobs. \u2751 Short-Term Scheduler (or CPU Scheduler) This is the most frequently executed scheduler. It selects a process from the ready queue and allocates the CPU to it. Its main goal is to optimize CPU performance and utilization. \u2751 Medium-Term Scheduler (Present in some systems) This scheduler is involved in swapping processes out of memory to reduce the degree of multiprogramming and later swapping them back in to continue execution. Scheduling - Categories \u2751 Non-Preemptive Scheduling Once the CPU has been allocated to a process, that process keeps the CPU until it either terminates or switches to a waiting state (e.g., for an I/O operation). The resource cannot be forcibly taken away. \u2751 Preemptive Scheduling The operating system can forcibly remove a running process from the CPU and reallocate it to"
  },
  {
    "id": 9,
    "source": "Module 3.txt",
    "text": "is to optimize CPU performance and utilization. \u2751 Medium-Term Scheduler (Present in some systems) This scheduler is involved in swapping processes out of memory to reduce the degree of multiprogramming and later swapping them back in to continue execution. Scheduling - Categories \u2751 Non-Preemptive Scheduling Once the CPU has been allocated to a process, that process keeps the CPU until it either terminates or switches to a waiting state (e.g., for an I/O operation). The resource cannot be forcibly taken away. \u2751 Preemptive Scheduling The operating system can forcibly remove a running process from the CPU and reallocate it to another process. This often happens when a higher-priority process arrives or when a running process has exceeded its allocated time slice Scheduling \u2013 Metrics Measured \u2751 CPU Burst Time \u2751 CPU Burst Time is the amount of time a process needs to run on the CPU to complete its computations. \u2751 A process's life is made up of cycles of CPU bursts and I/O waiting periods \u2751 CPU bursts can vary in length depending on the nature of the process \u2751 CPU burst time is critical for scheduling algorithms to determine when to switch processes to optimize CPU utilization and responsiveness Scheduling \u2013 Metrics Measured \u2751 I/O Burst Time \u2751 I/O Burst Time is the period a process spends waiting for an Input/Output (I/O) operation to finish . \u2751 This happens when the process needs to read data from a disk, get input from a user, or send data to a printer . \u2751 During this time, the process is not using the CPU; it is \"blocked\" and waiting for an external device. \u2751 I/O bursts are typically followed by CPU bursts as processes alternate between computation and I/O activities. Scheduling \u2013 Metrics Measured \u2751 Arrival Time \u2751 Arrival Time is the exact moment a process enters the ready queue, meaning it is ready to be executed and is waiting for the CPU to become available . \u2751 It is the official entry time of a process into the system to compete for CPU time Scheduling \u2013 Metrics Measured \u2751 Completion Time \u2751 Completion Time is the time at which a process finishes its execution and exits the system \u2751 It marks the moment the process has completed all its CPU bursts and I/O operations and is officially done. Scheduling \u2013 Metrics Measured \u2751 Turnaround Time \u2751 Turnaround Time is the total time a process spends in the system, from its arrival to its completion . \u2751 It measures the entire lifecycle of a process and includes both the time spent executing on the CPU and the time spent waiting (in the ready queue and for I/O) Turnaround Time = Completion Time - Arrival Time Scheduling \u2013 Metrics Measured \u2751 Waiting Time \u2751 Waiting Time is the total time a process spends in the ready queue, waiting for its turn to use the CPU . \u2751 It specifically measures the time a process is ready to run but is forced"
  },
  {
    "id": 10,
    "source": "Module 3.txt",
    "text": "the total time a process spends in the system, from its arrival to its completion . \u2751 It measures the entire lifecycle of a process and includes both the time spent executing on the CPU and the time spent waiting (in the ready queue and for I/O) Turnaround Time = Completion Time - Arrival Time Scheduling \u2013 Metrics Measured \u2751 Waiting Time \u2751 Waiting Time is the total time a process spends in the ready queue, waiting for its turn to use the CPU . \u2751 It specifically measures the time a process is ready to run but is forced to wait because the CPU is busy with other processes. \u2751 It does not include the time spent executing or waiting for I/O. Waiting Time = Turnaround Time - CPU Burst Time Scheduling \u2013 Metrics Measured \u2751 Response Time \u2751 Response Time is the time elapsed from when a process arrives until it gets the CPU for the first time . \u2751 This metric is crucial for interactive systems because it reflects how quickly the system acknowledges and begins to handle a user's request. \u2751 The time from when a process is submitted until the first response is produced Response Time = Time of First Response - Arrival Time Scheduling \u2013 Uniprocessor - FCFS \u2751 First Come First Serve \u2751 It is a non-pre-emptive, arrival-order scheduling policy for the ready queue in a uniprocessor system \u2751 Every newly admitted process Pi is appended to the tail of the queue. \u2751 The dispatcher removes the head process, loads its context, and runs it until it voluntarily yields: \u2751 terminates, or \u2751 executes a blocking system call (e.g., I/O, semaphore P operation, page fault). Scheduling \u2013 Uniprocessor - FCFS \u2751 First Come First Serve \u2751 When the running process blocks, the dispatcher selects the next ready process (again, queue head). \u2751 When a blocked process becomes ready, it is always enqueued at the tail\u2014 never reinserted ahead of older entrants. \u2751 Because no timer pre-emption occurs, the CPU busy interval is a sequence of maximal CPU bursts. Scheduling \u2013 Uniprocessor - FCFS \u2751 First Come First Serve \u2751 The ready queue is treated as a simple FIFO: the process that arrives first gets the CPU first, and once a process starts execution it runs to completion of its current CPU burst (i.e. non-preemptive). Scheduling \u2013 Uniprocessor - FCFS \u2751 First Come First Serve \u2013 Example Consider the system image at the current moment and suggest how FCFS will work in this: Process Arrival Time CPU Burst P1 0 5 P2 2 3 P3 4 4 P4 6 6 P5 8 2 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P4 6 6 P5 8 2 P1 0 5 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3"
  },
  {
    "id": 11,
    "source": "Module 3.txt",
    "text": "Serve \u2013 Example Consider the system image at the current moment and suggest how FCFS will work in this: Process Arrival Time CPU Burst P1 0 5 P2 2 3 P3 4 4 P4 6 6 P5 8 2 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P4 6 6 P5 8 2 P1 0 5 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P4 6 6 P5 8 2 P1 P2 0 5 8 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P4 6 6 P5 8 2 P1 P2 P3 0 5 8 12 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P4 6 6 P5 8 2 P1 P2 P3 P4 0 5 8 12 18 Scheduling \u2013 Uniprocessor - FCFS Process Arrival Time CPU Burst \u2751 First Come First Serve \u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P4 6 6 P5 8 2 P1 P2 P3 P4 P5 0 5 8 12 18 20 Scheduling \u2013 Uniprocessor - FCFS \u2751 First Come First Serve \u2013 Example Calculating other Metrics Turnaround Time (Completion Waiting Time Response Time Time \u2013 Arrival Time) (Turnaround Time \u2013 time until first Arrival Completion Process CPU Burst Burst Time) scheduled \u2013 Arrival (for Time Time non-preemptive, RT = WT). P1 0 5 5 5 \u2013 0 = 5 5 \u2013 5 = 0 0 \u2013 0 = 0 P2 2 3 8 8 \u2013 2 = 6 6 \u2013 3 = 3 5 \u2013 2 = 3 P3 4 4 12 12 \u2013 4 = 8 8 \u2013 4 = 4 8 \u2013 4 = 4 P4 6 6 18 18 \u2013 6 = 12 12 \u2013 6 = 6 12 \u2013 6 = 6 P5 8 2 20 20 \u2013 8 = 12 12 \u2013 2 = 10 18 \u2013 8 = 10 Average 8.6 4.6 Scheduling \u2013 Uniprocessor - SJF \u2751 Shortest Job First \u2751 Shortest Job First (SJF) is a CPU scheduling algorithm that always selects the process with the smallest next CPU burst to execute next. \u2751 By prioritizing shorter jobs, SJF minimizes the average waiting time in the ready queue. \u2751 Non-Preemptive SJF \u2751 Once the CPU is assigned to a process, it runs to completion of its current CPU burst. \u2751 New arrivals\u2014even if shorter\u2014must wait. Scheduling \u2013 Uniprocessor - SJF \u2751 Shortest Job First \u2751 Preemptive SJF (a.k.a. Shortest Remaining Time First, SRTF) \u2751 If a new process arrives whose remaining CPU burst is"
  },
  {
    "id": 12,
    "source": "Module 3.txt",
    "text": "SJF \u2751 Shortest Job First \u2751 Shortest Job First (SJF) is a CPU scheduling algorithm that always selects the process with the smallest next CPU burst to execute next. \u2751 By prioritizing shorter jobs, SJF minimizes the average waiting time in the ready queue. \u2751 Non-Preemptive SJF \u2751 Once the CPU is assigned to a process, it runs to completion of its current CPU burst. \u2751 New arrivals\u2014even if shorter\u2014must wait. Scheduling \u2013 Uniprocessor - SJF \u2751 Shortest Job First \u2751 Preemptive SJF (a.k.a. Shortest Remaining Time First, SRTF) \u2751 If a new process arrives whose remaining CPU burst is shorter than the remaining time of the running process, the CPU is preempted and given to the new process. Scheduling \u2013 Uniprocessor - SJF \u2751 Shortest Job First (Non \u2013 Preemptive) \u2013 Example Consider the system image at the current moment and suggest how SJF will work in this: Process Arrival Time CPU Burst P1 0 5 P2 2 3 P3 4 4 P4 6 6 P5 8 2 Scheduling \u2013 Uniprocessor - SJF Process Arrival Time CPU Burst \u2751 SJF (Non-Preemptive)\u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 At time 0 only P1 arrives \u2192 CPU allotted to P1 P4 6 6 P5 8 2 P1 0 5 Scheduling \u2013 Uniprocessor - SJF Process Arrival Time CPU Burst \u2751 SJF (Non-Preemptive)\u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P1 will run till time 5, In between P2 & P3 arrives. P4 6 6 P2 has the shortest CPU burst time hence \u2192 CPU P5 8 2 allotted to P2 P1 P2 0 5 8 Scheduling \u2013 Uniprocessor - SJF Process Arrival Time CPU Burst \u2751 SJF (Non-Preemptive)\u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P2 will run till time 8, In between P4 & P5 arrives P4 6 6 and already P3 is waiting. Out of P3, P4 & P5, P5 P5 8 2 has shortest CPU Burst time, Hence P5 gets CPU P1 P2 P5 0 5 8 10 Scheduling \u2013 Uniprocessor - SJF Process Arrival Time CPU Burst \u2751 SJF (Non-Preemptive)\u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P5 will run till time 10, P3 & P4 are waiting. Out of P4 6 6 both P3 has the shortest CPU Burst Time hence P3 P5 8 2 \u2192 CPU allocated P1 P2 P5 P3 0 5 8 10 14 Scheduling \u2013 Uniprocessor - SJF Process Arrival Time CPU Burst \u2751 SJF (Non-Preemptive)\u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P3 will run till time 14, After P3 completes only P4 P4 6 6 is left hence CPU allocated to it. P5 8 2 P1 P2 P5 P3 P4 0 5 8 10 14 20 Scheduling \u2013 Uniprocessor - SJF \u2751 Shortest Job First (Non \u2013 Preemptive) \u2013 Example Calculating other Metrics Turnaround Time Waiting Time Response Time"
  },
  {
    "id": 13,
    "source": "Module 3.txt",
    "text": "P3 P5 8 2 \u2192 CPU allocated P1 P2 P5 P3 0 5 8 10 14 Scheduling \u2013 Uniprocessor - SJF Process Arrival Time CPU Burst \u2751 SJF (Non-Preemptive)\u2013 Example P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P3 will run till time 14, After P3 completes only P4 P4 6 6 is left hence CPU allocated to it. P5 8 2 P1 P2 P5 P3 P4 0 5 8 10 14 20 Scheduling \u2013 Uniprocessor - SJF \u2751 Shortest Job First (Non \u2013 Preemptive) \u2013 Example Calculating other Metrics Turnaround Time Waiting Time Response Time (Completion Time \u2013 (Turnaround Time time until first Arrival CPU Completion Process Arrival Time) \u2013 Burst Time) scheduled \u2013 Arrival Time Burst Time (for non-preemptive, RT = WT). P1 0 5 5 5 \u2013 0 = 5 5 \u2013 5 = 0 0 \u2013 0 = 0 P2 2 3 8 8 \u2013 2 = 6 6 \u2013 3 = 3 5 \u2013 2 = 3 P5 8 2 10 10 \u2013 8 = 2 2 \u2013 2 = 0 8 \u2013 8 = 0 P3 4 4 14 14 \u2013 4 = 10 10 \u2013 4 = 6 10 \u2013 4 = 6 P4 6 6 20 20 \u2013 6 = 14 14 \u2013 6 = 8 14 \u2013 6 = 8 Average 7.4 3.4 Scheduling \u2013 Uniprocessor - SRTF \u2751 Shortest Job First (Preemptive) \u2013 Example Consider the system image at the current moment and suggest how SJF will work in this: Process Arrival Time CPU Burst P1 0 5 P2 2 3 P3 4 4 P4 6 6 P5 8 2 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 At time 0 only P1 arrives \u2192 CPU allotted to P1 P4 6 6 At time 2 P1 remains with 3 Bursts and P2 arrives P5 8 2 with 3 BT so let P1 Continue and complete. P1 0 5 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 At time 4 P3 arrives with 4 BT but P2 is waiting P4 6 6 with 3 BT hence P2 is allocated to CPU P5 8 2 P1 P2 0 5 8 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 At time 8 P4 & P5 arrives with 6 & 2 BT P4 6 6 respectively, but P3 is also waiting with 4 BT, P5 8 2 Shortest BT is P5 hence it is allocated. P1 P2 P5 0 5 8 10 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 Out of remaining process"
  },
  {
    "id": 14,
    "source": "Module 3.txt",
    "text": "Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 At time 8 P4 & P5 arrives with 6 & 2 BT P4 6 6 respectively, but P3 is also waiting with 4 BT, P5 8 2 Shortest BT is P5 hence it is allocated. P1 P2 P5 0 5 8 10 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 Out of remaining process P3 has shortest BT hence P4 6 6 allocated P5 8 2 P1 P2 P5 P3 0 5 8 10 14 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst \u2751 Shortest Job First (Preemptive) P1 0 5 P2 2 3 Constructing Gantt Chart P3 4 4 P3 will run till time 14, After P3 completes only P4 P4 6 6 is left hence CPU allocated to it. P5 8 2 P1 P2 P5 P3 P4 0 5 8 10 14 20 Scheduling \u2013 Uniprocessor - SRTF \u2751 Shortest Job First (Preemptive) \u2013 Example Calculating other Metrics Turnaround Time Waiting Time Response Time Arriv (Completion Time \u2013 (Turnaround Time \u2013 time until first CPU Completio Process al Arrival Time) Burst Time) scheduled \u2013 Arrival Burst n Time Time (for non-preemptive, RT = WT). P1 0 5 5 5 \u2013 0 = 5 5 \u2013 5 = 0 0 \u2013 0 = 0 P2 2 3 8 8 \u2013 2 = 6 6 \u2013 3 = 3 5 \u2013 2 = 3 P5 8 2 10 10 \u2013 8 = 2 2 \u2013 2 = 0 8 \u2013 8 = 0 P3 4 4 14 14 \u2013 4 = 10 10 \u2013 4 = 6 10 \u2013 4 = 6 P4 6 6 20 20 \u2013 6 = 14 14 \u2013 6 = 8 14 \u2013 6 = 8 Average 7.4 3.4 Scheduling \u2013 Uniprocessor \u2013 Priority \u2751 Priority Scheduling \u2751 Each ready process is assigned an explicit priority value \u2751 The scheduler always dispatches the highest-priority ready process next \u2751 Pre-emptive priority \u2013 a newly-arriving job with higher priority immediately pre-empts the running job \u2751 Non-pre-emptive priority \u2013 the running job keeps the CPU until it blocks or finishes; new jobs wait even if they have higher priority \u2751 If several jobs share the same priority, the scheduler falls back to FCFS \u2751 Very low-priority jobs may wait forever while higher-priority jobs keep arriving; ageing (gradually boosting a job\u2019s priority the longer it waits) is a common fix. Scheduling \u2013 Uniprocessor \u2013 Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example Consider the system image at the current moment and suggest how Priority scheduling will work if the lower number are considered as high priority in this: Process Arrival Time CPU Burst Priority P1 0 5 2 P2 2 3 1 P3 4 4 3 P4 6 6 4 P5 8"
  },
  {
    "id": 15,
    "source": "Module 3.txt",
    "text": "If several jobs share the same priority, the scheduler falls back to FCFS \u2751 Very low-priority jobs may wait forever while higher-priority jobs keep arriving; ageing (gradually boosting a job\u2019s priority the longer it waits) is a common fix. Scheduling \u2013 Uniprocessor \u2013 Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example Consider the system image at the current moment and suggest how Priority scheduling will work if the lower number are considered as high priority in this: Process Arrival Time CPU Burst Priority P1 0 5 2 P2 2 3 1 P3 4 4 3 P4 6 6 4 P5 8 2 2 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 0 only P1 arrives \u2192 CPU allotted to P1 P5 8 2 2 P1 0 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 2 \u2192 P1 Priority < P2 Priority, P1 P5 8 2 2 preempted and 3 CPU BT pending for P1 P1 P2 0 2 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 4, P3 priority < P2 & P1 priority, hence P5 8 2 2 allow P2 to complete the CPU BT till 5. P1 P2 0 2 5 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P2 completes. Since P1 has higher Priority, it takes P5 8 2 2 CPU and completes its remaining BT. At time 6, P4 Priority < P3 & P1 hence P1 continues till time 8. P1 P2 P1 0 2 5 8 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P1 completes. At time 8, P5 Priority > P3 & P4, P5 8 2 2 Hence P5 will take CPU P1 P2 P1 P5 0 2 5 8 10 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P5 Completes. P3 Priority > P4 Priority. Hence P3 P5 8 2 2 runs P1 P2 P1 P5 P3 0 2 5 8 10 14 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013"
  },
  {
    "id": 16,
    "source": "Module 3.txt",
    "text": "P5 Priority > P3 & P4, P5 8 2 2 Hence P5 will take CPU P1 P2 P1 P5 0 2 5 8 10 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P5 Completes. P3 Priority > P4 Priority. Hence P3 P5 8 2 2 runs P1 P2 P1 P5 P3 0 2 5 8 10 14 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P3 Completes. P4 runs P5 8 2 2 P1 P2 P1 P5 P3 P4 0 2 5 8 10 14 20 Scheduling \u2013 Uniprocessor \u2013 Priority \u2751 Priority Scheduling \u2013 (Preemptive) \u2013 Example Calculating other Metrics Turnaround Time Waiting Time Response Time (Completion Time \u2013 (Turnaround Time \u2013 time until first scheduled Arrival Completion Process CPU Burst Arrival Time) Burst Time) \u2013 Arrival (for Time Time non-preemptive, RT = WT). P1 0 5 8 8 \u2013 0 = 8 8 \u2013 5 = 3 0 \u2013 0 = 0 P2 2 3 5 5 \u2013 2 = 3 3 \u2013 3 = 0 2 \u2013 2 = 0 P5 8 2 10 10 \u2013 8 = 2 2 \u2013 2 = 0 8 \u2013 8 = 0 P3 4 4 14 14 \u2013 4 = 10 10 \u2013 4 = 6 10 \u2013 4 = 6 P4 6 6 20 20 \u2013 6 = 14 14 \u2013 6 = 8 14 \u2013 6 = 8 Average 7.4 3.4 Scheduling \u2013 Uniprocessor \u2013 Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example Consider the system image at the current moment and suggest how Priority scheduling will work if the lower number are considered as high priority in this: Process Arrival Time CPU Burst Priority P1 0 5 2 P2 2 3 1 P3 4 4 3 P4 6 6 4 P5 8 2 2 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 0 only P1 arrives \u2192 CPU allotted to P1 P5 8 2 2 P1 0 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 2 \u2192 P1 Priority < P2 Priority, P1 cannot be P5 8 2 2 preempted and P1 continues P1 P2 0 5 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6"
  },
  {
    "id": 17,
    "source": "Module 3.txt",
    "text": "8 2 2 P1 0 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 2 \u2192 P1 Priority < P2 Priority, P1 cannot be P5 8 2 2 preempted and P1 continues P1 P2 0 5 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 At time 4, is P3 priority < P2 & P1 priority, hence P5 8 2 2 allow P2 to complete the CPU BT till 8. P1 P2 0 2 8 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P5 8 2 2 P1 P2 P5 0 5 8 10 Scheduling \u2013 Uniprocessor \u2013 Priority Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P5 8 2 2 P1 P2 P5 P3 0 5 8 10 14 Scheduling \u2013 Uniprocessor - SRTF Process Arrival Time CPU Burst Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) Example P1 0 5 2 P2 2 3 1 Constructing Gantt Chart P3 4 4 3 P4 6 6 4 P5 8 2 2 P1 P2 P5 P3 P4 0 5 8 10 14 20 Scheduling \u2013 Uniprocessor \u2013 Priority \u2751 Priority Scheduling \u2013 (Non-Preemptive) \u2013 Example Calculating other Metrics Turnaround Time Waiting Time Response Time (Completion Time \u2013 (Turnaround Time \u2013 time until first scheduled Arrival Completion Process CPU Burst Arrival Time) Burst Time) \u2013 Arrival (for Time Time non-preemptive, RT = WT). P1 0 5 5 5 \u2013 0 = 5 5 \u2013 5 = 0 0 \u2013 0 = 0 P2 2 3 8 8 \u2013 2 = 6 6 \u2013 3 = 3 5 \u2013 2 = 3 P5 8 2 10 10 \u2013 8 = 2 2 \u2013 2 = 0 8 \u2013 8 = 0 P3 4 4 14 14 \u2013 4 = 10 10 \u2013 4 = 6 10 \u2013 4 = 6 P4 6 6 20 20 \u2013 6 = 14 14 \u2013 6 = 8 14 \u2013 6 = 8 Average 7.4 3.4 Scheduling \u2013 Uniprocessor \u2013 Round Robin \u2751 It is a preemptive algorithm that gives each process a fixed amount of time, known as a time quantum or time slice, to run on the CPU. \u2751 This method ensures that all processes get a fair share of the CPU's time. \u2751 The Round Robin algorithm works by assigning each process a turn to execute in a cyclical fashion. \u2751Note: The preempted process is added to the"
  },
  {
    "id": 18,
    "source": "Module 3.txt",
    "text": "4 = 6 P4 6 6 20 20 \u2013 6 = 14 14 \u2013 6 = 8 14 \u2013 6 = 8 Average 7.4 3.4 Scheduling \u2013 Uniprocessor \u2013 Round Robin \u2751 It is a preemptive algorithm that gives each process a fixed amount of time, known as a time quantum or time slice, to run on the CPU. \u2751 This method ensures that all processes get a fair share of the CPU's time. \u2751 The Round Robin algorithm works by assigning each process a turn to execute in a cyclical fashion. \u2751Note: The preempted process is added to the tail of the ready queue first, followed by the newly arrived process. Scheduling \u2013 Uniprocessor \u2013 Round Robin \u2751Ready Queue: All active processes are kept in a ready queue, which is typically managed in a First-In, First-Out (FIFO) order. \u2751 Time Quantum: The system defines a fixed time quantum \u2751 Process Execution: The scheduler selects the first process from the ready queue and allows it to run on the CPU for the duration of one-time quantum \u2751 Process Completes: If the process finishes its execution before the time quantum expires, it voluntarily releases the CPU. The scheduler then immediately selects the next process in the queue. \u2751 Time Quantum Expires: If the process is still running when the time quantum ends, the CPU is preempted (taken away) from the process. The process is then moved to the back of the ready queue to await its next turn \u2751 Cyclic Repetition: The scheduler continues this cycle, picking the next process from the front of the queue, until all processes have completed their execution Scheduling \u2013 Uniprocessor \u2013 Round Robin \u2751 Consider the system image at the current moment and suggest how Round Robin scheduling will work if the given time quantum is 2 seconds. Process Arrival Time CPU Burst Priority P1 0 5 2 P2 2 3 1 P3 4 4 3 P4 6 6 4 P5 8 2 2 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 0 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1 Gantt Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 0 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1 Gantt P1 0 2 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 2 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P2 P1 Gantt P1 0 2 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 2 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1(3) P2 P1 Gantt P1 0"
  },
  {
    "id": 19,
    "source": "Module 3.txt",
    "text": "Head P4 6 6 P5 8 2 P1 Gantt P1 0 2 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 2 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P2 P1 Gantt P1 0 2 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 2 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1(3) P2 P1 Gantt P1 0 2 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 2 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1(3) P2 P1 Gantt P1 P2 0 2 4 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 4 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P3 P1(3) P2 P1 Gantt P1 P2 0 2 4 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 4 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 0 2 4 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 4 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 0 2 4 6 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 6 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 0 2 4 6 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 6 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 0 2 4 6 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 6 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 0 2 4 6 8 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 8 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P5 P1(1) P4 P2(1) P3 P1(3)"
  },
  {
    "id": 20,
    "source": "Module 3.txt",
    "text": "6 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 6 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 0 2 4 6 8 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 8 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 0 2 4 6 8 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 8 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 0 2 4 6 8 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 8 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 0 2 4 6 8 9 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 9 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 0 2 4 6 8 9 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 9 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 0 2 4 6 8 9 11 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 11 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 0 2 4 6 8 9 11 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 11 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 0 2 4 6 8 9 11 12 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 12 P2 2"
  },
  {
    "id": 21,
    "source": "Module 3.txt",
    "text": "P3 P2 P4 0 2 4 6 8 9 11 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 11 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 0 2 4 6 8 9 11 12 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 12 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 0 2 4 6 8 9 11 12 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 12 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 0 2 4 6 8 9 11 12 14 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 14 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 0 2 4 6 8 9 11 12 14 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 14 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 P3 0 2 4 6 8 9 11 12 14 16 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 16 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 P3 0 2 4 6 8 9 11 12 14 16 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 16 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 P3 P4 0 2 4 6 8 9 11 12 14 16 18 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 18"
  },
  {
    "id": 22,
    "source": "Module 3.txt",
    "text": "8 9 11 12 14 16 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 16 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 P3 P4 0 2 4 6 8 9 11 12 14 16 18 Scheduling \u2013 Uniprocessor \u2013 Round Robin Process Arrival Time CPU Burst Constructing Gantt Chart P1 0 5 Ready Queue Image \u2013 At Time 18 P2 2 3 P3 4 4 Tail Head P4 6 6 P5 8 2 P4(4) P3(2) P5 P1(1) P4 P2(1) P3 P1(3) P2 P1 Gantt P1 P2 P1 P3 P2 P4 P1 P5 P3 P4 P4 0 2 4 6 8 9 11 12 14 16 18 20 Scheduling \u2013 Uniprocessor \u2013 Round Robin \u2751 Round Robin Scheduling \u2013 (Preemptive) \u2013 Example Calculating other Metrics Turnaround Time Waiting Time Response Time (Completion Time \u2013 (Turnaround Time \u2013 time until first scheduled Arrival Completion Process CPU Burst Arrival Time) Burst Time) \u2013 Arrival (for Time Time non-preemptive, RT = WT). P1 0 5 12 12 \u2013 0 = 12 12 \u2013 5 = 7 0 \u2013 0 = 0 P2 2 3 9 9 \u2013 2 = 7 7 \u2013 3 = 4 2 \u2013 2 = 0 P3 4 4 16 16 \u2013 4 = 12 12 \u2013 4 = 8 6 \u2013 4 = 2 P4 6 6 20 20 \u2013 6 = 14 14 \u2013 6 = 8 9 \u2013 6 = 3 P5 8 2 14 14 \u2013 8 = 6 6 \u2013 2 = 4 12 \u2013 8 = 4 Average 10.2 6.2 Scheduling \u2013 Uniprocessor \u2013 Example \u2751 Run the process with the highest priority. Processes with the same priority run round-robin with Time slice = 2. Process Arrival Time Burst Time Priority p1 0 4 3 p2 0 5 2 p3 0 8 2 p4 0 7 1 p5 0 3 3 Scheduling \u2013 Uniprocessor \u2013 Example \u2751 Run the process with the highest priority. Processes with the same priority run round-robin with Time slice = 2. Process Arrival Time Burst Time Priority p1 0 4 3 p2 0 5 2 p3 0 8 2 p4 0 7 1 p5 0 3 3 p4 p2 p3 p2 p3 p2 p3 p1 p5 p1 p5 Scheduling \u2013 Uniprocessor \u2013 Example \u2751 Run the process with the highest priority. Processes with the same priority run round-robin with Time slice = 2. Proces Arrival Burst Priority Completion Turnaround Time Waiting Time s Time Time Time (Completion Time \u2013 (Turnaround Time \u2013 Arrival Time) Burst Time) p1 0 4 3 26 26 \u2013 0 = 26 26 \u2013 4 = 22 p2 0 5 2 16 16 \u2013 0 = 16 16 \u2013 5 = 11 p3 0 8 2 20 20 \u2013 0 = 20 20 \u2013"
  },
  {
    "id": 23,
    "source": "Module 3.txt",
    "text": "p2 p3 p2 p3 p2 p3 p1 p5 p1 p5 Scheduling \u2013 Uniprocessor \u2013 Example \u2751 Run the process with the highest priority. Processes with the same priority run round-robin with Time slice = 2. Proces Arrival Burst Priority Completion Turnaround Time Waiting Time s Time Time Time (Completion Time \u2013 (Turnaround Time \u2013 Arrival Time) Burst Time) p1 0 4 3 26 26 \u2013 0 = 26 26 \u2013 4 = 22 p2 0 5 2 16 16 \u2013 0 = 16 16 \u2013 5 = 11 p3 0 8 2 20 20 \u2013 0 = 20 20 \u2013 8 = 12 p4 0 7 1 7 7 - 0 = 7 7 \u2013 7 = 0 p5 0 3 3 27 27 - 0 = 27 27 \u2013 3 = 24 Average Turnaround Time 19 Average Waiting Time 13.8 Multi-Level Queue Scheduling \u2751 Multilevel Queue Scheduling is like creating multiple, separate waiting lines for the CPU (the computer's processor) instead of just one. \u2751 Processes are sorted into these different lines, or queues, based on their characteristics, such as priority, memory size, or process type (Interactive, Batch or User). \u2751 Once a process is placed in a queue, it generally stays there Multi-Level Queue Scheduling \u2751 Multiple Queues: The main ready queue is divided into several separate queues. \u2751 Process Sorting: Processes are permanently assigned to a specific queue based on a property, like whether they are interactive (foreground) or batch (background) jobs. \u2751 Independent Scheduling: Each queue can have its own unique scheduling algorithm. For example, a high-priority queue might use Round Robin (RR) scheduling to ensure responsiveness, while a low- priority queue might use First-Come, First-Served (FCFS). \u2751 Scheduling Between Queues: There is also a scheduling method for the queues themselves. This is typically a fixed-priority preemptive scheduling system. This means that no process in a lower-priority queue can run unless all the higher-priority queues are empty. If a high-priority process arrives while a low-priority process is running, the low-priority process is interrupted (preempted). Multi-Level Queue Scheduling \u2022 The ready queue consists of multiple queues \u2022 Multilevel queue scheduler defined by the following parameters: \u2022 Number of queues \u2022 Scheduling algorithms for each queue \u2022 Method used to determine which queue a process will enter when that process needs service \u2022 Scheduling among the queues Multi-Level Queue Scheduling \u25aa With priority scheduling, have separate queues for each priority. \u25aa Schedule the process in the highest-priority queue! Multi-Level Queue Scheduling - Example \u25aa Consider a system with four processes and two ready queues. The scheduling is governed by the following rules: \u25aa There are two queues: Q1 (High Priority) and Q2 (Low Priority). \u25aa Processes in Q1 have absolute priority over processes in Q2. If a Q1 process arrives while a Q2 process is running, the Q2 process will be preempted. \u25aa Q1 uses a Round Robin (RR) scheduling algorithm with a time quantum of 2ms. \u25aa Q2 uses a First-Come, First-Served (FCFS) scheduling algorithm. Process Arrival"
  },
  {
    "id": 24,
    "source": "Module 3.txt",
    "text": "each priority. \u25aa Schedule the process in the highest-priority queue! Multi-Level Queue Scheduling - Example \u25aa Consider a system with four processes and two ready queues. The scheduling is governed by the following rules: \u25aa There are two queues: Q1 (High Priority) and Q2 (Low Priority). \u25aa Processes in Q1 have absolute priority over processes in Q2. If a Q1 process arrives while a Q2 process is running, the Q2 process will be preempted. \u25aa Q1 uses a Round Robin (RR) scheduling algorithm with a time quantum of 2ms. \u25aa Q2 uses a First-Come, First-Served (FCFS) scheduling algorithm. Process Arrival Time Burst Time Priority P1 0 5 1(Q1) P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 0 \u2192 P1 arrives and enters Q1. As it's the P1 0 5 1(Q1) only process, it starts executing P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1 Tail Head Ready Queue 2 Gantt 0 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 0 \u2192 P1 arrives and enters Q1. As it's the P1 0 5 1(Q1) only process, it starts executing P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1 Tail Head Ready Queue 2 Gantt P1 0 2 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 1 \u2192 P2 arrives and is placed in Q2. P1 P1 0 5 1(Q1) continues to run because Q1 has higher priority P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1 Tail Head Ready Queue 2 P2 Gantt P1 0 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 2 \u2192 P3 arrives and is placed in Q1. P1 is P1 0 5 1(Q1) Preempted and added back to Ready Queue 1 P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3 P1 Tail Head Ready Queue 2 P2 Gantt P1 0 2 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 2 \u2192 P3 arrives and is placed in Q1. P1 is P1 0 5 1(Q1) Preempted and added back to Ready Queue 1 P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1(3) P3 P1 Tail Head Ready Queue 2 P2 Gantt P1 P3 0 2 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 3 \u2192 P4 arrives and is placed in Q2. P3 is P1 0 5 1(Q1) continuing its execution since it has high priority. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt"
  },
  {
    "id": 25,
    "source": "Module 3.txt",
    "text": "to Ready Queue 1 P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1(3) P3 P1 Tail Head Ready Queue 2 P2 Gantt P1 P3 0 2 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 3 \u2192 P4 arrives and is placed in Q2. P3 is P1 0 5 1(Q1) continuing its execution since it has high priority. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 0 2 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 4 \u2192 P3 complete the quantum and is P1 0 5 1(Q1) preempted. P1 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 0 2 4 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 4 \u2192 P3 complete the quantum and is P1 0 5 1(Q1) preempted. P1 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 0 2 4 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 6 \u2192 P1 completes the quantum and is P1 0 5 1(Q1) preempted. P3 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 0 2 4 6 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 6 \u2192 P1 completes the quantum and is P1 0 5 1(Q1) preempted. P3 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 0 2 4 6 8 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 8 \u2192 P3 completes the quantum and is P1 0 5 1(Q1) preempted. P1 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 0 2 4 6 8 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 8 \u2192 P3 completes the quantum and is P1 0 5 1(Q1) preempted. P1 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail"
  },
  {
    "id": 26,
    "source": "Module 3.txt",
    "text": "quantum and is P1 0 5 1(Q1) preempted. P1 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 0 2 4 6 8 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 8 \u2192 P3 completes the quantum and is P1 0 5 1(Q1) preempted. P1 is given CPU to execute. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 0 2 4 6 8 9 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 9 \u2192 P1 terminates and P3 gets the CPU. P1 0 5 1(Q1) P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 0 2 4 6 8 9 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 11 \u2192 P3 gets the CPU since it is the only P1 0 5 1(Q1) high priority process. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 P3 P3 0 2 4 6 8 9 11 12 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 12 \u2192 All process in Queue 1 is terminated P1 0 5 1(Q1) and P2 starts executing. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 P3 P3 P2 0 2 4 6 8 9 11 12 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 12 \u2192 All process in Queue 1 is terminated P1 0 5 1(Q1) and P2 starts executing. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 P3 P3 P2 0 2 4 6 8 9 11 12 16 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 16 \u2192 All process in Queue 1 is terminated P1 0 5 1(Q1) and P4 starts executing. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1"
  },
  {
    "id": 27,
    "source": "Module 3.txt",
    "text": "Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 P3 P3 P2 0 2 4 6 8 9 11 12 16 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 16 \u2192 All process in Queue 1 is terminated P1 0 5 1(Q1) and P4 starts executing. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 P3 P3 P2 P4 0 2 4 6 8 9 11 12 16 19 Multi-Level Queue Scheduling - Example Process Arrival Time Burst Time Priority \u25aa At time 16 \u2192 All process in Queue 1 is terminated P1 0 5 1(Q1) and P4 starts executing. P2 1 4 2(Q2) P3 2 7 1(Q1) P4 3 3 2(Q2) Tail Head Ready Queue 1 P3(3) P1(1) P3(4) P1(3) P3 P1 Tail Head Ready Queue 2 P4 P2 Gantt P1 P3 P1 P3 P1 P3 P3 P2 P4 0 2 4 6 8 9 11 12 16 19 Multi-Level Queue Scheduling - Example \u25aa Metrics Calculation Finish Time/ Turnaround Time Waiting Time Response Time Priority and Completion Time (Completion Time \u2013 (Turnaround Time time until first Arrival CPU Process Ready Arrival Time) \u2013 Burst Time) scheduled \u2013 Arrival Time Burst Queue (for non-preemptive, RT = WT). 0 5 1(Q1) P1 9 9 \u2013 0 = 9 9 \u2013 5 = 4 0 \u2013 0 = 0 1 4 2(Q2) P2 16 16 \u2013 1 = 15 15 \u2013 4 = 11 12 \u2013 1 = 11 2 7 1(Q1) P3 12 12 \u2013 2 = 10 10 \u2013 7 = 3 2 \u2013 2 = 0 3 3 2(Q2) P4 19 19 \u2013 3 = 16 16 \u2013 3 = 13 16 \u2013 3 = 13 Average 12.5 7.75 6.0 Multi-Level Feedback Queue Scheduling \u2022 A process can move between the various queues. (It allows processes to move between different priority queues) \u2022 Multilevel-feedback-queue scheduler defined by the following parameters: \u2022 Number of queues \u2022 Scheduling algorithms for each queue \u2022 Method used to determine when to upgrade a process \u2022 Method used to determine when to demote a process \u2022 Method used to determine which queue a process will enter when that process needs service \u2022 Aging can be implemented using multilevel feedback queue Multi-Level Feedback Queue Scheduling \u2022 Three queues: \u2022 Q0 \u2013 RR with time quantum 8 milliseconds \u2022 Q1 \u2013 RR time quantum 16 milliseconds \u2022 Q2 \u2013 FCFS \u2022 Scheduling \u2022 A new process enters queue Q0 which is served in RR \u2022 When it gains CPU, the process receives 8 milliseconds \u2022 If it does not finish in 8 milliseconds, the process is moved to queue Q1 \u2022 At Q1 job is again served in RR and receives 16 additional milliseconds \u2022 If it still does not"
  },
  {
    "id": 28,
    "source": "Module 3.txt",
    "text": "process needs service \u2022 Aging can be implemented using multilevel feedback queue Multi-Level Feedback Queue Scheduling \u2022 Three queues: \u2022 Q0 \u2013 RR with time quantum 8 milliseconds \u2022 Q1 \u2013 RR time quantum 16 milliseconds \u2022 Q2 \u2013 FCFS \u2022 Scheduling \u2022 A new process enters queue Q0 which is served in RR \u2022 When it gains CPU, the process receives 8 milliseconds \u2022 If it does not finish in 8 milliseconds, the process is moved to queue Q1 \u2022 At Q1 job is again served in RR and receives 16 additional milliseconds \u2022 If it still does not complete, it is preempted and moved to queue Q2 Multi-Processors Scheduling \u25aa A Multi-processor is a system that has more than one processor but shares the same memory, bus, and input/output devices. \u25aa Multiprocessor scheduling is the method used to decide which processes should run when there are multiple processors available. \u25aa The main goal is to design a scheduling system that keeps all the processors as busy as possible, improving the overall performance and throughput of the system. Multi-Processors Scheduling \u25aa Why is it this Complex? \u25aa Load balancing is a problem since more than one processors are present. \u25aa Processes executing simultaneously may require access to shared data. \u25aa Cache affinity should be considered in scheduling. Approaches for Multiprocessor Scheduling Symmetric Asymmetric Multiprocessing Multiprocessing (SMP) (AMP) Multi-Processors Scheduling - AMP \u25aa In this approach, one processor acts as the \"master\" and the others are \"slaves.\" \u25aa The master processor is responsible for all scheduling decisions and assigning processes to the slave processors. \u25aa This is a simple approach because all the complex scheduling logic is handled by one processor, but it can create a bottleneck if the master processor gets overwhelmed. \u25aa If the master server goes down, the whole system comes to a halt. \u25aa However, if one of the slave servers goes down, the rest of the system keeps working. Multi-Processors Scheduling - SMP \u25aa This is the more common approach. \u25aa In SMP, each processor is self-scheduling. \u25aa All processes are kept in a common ready queue, and each processor independently picks a process to run from that queue. \u25aa This is more balanced because the scheduling work is distributed among all the processors. \u25aa However, it's more complex to design because the processors need to be careful not to pick the same process at the same time. Multi-Processors Scheduling - SMP \u25aa Pattern of Deployment \u25aa Symmetrical Scheduling with global queues: If the processes to be executed are in a common queue or a global queue, the scheduler for each processor checks this global- ready queue and selects a process to execute. \u25aa Symmetrical Scheduling with per queues: If the processors in the system have their own private ready queues, the scheduler for each processor checks their own private queue to select a process. Multi-Processors Scheduling - SMP \u25aa Processor Affinity \u25aa When a process runs on a processor, it builds up a cache"
  },
  {
    "id": 29,
    "source": "Module 3.txt",
    "text": "the same time. Multi-Processors Scheduling - SMP \u25aa Pattern of Deployment \u25aa Symmetrical Scheduling with global queues: If the processes to be executed are in a common queue or a global queue, the scheduler for each processor checks this global- ready queue and selects a process to execute. \u25aa Symmetrical Scheduling with per queues: If the processors in the system have their own private ready queues, the scheduler for each processor checks their own private queue to select a process. Multi-Processors Scheduling - SMP \u25aa Processor Affinity \u25aa When a process runs on a processor, it builds up a cache of data in that processor's memory. \u25aa If the process is moved to a different processor, this cache becomes invalid, and the new processor has to rebuild it. \u25aa This can be inefficient. \u25aa To avoid this, schedulers often try to keep a process running on the same processor. \u25aa This is called processor affinity. Multi-Processors Scheduling - SMP Soft Affinity Hard Affinity \u2022 The operating system tries to keep \u2022 The system allows a process to a process on the same processor specify a subset of processors on but doesn't guarantee it. which it can run, ensuring it never moves to a processor outside that \u2022 If the system needs to move the set. process for load balancing, it will. Multi-Processors Scheduling - SMP \u2022 Load Balancing \u2022 In systems with Symmetric Multiprocessing, it's possible for one processor to be very busy while another is idle. \u2022 Load balancing is the process of distributing the workload evenly across all processors to keep them all busy. This can be done in two ways: \u2022 Push Migration: A specific task periodically checks the load on each processor and, if it finds an imbalance, moves processes from overloaded processors to idle or less-busy ones. \u2022 Pull Migration: An idle processor pulls a waiting task from a busy processor to execute."
  },
  {
    "id": 30,
    "source": "M3_Deadlock.txt",
    "text": "Operating System Module \u2013 3 Deadlock Dr. Naveenkumar Jayakumar PRP \u2013 217 4 Deadlock \u25aa A deadlock in an operating system is a situation where two or more processes are permanently blocked because each process is waiting for a resource that is held by another process in the same set. How Process Consume Resource \u2022 A process requests a resource. If the Request resource is available, the system grants it. If not, the process waits. \u2022 The process utilizes the resource it has Use acquired. \u2022 The process releases the resource, making it Release available for other processes. Deadlock \u2013 System Model \u25aa Deadlocks are commonly modeled using a system resource-allocation graph. This directed graph consists of two types of nodes: \u25aa Processes (P): Represented by circles. \u25aa Resources (R): Represented by rectangles. \u25aa There are two types of directed edges in the graph: \u25aa Request Edge: A directed edge from a process Pi to a resource type Rj (Pi \u2192 Rj) indicates that process Pi has requested an instance of resource type Rj and is currently waiting for it. \u25aa Assignment Edge: A directed edge from a resource type Rj to a process Pi (Rj \u2192 Pi) indicates that an instance of resource type Rj has been allocated to process Pi. Deadlock \u2013 System Model \u25aa Within each resource type node Rj, there are dots representing each instance of that resource type. \u25aa An assignment edge from Rj to Pi originates from one of these dots, signifying that one instance of Rj is assigned to Pi. \u25aa A system is in a deadlock state if and only if the resource-allocation graph contains a cycle. \u25aa If the graph contains no cycles, then no process is deadlocked. \u25aa If a cycle exists, a deadlock may occur, depending on whether there are multiple instances of a resource type. \u25aa If each resource type has exactly one instance, then a cycle implies a deadlock. \u25aa If there are multiple instances, a cycle indicates the possibility of a deadlock, but not necessarily a guaranteed deadlock. Deadlock \u2013 Conditions \u25aa A deadlock in an operating system occurs when four specific conditions, often called the Coffman conditions, are simultaneously present. \u2022 This condition states that at least one resource must be non-shareable, meaning only Mutual Exclusion one process can use it at any given time. \u2022 If another process requests that resource, it must wait until the resource is released. \u2022 A process is holding at least one resource and is simultaneously waiting to acquire additional resources that are currently held by other processes. Hold and Wait \u2022 For example, Process 1 holds Resource A and is waiting for Resource B, which is held by Process 2. \u2022 Resources cannot be forcibly taken away from a process. No Preemption \u2022 A resource can only be released voluntarily by the process that is holding it, typically after the process has completed its task. \u2022 This condition exists when a set of processes are waiting for each"
  },
  {
    "id": 31,
    "source": "M3_Deadlock.txt",
    "text": "until the resource is released. \u2022 A process is holding at least one resource and is simultaneously waiting to acquire additional resources that are currently held by other processes. Hold and Wait \u2022 For example, Process 1 holds Resource A and is waiting for Resource B, which is held by Process 2. \u2022 Resources cannot be forcibly taken away from a process. No Preemption \u2022 A resource can only be released voluntarily by the process that is holding it, typically after the process has completed its task. \u2022 This condition exists when a set of processes are waiting for each other in a circular fashion. For instance, Process P1 is waiting for a resource held by P2, P2 is waiting Circular Wait for a resource held by P3, and so on, until the last process Pn is waiting for a resource held by P1. This forms a closed chain where each process needs a resource held by another process in the cycle. Deadlock \u2013 Management \u25aa It refers to the set of strategies an operating system uses to handle deadlocks. \u25aa These strategies aim to either prevent deadlocks from occurring, or to detect them and recover the system if they do occur. Deadlock \u2013 Management \u25aa Deadlock Prevention \u25aa This is the most stringent method, which focuses on ensuring that deadlocks are structurally impossible. \u25aa It works by making sure that at least one of the four necessary conditions for a deadlock (mutual exclusion, hold and wait, no preemption, or circular wait) can never happen. \u25aa By preventing one of these conditions, the system can guarantee that a deadlock will never occur. Deadlock \u2013 Management - Avoidance \u25aa Deadlock Avoidance \u25aa In this strategy, the operating system uses an algorithm to avoid entering a state that could lead to a deadlock. \u25aa It requires the system to have advance information, such as the maximum resources each process will need, to decide if a resource allocation request is safe. \u25aa The system must remain in a safe state, where there is at least one execution sequence for all processes that does not result in a deadlock. \u25aa A well-known example of a deadlock avoidance algorithm is the Banker's Algorithm. Deadlock \u2013 Management - Detection \u25aa Deadlock Detection \u25aa This approach allows deadlocks to happen, periodically checks for them, and then takes action to resolve them. \u25aa Detection: The system uses an algorithm, such as analyzing a resource-allocation graph or a wait-for graph, to determine if a deadlock has occurred. Deadlock \u2013 Management - Recovery \u25aa Recovery Once a deadlock is detected, the system can use one of several methods to recover: \u25aa Process Termination \u25aa Aborting one or more of the processes involved in the deadlock cycle. \u25aa Resource Preemption \u25aa Forcibly taking a resource from one process and giving it to another. \u25aa Rollback \u25aa Returning the deadlocked processes to a previous safe state that was saved at a checkpoint. Deadlock \u2013 Management - Ignoring \u25aa Ignoring Deadlocks \u25aa Also"
  },
  {
    "id": 32,
    "source": "M3_Deadlock.txt",
    "text": "algorithm, such as analyzing a resource-allocation graph or a wait-for graph, to determine if a deadlock has occurred. Deadlock \u2013 Management - Recovery \u25aa Recovery Once a deadlock is detected, the system can use one of several methods to recover: \u25aa Process Termination \u25aa Aborting one or more of the processes involved in the deadlock cycle. \u25aa Resource Preemption \u25aa Forcibly taking a resource from one process and giving it to another. \u25aa Rollback \u25aa Returning the deadlocked processes to a previous safe state that was saved at a checkpoint. Deadlock \u2013 Management - Ignoring \u25aa Ignoring Deadlocks \u25aa Also known as the Ostrich Algorithm, this approach involves simply ignoring the problem. \u25aa The operating system assumes that deadlocks will occur so infrequently that the performance overhead of implementing prevention, avoidance, or detection is not worthwhile. \u25aa Many general-purpose operating systems, including variants of UNIX and Windows, use this approach, as it is often cheaper to reboot the system than to implement complex deadlock-handling mechanisms. Deadlock \u2013 Management \u2013 Prevention - Strategy \u25aa To prevent deadlocks, the operating system must ensure that at least one of the four necessary conditions\u2014Mutual Exclusion, Hold and Wait, No Preemption, or Circular Wait\u2014can never hold true. Deadlock - Prevention \u2013 Strategy \u2013 Eliminating Mutual Exclusion \u25aa Making Resources Shareable The most direct way to eliminate mutual exclusion is to allow multiple processes to access a resource simultaneously. \u25aa This works for resources that can be shared without conflict. \u25aa For example, a file can be simultaneously accessed by multiple processes if they are all only reading from it. Since no process has to wait for another to finish, mutual exclusion is not enforced, and that resource cannot be part of a deadlock. However, if any process needs to write to the file, exclusive access is required. Deadlock - Prevention \u2013 Strategy \u2013 Eliminating Mutual Exclusion \u25aa Spooling For devices that are inherently non-shareable, like printers, a technique called spooling (Simultaneous Peripheral Operations Online) can be used. \u25aa Instead of giving a process exclusive control of the printer, its print job is sent to a queue in a special disk area or memory buffer called a spool. \u25aa A separate system process, often called a printer daemon, is the only process that has direct access to the printer. \u25aa It reads the jobs from the queue and prints them one by one, typically in a first- come, first-served order. Deadlock - Prevention \u2013 Strategy \u2013 Eliminating Hold & Wait \u25aa Request all resources at the start A process must request and be allocated all the resources it will need before its execution begins. \u25aa This approach ensures that the process will never be in a state of holding some resources while waiting for others. \u25aa The main disadvantage of this method is that it can lead to low resource utilization, as resources may be allocated long before they are actually needed. It is also often impractical because it's difficult for a process to know all"
  },
  {
    "id": 33,
    "source": "M3_Deadlock.txt",
    "text": "first- come, first-served order. Deadlock - Prevention \u2013 Strategy \u2013 Eliminating Hold & Wait \u25aa Request all resources at the start A process must request and be allocated all the resources it will need before its execution begins. \u25aa This approach ensures that the process will never be in a state of holding some resources while waiting for others. \u25aa The main disadvantage of this method is that it can lead to low resource utilization, as resources may be allocated long before they are actually needed. It is also often impractical because it's difficult for a process to know all its required resources in advance. Deadlock - Prevention \u2013 Strategy \u2013 Eliminating Hold & Wait \u25aa Release resources before new requests \u25aa This protocol requires a process to release all the resources it currently holds before it can request any additional ones. \u25aa After releasing its resources, the process can then attempt to re-acquire its old resources along with the new ones it needs. \u25aa While this prevents the hold and wait condition, it is often inefficient. \u25aa It can also lead to a problem known as starvation, where a process that needs a popular resource may have to wait indefinitely because the resource is always allocated to some other Deadlock - Prevention \u2013 Strategy \u2013 Eliminating No Preemption \u25aa Preempt on Request Failure \u25aa If a process holding some resources requests another resource that cannot be immediately allocated, the operating system can preempt all the resources currently held by that process. \u25aa These preempted resources are then added to the list of resources for which the process is waiting. \u25aa The process is only restarted when it can regain its old resources along with the newly requested ones. \u25aa Preempt from Waiting Processes \u25aa If a process requests a resource that is currently held by another process, and that other process is itself blocked and waiting for a different resource, the system can preempt the resource from the waiting process. \u25aa The resource is then allocated to the requesting process. Deadlock - Prevention \u2013 Strategy \u2013 Eliminating Circular Wait \u25aa Assign a unique number to each resource type: \u25aa Every resource in the system is assigned a unique integer value or priority. \u25aa Enforce an ordered resource allocation policy: \u25aa A process is only allowed to request resources in a strictly increasing order of their assigned numbers. For example, if a process is holding resource R(i), it can only request a resource R(j) if the number assigned to R(j) is higher than the number assigned to R(i). \u25aa Release higher-numbered resources if a lower one is needed: If a process holds a resource and needs to request another resource with a lower assigned number, it must first release the higher-numbered resource it currently holds. Deadlock - Detection \u2013 Strategy \u25aa Single Instance of Each Resource Type: \u25aa If every resource type has only one instance, deadlock detection is straightforward. \u25aa The system uses a Resource Allocation Graph (RAG) to"
  },
  {
    "id": 34,
    "source": "M3_Deadlock.txt",
    "text": "if a process is holding resource R(i), it can only request a resource R(j) if the number assigned to R(j) is higher than the number assigned to R(i). \u25aa Release higher-numbered resources if a lower one is needed: If a process holds a resource and needs to request another resource with a lower assigned number, it must first release the higher-numbered resource it currently holds. Deadlock - Detection \u2013 Strategy \u25aa Single Instance of Each Resource Type: \u25aa If every resource type has only one instance, deadlock detection is straightforward. \u25aa The system uses a Resource Allocation Graph (RAG) to model the state. \u25aa A deadlock exists if and only if the graph contains a cycle. The presence of a cycle is a sufficient condition for a deadlock in this scenario. \u25aa Multiple Instances of Resource Types: \u25aa When resource types have multiple instances, the detection method is more complex. \u25aa A cycle in the Resource Allocation Graph is a necessary but not sufficient condition for a deadlock. \u25aa A cycle indicates that a deadlock might exist, but it is not guaranteed. \u25aa Therefore, the system must use more advanced algorithms to determine if a deadlock has actually occurred. Deadlock - Detection \u2013 Algorithm \u25aa Wait-For Graph \u25aa It shows only the dependencies between processes. \u25aa The nodes in the graph represent the processes in the system. \u25aa A directed edge from process Pi to Pj (Pi \u2192 Pj) exists if process Pi is waiting for a resource that is currently held by process Pj. \u25aa A deadlock is present in the system if and only if the wait-for graph contains a cycle. \u25aa The system must periodically invoke an algorithm that searches for cycles in this graph to detect deadlocks. Deadlock - Detection \u2013 Algorithm \u25aa Wait-For Graph \u25aa Example 1 For the given resource allocation Graph create wait for graph Deadlock - Detection \u2013 Algorithm \u25aa Wait-For Graph \u25aa Example 1 wait for graph Deadlock - Detection \u2013 Algorithm \u25aa Detection Algorithm using Resource Matrices \u25aa This algorithm, which is a variant of the Banker's Algorithm, is used for systems with multiple instances of resources. \u25aa It uses several data structures to analyze the state of the system: \u25aa Available: A vector indicating the number of available resources of each type. \u25aa Allocation: A matrix defining the number of resources of each type currently allocated to each process. \u25aa Request: A matrix indicating the current resource requests of each process. \u25aa The algorithm works by checking if there is a sequence of processes that can complete their execution with the available resources. \u25aa If no such sequence exists, the system is in a deadlock state. \u25aa This requires O(m x n\u00b2) operations, where m is the number of resource types and n is the number of processes. Deadlock - Detection \u2013 Algorithm \u25aa Multi Instance of Each Resource Type \u25aa Example 1 - Consider the given scenario where, \u25aa 1 instance of R1 is held by P1 \u25aa"
  },
  {
    "id": 35,
    "source": "M3_Deadlock.txt",
    "text": "\u25aa Request: A matrix indicating the current resource requests of each process. \u25aa The algorithm works by checking if there is a sequence of processes that can complete their execution with the available resources. \u25aa If no such sequence exists, the system is in a deadlock state. \u25aa This requires O(m x n\u00b2) operations, where m is the number of resource types and n is the number of processes. Deadlock - Detection \u2013 Algorithm \u25aa Multi Instance of Each Resource Type \u25aa Example 1 - Consider the given scenario where, \u25aa 1 instance of R1 is held by P1 \u25aa P1 is requesting for R2 \u25aa 1 instance of R2 is held by P2 \u25aa P2 is requesting for 1 Instance R1 \u25aa 1 Instance of R2 is held by P3. Draw the resource allocation graph for the scenario. Identify whether the scenario leads to deadlock otherwise find a safe sequence. Deadlock - Detection \u2013 Algorithm \u25aa Example 1 \u25aa The given resource allocation graph is multi-instance with a cycle contained in it. \u25aa The system may or may not be in a deadlock state. Deadlock - Detection \u2013 Algorithm \u25aa Example 1 - Detection Algorithm using Resource Matrices \u25aa Construct a table Process Resource Allocation Resource Need R1 R2 R1 R2 P1 1 0 0 1 P2 0 1 1 0 P3 0 1 0 0 \u25aa Available Resource \u2013 R1=0, R2=0 Deadlock - Detection \u2013 Algorithm \u25aa Example 1 - Detection Algorithm using Resource Matrices \u25aa P3 executes and releases its resource then the table changes Process Resource Allocation Resource Need R1 R1 R1 R2 P1 1 0 0 1 P2 0 1 1 0 P3 0 0 0 0 \u25aa Available Resource \u2013 R1=0, R2=1 Deadlock - Detection \u2013 Algorithm \u25aa Example 1 - Detection Algorithm using Resource Matrices \u25aa With available resources only P1 can be satisfied. \u25aa P1 is allocated with R1 resource, it completes execution and releases its resources. Process Resource Allocation Resource Need R1 R2 R1 R2 P1 0 0 0 0 P2 0 1 1 0 P3 0 0 0 0 \u25aa Available Resource \u2013 R1=1, R2=1 Deadlock - Detection \u2013 Algorithm \u25aa Example 1 - Detection Algorithm using Resource Matrices \u25aa Now with available P2 can be satisfied. \u25aa P2 is allocated with R1 resource, it completes execution and releases its resources. Process Resource Allocation Resource Need R1 R2 R1 R2 P1 0 0 0 0 P2 0 0 0 0 P3 0 0 0 0 \u25aa Available Resource \u2013 R1=1, R2=2, There exists a safe sequence P3, P1, P2 in which all the processes can be executed. Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Consider the following scenario and identify whether the system in deadlock, otherwise find a safe sequence. \u2022 There are R1- 2 instances, R2 \u2013 3 instances, R3 \u2013 2 Instances \u2022 P1 requests R1 \u2022 1 instance of R1 is allocated to P1 \u2022 1 instance of R1"
  },
  {
    "id": 36,
    "source": "M3_Deadlock.txt",
    "text": "P1 0 0 0 0 P2 0 0 0 0 P3 0 0 0 0 \u25aa Available Resource \u2013 R1=1, R2=2, There exists a safe sequence P3, P1, P2 in which all the processes can be executed. Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Consider the following scenario and identify whether the system in deadlock, otherwise find a safe sequence. \u2022 There are R1- 2 instances, R2 \u2013 3 instances, R3 \u2013 2 Instances \u2022 P1 requests R1 \u2022 1 instance of R1 is allocated to P1 \u2022 1 instance of R1 is allocated to P0 \u2022 P0 requests R2 \u2022 1 instance of R2 is allocated to P1 \u2022 1 instance of R2 is allocated to P2 \u2022 1 instance of R2 is allocated to P3 \u2022 P3 requests 2 instances of R2 \u2022 1 instance of R3 is allocated to P0 \u2022 P0 requests 1 instance of R3 \u2022 P2 Requests 1 instance of R3 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices The given resource allocation graph is multi-instance with a cycle contained in it. So, the system may or may not be in a deadlock state Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Constructing table - Available = 0 0 1 Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 1 0 1 0 1 1 P1 1 1 0 1 0 0 P2 0 1 0 0 0 1 P3 0 1 0 0 2 0 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Available = 0 0 1 => 0 1 1 \u25aa only the requirement of the process P2 can be satisfied. So, process P2 is allocated the requested resources. It completes its execution and then free up the instances of resources held by it. Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 1 0 1 0 1 1 P1 1 1 0 1 0 0 P2 0 1 0 0 0 1 P3 0 1 0 0 2 0 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Available = 0 1 1 => 1 1 2 \u25aa only the requirement of the process P0 can be satisfied. So, process P0 is allocated the requested resources. It completes its execution and then free up the instances of resources held by it. Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 0 0 0 0 0 0 P1 1 1 0 1 0 0 P2 0 0 0 0 0 0 P3 0 1 0 0 2 0 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Available = 1 1 2 => 2 2 2 \u25aa only the requirement of the process P1 can be satisfied. So, process P1 is allocated"
  },
  {
    "id": 37,
    "source": "M3_Deadlock.txt",
    "text": "P0 is allocated the requested resources. It completes its execution and then free up the instances of resources held by it. Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 0 0 0 0 0 0 P1 1 1 0 1 0 0 P2 0 0 0 0 0 0 P3 0 1 0 0 2 0 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Available = 1 1 2 => 2 2 2 \u25aa only the requirement of the process P1 can be satisfied. So, process P1 is allocated the requested resources. It completes its execution and then free up the instances of resources held by it Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 0 0 0 0 0 0 P1 0 0 0 0 0 0 P2 0 0 0 0 0 0 P3 0 1 0 0 2 0 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Available = 2 2 2 => 2 3 2 \u25aa only the requirement of the process P3 can be satisfied. So, process P3 is allocated the requested resources. It completes its execution and then free up the instances of resources held by it. Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 0 0 0 0 0 0 P1 0 0 0 0 0 0 P2 0 0 0 0 0 0 P3 0 0 0 0 0 0 Deadlock - Detection \u2013 Algorithm \u25aa Example 2 - Detection Algorithm using Resource Matrices \u25aa Available = 2 2 2 => 2 3 2 \u25aa There exists a safe sequence P2, P0, P1, P3 in which all the processes can be executed. So, the system is in a safe state. Process Resource Allocation Resource Need R1 R2 R3 R1 R2 R3 P0 0 0 0 0 0 0 P1 0 0 0 0 0 0 P2 0 0 0 0 0 0 P3 0 0 0 0 0 0 Deadlock - Avoidance \u2013 Algorithm \u25aa Deadlock Avoidance in operating systems is a way to make sure that when resources are given to processes, the system never ends up in a situation where processes get stuck waiting for each other forever. \u25aa Operating system checks every time a process requests a resource to see if giving out that resource might cause a deadlock now or later. If there\u2019s even a small chance of deadlock, the OS waits and doesn\u2019t give out the resource right away, keeping the system safe. \u25aa Working Concept: \u25aa Each process must tell the OS its maximum resource needs ahead of time. \u25aa Before granting a resource request, the OS simulates whether, after giving out resources, the system can still let every process finish if it asks for its maximum. \u25aa If it\u2019s always possible for all processes to finish (a safe state), the OS gives the resource. If not, it waits. Deadlock"
  },
  {
    "id": 38,
    "source": "M3_Deadlock.txt",
    "text": "that resource might cause a deadlock now or later. If there\u2019s even a small chance of deadlock, the OS waits and doesn\u2019t give out the resource right away, keeping the system safe. \u25aa Working Concept: \u25aa Each process must tell the OS its maximum resource needs ahead of time. \u25aa Before granting a resource request, the OS simulates whether, after giving out resources, the system can still let every process finish if it asks for its maximum. \u25aa If it\u2019s always possible for all processes to finish (a safe state), the OS gives the resource. If not, it waits. Deadlock - Avoidance \u2013 Algorithm \u25aa SAFE STATE \u25aa A safe state in an operating system is a condition where the system can allocate resources to every process (up to their maximum needs) in some order\u2014called a safe sequence\u2014so that all processes can finish without leading to deadlock. \u25aa The OS checks if resources can be given out in such a way that, for every process, either it can finish now or wait for others to finish and then get its resources \u25aa There must exist at least one sequence where each process can finish, get its job done, release its resources, and allow other waiting processes to proceed. \u25aa If no such sequence exists, the system is in an unsafe state, which could lead to deadlock, but not always. Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa The Banker's Algorithm is a resource allocation and deadlock avoidance algorithm in operating systems that ensures the system always remains in a safe state when granting resources to multiple processes. \u25aa Banker's Algorithm checks whether it is safe to grant resource requests made by processes, considering both current allocations and future maximum demands. \u25aa The OS acts like a cautious banker who only allocates resources if doing so will never put the system at risk of deadlock. Banker\u2019s Algorithm Resource Safety Check Request Algorithm Algorithm Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Key Data Structures \u25aa Available: 1D array\u2014each element shows the amount of each resource type currently available. \u25aa Max: 2D array\u2014maximum number of resources each process could ever request. \u25aa Allocation: 2D array\u2014resources currently allocated to each process. \u25aa Need: 2D array\u2014resources each process still needs to finish (Need = Max - Allocation) Deadlock - Avoidance \u2013 Banker\u2019s Algorithm Steps of the Banker's Algorithm Safety Resource Request Safe Check: Decision: Sequence: Before allocating If a possible order requested resources, exists where every the OS simulates the If granting the request If not, the OS makes process can complete allocation to see if it keeps the system in a the process wait until with available and can find a sequence safe state, the OS it is safe to proceed. future-released (safe sequence) where grants it. resources, the all processes can allocation is safe. finish. Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Consider the current system where resources R1, R2 and R3 respectively are consumed by Processes P0 to P4. The total number of"
  },
  {
    "id": 39,
    "source": "M3_Deadlock.txt",
    "text": "allocating If a possible order requested resources, exists where every the OS simulates the If granting the request If not, the OS makes process can complete allocation to see if it keeps the system in a the process wait until with available and can find a sequence safe state, the OS it is safe to proceed. future-released (safe sequence) where grants it. resources, the all processes can allocation is safe. finish. Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Consider the current system where resources R1, R2 and R3 respectively are consumed by Processes P0 to P4. The total number of Instances of each resource in the system is given as R1=10, R2=5 and R3=7. Apply Banker\u2019s algorithm to check whether the system is in safe state and find the safe sequence. RESOURCE Process MAXIMUM RESOURCE ALLOCATION R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm RESOURCE MAXIMUM \u25aa Step 1 - Calculate the current Available resources Process ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 \u25aa R1 = 10 \u2013 7(2+3+2) = 3 P0 0 1 0 7 5 3 P1 2 0 0 3 2 2 \u25aa R2 = 5 \u2013 2(1+1) = 3 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 \u25aa R3 = 7 \u2013 5(2+1+2) = 2 P4 0 0 2 4 3 3 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm RESOURCE MAXIMUM \u25aa Step 2 - Calculate the Resource Need of every Process Process ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 \u25aa Need = Max Resource \u2013 Resource Allocation P0 0 1 0 7 5 3 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 RESOURCE MAXIMUM Process NEED RESOURCES ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 7 4 3 P1 2 0 0 3 2 2 1 2 2 P2 3 0 2 9 0 2 6 0 0 P3 2 1 1 2 2 2 0 1 1 P4 0 0 2 4 3 3 4 3 1 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Step 3 - Compare All Available Resources with All RESOURCE MAXIMUM Process ALLOCATION RESOURCE Need Resources, if All Need is less than or equal to All R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 Available then we can allocate else check with next P1 2 0 0 3 2 2 process P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 Is R1, R2 & R3 RESOURCE MAXIMUM NEED Process can be ALLOCATION RESOURCE RESOURCES allocated? R1 R2"
  },
  {
    "id": 40,
    "source": "M3_Deadlock.txt",
    "text": "- Avoidance \u2013 Banker\u2019s Algorithm \u25aa Step 3 - Compare All Available Resources with All RESOURCE MAXIMUM Process ALLOCATION RESOURCE Need Resources, if All Need is less than or equal to All R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 Available then we can allocate else check with next P1 2 0 0 3 2 2 process P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 Is R1, R2 & R3 RESOURCE MAXIMUM NEED Process can be ALLOCATION RESOURCE RESOURCES allocated? R1 R2 R3 R1 R2 R3 R1 R2 R3 Available - 3 3 2 P0 0 1 0 7 5 3 7 4 3 No P1 2 0 0 3 2 2 1 2 2 Yes P2 3 0 2 9 0 2 6 0 0 P3 2 1 1 2 2 2 0 1 1 P4 0 0 2 4 3 3 4 3 1 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Step 4 \u2013 Process Sequence Noting: RESOURCE MAXIMUM Process ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 \u25aa P1 -> P0 0 1 0 7 5 3 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 (After Allocation & Process RESOURCE MAXIMUM Allocation Process NEED termination) Resource ALLOCATION RESOURCE Possible Available Status R1 R2 R3 R1 R2 R3 R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 7 4 3 No P1 2 0 0 3 2 2 1 2 2 Yes 5 3 2 P2 3 0 2 9 0 2 6 0 0 No P3 2 1 1 2 2 2 0 1 1 P4 0 0 2 4 3 3 4 3 1 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Step 4 \u2013 Process Sequence Noting: RESOURCE MAXIMUM Process ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 \u25aa P1 \u2192 P3 \u2192 P0 0 1 0 7 5 3 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 (After Allocation & Process RESOURCE MAXIMUM Allocation Process NEED termination) Resource ALLOCATION RESOURCE Possible Available Status R1 R2 R3 R1 R2 R3 R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 7 4 3 No P1 2 0 0 3 2 2 1 2 2 Yes 5 3 2 P2 3 0 2 9 0 2 6 0 0 No P3 2 1 1 2 2 2 0 1 1 Yes 7 4 3 P4 0 0 2 4 3 3 4 3 1 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Step 4 \u2013 Process Sequence Noting: RESOURCE MAXIMUM Process ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 \u25aa P1 \u2192 P3 \u2192 P4 \u2192 P0 \u2192 P2 P0 0 1 0"
  },
  {
    "id": 41,
    "source": "M3_Deadlock.txt",
    "text": "R2 R3 P0 0 1 0 7 5 3 7 4 3 No P1 2 0 0 3 2 2 1 2 2 Yes 5 3 2 P2 3 0 2 9 0 2 6 0 0 No P3 2 1 1 2 2 2 0 1 1 Yes 7 4 3 P4 0 0 2 4 3 3 4 3 1 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm \u25aa Step 4 \u2013 Process Sequence Noting: RESOURCE MAXIMUM Process ALLOCATION RESOURCE R1 R2 R3 R1 R2 R3 \u25aa P1 \u2192 P3 \u2192 P4 \u2192 P0 \u2192 P2 P0 0 1 0 7 5 3 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 (After Allocation & Process RESOURCE MAXIMUM Allocation Process NEED termination) Resource ALLOCATION RESOURCE Possible Available Status R1 R2 R3 R1 R2 R3 R1 R2 R3 R1 R2 R3 P0 0 1 0 7 5 3 7 4 3 Yes 7 5 5 P1 2 0 0 3 2 2 1 2 2 Yes 5 3 2 P2 3 0 2 9 0 2 6 0 0 Yes 10 5 7 P3 2 1 1 2 2 2 0 1 1 Yes 7 4 3 P4 0 0 2 4 3 3 4 3 1 Yes 7 4 5 Deadlock - Avoidance \u2013 Banker\u2019s Algorithm Resource Request Algorithm Safety Check Algorithm If Request <= Need Work = Available { Finish[i] = FALSE for i=0 \u2026 n-1 If Request <= Available Find such i that { Finish[i] = FALSE && Need <= Work Allocation = Allocation + request Work = Work + Allocation Available = Available \u2013 Request Need = Need \u2013 Request Finish[i] = TRUE } If ( Finish[i] = TRUE for all i = 0 \u2026 n-1) } Then system is in Safe state. Deadlock - Avoidance \u2013 Banker\u2019s Algorithm Example 2: Consider the process table with number of processes that contains: Process Allocation MAX R1 R2 R3 R1 R2 R3 \u25aa Allocation field (for showing the number of resources of P0 1 1 2 5 4 4 type: R1, R2 and R3 allocated to each process in the table), P1 2 1 2 4 3 3 \u25aa Max field (for showing the maximum number of resources of P2 3 0 1 9 1 3 type: A, B, and C that can be allocated to each process). P3 0 2 0 8 6 4 P4 1 1 2 2 2 3 \u25aa Available resources are: R1=3, R2=2, R1=1 \u25aa Find whether the Deadlock can be avoided using banker\u2019s Algorithm? Deadlock - Recovery \u2013 Banker\u2019s Algorithm \u25aa Process Termination \u25aa Abort all deadlocked threads \u25aa Abort one process at a time until the deadlock cycle is eliminated \u25aa In which order should we choose to abort? \u25aa Priority of the thread \u25aa How long has the thread computed, and how much longer to completion \u25aa Resources that the thread has"
  },
  {
    "id": 42,
    "source": "M3_Deadlock.txt",
    "text": "C that can be allocated to each process). P3 0 2 0 8 6 4 P4 1 1 2 2 2 3 \u25aa Available resources are: R1=3, R2=2, R1=1 \u25aa Find whether the Deadlock can be avoided using banker\u2019s Algorithm? Deadlock - Recovery \u2013 Banker\u2019s Algorithm \u25aa Process Termination \u25aa Abort all deadlocked threads \u25aa Abort one process at a time until the deadlock cycle is eliminated \u25aa In which order should we choose to abort? \u25aa Priority of the thread \u25aa How long has the thread computed, and how much longer to completion \u25aa Resources that the thread has used \u25aa Resources that the thread needs to complete \u25aa How many threads will need to be terminated \u25aa Is the thread interactive or batch? Deadlock - Recovery \u2013 Banker\u2019s Algorithm \u25aa Resource Preemption \u25aa Selecting a victim \u2013 minimize cost \u25aa Rollback \u2013 return to some safe state, restart the thread for that state \u25aa Starvation \u2013 same thread may always be picked as victim, include number of rollback in cost factor Deadlock - Summary A situation in OS where two or more processes are permanently waiting for each Deadlock other to release resources, causing all to be stuck indefinitely. Deadlock Four necessary conditions for deadlock: Mutual Exclusion, Hold and Wait, No Conditions Preemption, Circular Wait. Deadlock Strategies ensuring that at least one of the deadlock conditions never holds, Prevention thus avoiding deadlock. Common algorithms include Wait-Die, Wound-Wait. The OS dynamically checks resource allocation to avoid unsafe states, ensuring Deadlock system remains in a safe state. The Banker\u2019s Algorithm is the most well-known Avoidance method. Deadlock Allows deadlocks to occur but detects them using algorithms like Wait-For Detection Graph and takes corrective actions to recover. Deadlock - Summary Banker\u2019s A deadlock avoidance algorithm that checks the \"safe state\" by simulating Algorithm allocation requests, granting them only if it keeps the system safe. Wait-Die A deadlock prevention method where older processes wait for younger ones, but Algorithm younger processes requesting older process resources get rolled back (die). Wound- A deadlock prevention approach where older processes preempt (wound) Wait younger ones holding needed resources; younger ones wait if resource is held Algorithm by older process."
  },
  {
    "id": 43,
    "source": "Module 2.txt",
    "text": "Operating System BCSE303L Module 2 Dr. Naveenkumar Jayakumar Associate Professor Department of Computational Intelligence PRP 217-4 naveenkumar.jk@vit.ac.in System Calls \u2751 An operating system (OS) acts as the fundamental intermediary between computer hardware and the software that runs on it. \u2751 At the heart of this interaction lies a crucial mechanism known as a system call. \u2751 A system call is a programmatic way for a running program to request a service from the operating system's kernel System Calls \u2751 An operating system provides a layer of abstraction, managing the computer's resources and offering a consistent interface to applications. \u2751 For security and stability, modern operating systems employ a dual- mode operation: user mode and kernel mode. System Calls User Mode Kernel Mode \u2022 This is the non-privileged mode \u2022 This is the privileged mode where where most applications run. the operating system kernel executes. \u2022 In this mode, a program has restricted access to system \u2022 In this mode, the code has resources and cannot directly complete access to all hardware interact with hardware. and system resources. System Calls \u2022 System calls provide a controlled and secure bridge between these two modes. \u2022 When an application needs to perform a privileged action, such as reading from a file or opening a network connection, it must request the operating system's kernel to perform the task on its behalf. \u2022 This request is a system call. System Calls - Working Call to Library Function TRAP/INT Instruction System call definition look (Switch to Kernel Mode) up User Application (User C Library (glibc, etc.) Kernel: Trap System Call Table Mode) \u2022 Wrapper: prepare args, Set \u2022 Save user context, \u2022 Look up kernel function \u2022 fopen(\"file.txt\", \"r\"); syscall number, (e.g., (registers, PC, etc.), Find (e.g., sys_open()) SYS_open) ISR / System Call Table Execute Kernel Function User Application (User Kernel: Restore Context Device Driver / File Kernel Function Mode) System (sys_open) \u2022 Return from TRAP, Switch \u2022 Continue execution (e.g., fd back to User Mode \u2022 Read from disk , Use \u2022 Check permissions, Locate = 3 returned) MMU, I/O controllers file on disk, Allocate file descriptor, Interact with file system Resume User Application Return Value to the User Hardware Interaction by Kernel System Calls - Working \u2751 The execution of a system call involves a well-defined sequence of steps that facilitates the transition from user mode to kernel mode and back. \u2751 Application Initiates the Call: A user program, written in a high- level language like C++ or Python, makes a call to a library function (e.g., fopen() to open a file). This library function is part of the Application Programming Interface (API) provided by the system. System Calls - Working \u2751The Library Function Invokes the System Call: \u2751The library function is a wrapper that contains the necessary code to prepare for the actual system call. \u2751This preparation involves placing the system call number (a unique integer identifying the requested service) and its arguments (e.g., the filename and the mode of access) into specific"
  },
  {
    "id": 44,
    "source": "Module 2.txt",
    "text": "the Call: A user program, written in a high- level language like C++ or Python, makes a call to a library function (e.g., fopen() to open a file). This library function is part of the Application Programming Interface (API) provided by the system. System Calls - Working \u2751The Library Function Invokes the System Call: \u2751The library function is a wrapper that contains the necessary code to prepare for the actual system call. \u2751This preparation involves placing the system call number (a unique integer identifying the requested service) and its arguments (e.g., the filename and the mode of access) into specific registers or a stack in memory. System Calls - Working \u2751 The TRAP Instruction: The library function then executes a special instruction often called a TRAP or INT (interrupt). \u2751 This instruction causes a software interrupt, which is a signal to the processor to switch from user mode to kernel mode. System Calls - Working \u2751The Kernel Takes Over: Upon receiving the trap, the processor saves the current state of the user program (including the program counter and registers) and jumps to a specific location in the kernel's memory. \u2751 This location is the starting address of the Interrupt Service Routine (ISR) or System Call Handler. System Calls - Working \u2751 Executing the System Call: The system call handler uses the system call number to look up the corresponding kernel function in a system call table. \u2751This table maps each system call number to the address of the kernel code that implements that service. System Calls - Working \u2751 Performing the Operation: The kernel then executes the requested operation. \u2751For instance, if the system call was for opening a file, the kernel would check file permissions, locate the file on the storage device, and create an entry in the system-wide open file table. System Calls - Working \u2751 Returning to User Mode: Once the kernel has completed the operation, it places the return value (e.g., a file descriptor or an error code) in a designated register. \u2751 It then restores the saved state of the user program and executes a special return-from-interrupt instruction. System Calls - Working \u2751 Resuming Application Execution: This instruction switches the processor back from kernel mode to user mode, and the execution of the user program resumes from where it left off, now with the result of the system call available. System Calls - Working System Calls Interface / API \u2751A set of functions, system calls and protocols that allows applications to interact with the OS and other software components. \u2751It defines the way that applications request services from the OS \u2751The API provides a level of abstraction between the application and the underlying OS, simplifying development and ensuring compatibility and security. System Calls Interface / API \u2022 A number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers. \u2022 The system-call interface then invokes the intended system call in the operating-system kernel and returns the status"
  },
  {
    "id": 45,
    "source": "Module 2.txt",
    "text": "Calls Interface / API \u2751A set of functions, system calls and protocols that allows applications to interact with the OS and other software components. \u2751It defines the way that applications request services from the OS \u2751The API provides a level of abstraction between the application and the underlying OS, simplifying development and ensuring compatibility and security. System Calls Interface / API \u2022 A number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers. \u2022 The system-call interface then invokes the intended system call in the operating-system kernel and returns the status of the system call. \u2022 The caller need know nothing about how the system call is implemented \u2022 Just needs to obey API and understand what OS will do as a result call \u2022 Most details of OS interface hidden from programmer by API \u2022 Managed by run-time support library (set of functions built into libraries included with compiler) System Calls - Importance \u2022 System calls provide a standardized way for programs to interact with hardware, abstracting Hardware Abstraction the complexities and ensuring hardware- independent operation. \u2022 They enable efficient management and sharing Resource Management of hardware resources like CPU, memory, and I/O devices. \u2022 System calls allow the OS to enforce security policies, ensuring that programs operate within Security and Protection their allocated permissions and preventing unauthorized access to critical resources. Parameter Passing between Programs & Kernel Parameter Passing between Programs & Kernel \u2751 Using Registers (Limited Space) \u2751 This is the simplest method. \u2751 The program puts the information directly into special locations in the CPU called registers. \u2751 However, there are only a few registers, so this might not work if there's a lot of information to send. Parameter Passing between Programs & Kernel \u2751Packing Information (Like a Block) \u2751 If there's too much information for registers, the program can create a block of memory like a box and store all the details there. \u2751 Then, the program sends the address of this \"block\" to the OS using a register. \u2751 This way, the OS knows where to find all the information it needs. This approach is used by operating systems like Linux and Solaris. Parameter Passing between Programs & Kernel \u2751 Stack It Up \u2751 Another way is for the program to put the information one piece at a time onto a special area of memory called the stack. \u2751 This is like stacking dishes. Then, the OS can access the information by taking it off the stack, one piece at a time, in the reverse order it was added. \u2751 This method, along with the block method, allows for sending a lot of information without limitations. Interrupts \u2751An interrupt in an operating system is a signal that prompts the OS to temporarily halt its current activities and execute a specific function, often referred to as an interrupt handler or interrupt service routine (ISR). \u2751 The CPU hardware has a wire called the interrupt-request line that"
  },
  {
    "id": 46,
    "source": "Module 2.txt",
    "text": "called the stack. \u2751 This is like stacking dishes. Then, the OS can access the information by taking it off the stack, one piece at a time, in the reverse order it was added. \u2751 This method, along with the block method, allows for sending a lot of information without limitations. Interrupts \u2751An interrupt in an operating system is a signal that prompts the OS to temporarily halt its current activities and execute a specific function, often referred to as an interrupt handler or interrupt service routine (ISR). \u2751 The CPU hardware has a wire called the interrupt-request line that the CPU senses after executing every instruction. Interrupts - Interrupt Handling Process - Example \u2022 The CPU issues an I/O command (e.g., read/write) to a device via its device driver (part of the I/O Request Initiated OS). \u2022 The I/O controller (hardware) receives the command and begins the data transfer between the I/O Controller Takes Over device and memory. \u2022 While I/O is in progress, the CPU continues executing other instructions (non-blocking CPU Continues Working operation). \u2022 When the I/O finishes (or an error occurs), the I/O controller sends an interrupt signal to the I/O Operation Completes CPU. \u2022 The CPU detects the interrupt, pauses current execution, and saves the current task state Interrupt Detected (context switching). \u2022 The OS calls the appropriate interrupt handler (a small routine), usually part of the device Interrupt Handler Invoked driver. Interrupt Processed \u2022 The handler processes the event (e.g., read input data, check for errors, signal completion). \u2022 Once complete, the CPU restores the previous task's state and resumes execution as if it was Return to Previous Task never interrupted. Interrupts - Interrupt Handling Process \u2022 The hardware device sends an interrupt request (IRQ) Interrupt Request to the CPU. \u2022 The CPU acknowledges the interrupt and temporarily Acknowledgment pauses its current execution. \u2022 The CPU uses an interrupt vector to locate the appropriate Interrupt Vector interrupt handler. This vector is essentially a table of pointers to interrupt service routines. \u2022 The CPU executes the ISR to handle the Interrupt Service interrupt. This may involve reading data from a Routine (ISR) device, processing input, or handling an error. \u2022 After the ISR completes, the CPU resumes its Resume Execution previous activity from where it was interrupted. Interrupts - Interrupt Handling Process Process \u2751 Essentially a program that's actively running \u2751 It is the foundation for all the computations that happen on your computer \u2751 A process is unit of work that OS manages. Process \u2751 A process is an active instance of a program that includes: \u2751 The program's machine code. \u2751 Its current activity (what it's doing right now). \u2751 Resources allocated to it by the OS, such as memory, CPU time, and open files. Process \u2013 Memory Layout \u2751 When the operating system loads a program to run it (creating a process), it doesn't just dump the code into memory randomly. \u2751 It organizes the process's memory into a logical, standardized structure"
  },
  {
    "id": 47,
    "source": "Module 2.txt",
    "text": "for all the computations that happen on your computer \u2751 A process is unit of work that OS manages. Process \u2751 A process is an active instance of a program that includes: \u2751 The program's machine code. \u2751 Its current activity (what it's doing right now). \u2751 Resources allocated to it by the OS, such as memory, CPU time, and open files. Process \u2013 Memory Layout \u2751 When the operating system loads a program to run it (creating a process), it doesn't just dump the code into memory randomly. \u2751 It organizes the process's memory into a logical, standardized structure called the process address space Process \u2013 How stored in Memory \u2751A process in an operating system has its own virtual address space, separate from other processes. \u2751This virtual space is typically divided into distinct regions: \u2751 Text \u2751 Data \u2751 Heap \u2751 Stack Process \u2013 How stored in Memory \u2751 Contains the compiled machine code of the program. \u2751 This segment is often marked as read-only so that the process cannot accidentally modify its own instructions. \u2751 It has a fixed size. Process \u2013 How stored in Memory \u2751 Contains global and static variables that are uninitialized or initialized by the programmer. \u2751 For example, int max_users = 100;. \u2751 The size of this segment is also fixed, as it is known at compile time. \u2751 For example, static int counter;. Process \u2013 How stored in Memory \u2751 This is the area for dynamic memory allocation. \u2751 When your program needs more memory while it's running\u2014for example, when you use malloc() in C or the new operator in C++\u2014the memory is allocated from the heap. \u2751 The heap starts at the end of the data or BSS segment and grows upwards towards higher memory addresses as more memory is requested. Process \u2013 How stored in Memory \u2751 Used for static, local memory allocation. \u2751 It stores local variables, function parameters, and return addresses for function calls. \u2751 Every time a function is called, a \"stack frame\" containing its local variables and context is pushed onto the stack. \u2751 When the function returns, the frame is popped off. \u2751 The stack is located at the top of the address space and grows downwards towards lower memory addresses. Process Memory Layout - Example #include <stdio.h> #include <stdlib.h> int global_var = 10; // Global variable void my_function() { // Function with local variables int local_var = 20; char *ptr = malloc(10); printf(\"Inside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); printf(\"Local variable: %d\\n\", local_var); printf(\"Malloced memory: %s\\n\", ptr); free(ptr); } int main() { my_function(); printf(\"Outside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); return 0; } Process Memory Layout - Example #include <stdio.h> #include <stdlib.h> int global_var = 10; // Global variable void my_function() // Function with local variables { int local_var = 20; char *ptr = malloc(10); printf(\"Inside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); Program converted to Machine Code and this printf(\"Local variable: %d\\n\", local_var); machine code is stored in text section printf(\"Malloced memory: %s\\n\", ptr); free(ptr); }"
  },
  {
    "id": 48,
    "source": "Module 2.txt",
    "text": "variables int local_var = 20; char *ptr = malloc(10); printf(\"Inside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); printf(\"Local variable: %d\\n\", local_var); printf(\"Malloced memory: %s\\n\", ptr); free(ptr); } int main() { my_function(); printf(\"Outside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); return 0; } Process Memory Layout - Example #include <stdio.h> #include <stdlib.h> int global_var = 10; // Global variable void my_function() // Function with local variables { int local_var = 20; char *ptr = malloc(10); printf(\"Inside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); Program converted to Machine Code and this printf(\"Local variable: %d\\n\", local_var); machine code is stored in text section printf(\"Malloced memory: %s\\n\", ptr); free(ptr); } int main() { my_function(); printf(\"Outside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); return 0; } Process Memory Layout - Example #include <stdio.h> #include <stdlib.h> Global variable which is initialized and will be int global_var = 10; // Global variable stored in Data section and no uninitialized void my_function() // Function with local variables { variable available so (block starting symbol) BSS int local_var = 20; section will be empty. char *ptr = malloc(10); printf(\"Inside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); printf(\"Local variable: %d\\n\", local_var); printf(\"Malloced memory: %s\\n\", ptr); free(ptr); } int main() { my_function(); printf(\"Outside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); return 0; } Process Memory Layout - Example #include <stdio.h> #include <stdlib.h> int global_var = 10; // Global variable My_function () and its local variables local_var void my_function() // Function with local variables will be stored in Stack. { In case of char *ptr variable only the variable int local_var = 20; and return address will be stored in the stack. char *ptr = malloc(10); (local variables, Function parameters, return printf(\"Inside function:\\n\"); address) printf(\"Global variable: %d\\n\", global_var); printf(\"Local variable: %d\\n\", local_var); printf(\"Malloced memory: %s\\n\", ptr); free(ptr); } int main() Int main() will be stored in Stack. { my_function(); printf(\"Outside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); return 0; } Process Memory Layout - Example #include <stdio.h> #include <stdlib.h> int global_var = 10; // Global variable void my_function() // Function with local variables { The program is asking for 10 bytes of memory while in int local_var = 20; executing status so heap segment will provide memory in char *ptr = malloc(10); runtime and all the intermediary data will be stored in heap printf(\"Inside function:\\n\"); segment printf(\"Global variable: %d\\n\", global_var); printf(\"Local variable: %d\\n\", local_var); printf(\"Malloced memory: %s\\n\", ptr); free(ptr); } int main() { my_function(); printf(\"Outside function:\\n\"); printf(\"Global variable: %d\\n\", global_var); return 0; } Process Memory Layout \u2013 Example 2 Process Memory Layout \u2013 Example 2 Process \u2751The operating system's memory management unit (MMU) translates virtual addresses used by the process into physical memory addresses. Process Control Block \u2751 A Data structure containing all the information about a process \u2751 The PCB contains pointers to the process\u2019s memory layout. \u2751 Each process is represented in the operating system by a process control block (PCB)\u2014also called a task control block. \u2751 The PCB acts as the handle for the OS to manage the process\u2019s memory layout, scheduling and other resources. Process Control Block Process ID Process State Process"
  },
  {
    "id": 49,
    "source": "Module 2.txt",
    "text": "\u2013 Example 2 Process Memory Layout \u2013 Example 2 Process \u2751The operating system's memory management unit (MMU) translates virtual addresses used by the process into physical memory addresses. Process Control Block \u2751 A Data structure containing all the information about a process \u2751 The PCB contains pointers to the process\u2019s memory layout. \u2751 Each process is represented in the operating system by a process control block (PCB)\u2014also called a task control block. \u2751 The PCB acts as the handle for the OS to manage the process\u2019s memory layout, scheduling and other resources. Process Control Block Process ID Process State Process Priority Program Counter CPU registers Heap & Stack Memory Limits I/O Permissions & Status CPU Scheduling Accounting Information List of Open files \u2026\u2026\u2026.. Pointers Process Control Block \u2022 The state may be new, ready, running, waiting, halted, and so Process state. on. \u2022 The counter indicates the address of the next instruction to Program counter. be executed for that process. \u2022 The registers vary in number and type, depending on the computer architecture. They include accumulators, index registers, stack pointers, and CPU registers. general-purpose registers, plus any condition-code information. Along with the program counter, this state information must be saved when an interrupt occurs, to allow the process to be continued correctly afterward when it is rescheduled to run. Process Control Block \u2022 This information includes a process priority, pointers to CPU-scheduling information scheduling queues, and any other scheduling parameters. \u2022 This information may include such items as the value of the Memory-management base and limit registers and the page tables, or the segment information tables, depending on the memory system used by the operating system. \u2022 This information includes the amount of CPU and real time Accounting information used, time limits, account numbers, job or process numbers, and so on. \u2022 This information includes the list of I/O devices allocated to I/O status information the process, a list of open files, and so on. Process Tables \u2751 A data structure maintained by the operating system to store information about all active processes. \u2751 It's a table or array of process control blocks (PCBs). \u2751 Each entry in the process table represents a process and contains a pointer to its corresponding PCB. Process States \u2751 As a process executes, it changes state. The state of a process is defined in part by the current activity of that process. \u2751 A process may be in one of the following states Process States New \u2022 The process is being created. Running \u2022 Instructions are being executed. \u2022 The process is waiting for some event to occur (such Waiting as an I/O completion or reception of a signal). Ready \u2022 The process is waiting to be assigned to a processor. Terminated \u2022 The process has finished execution. Process Creation \u2751A process in operating systems is an active instance of a program in execution, encompassing its code, data, resources, and execution state. \u2751 Creation allows the system to generate new processes, enabling multitasking, resource"
  },
  {
    "id": 50,
    "source": "Module 2.txt",
    "text": "process may be in one of the following states Process States New \u2022 The process is being created. Running \u2022 Instructions are being executed. \u2022 The process is waiting for some event to occur (such Waiting as an I/O completion or reception of a signal). Ready \u2022 The process is waiting to be assigned to a processor. Terminated \u2022 The process has finished execution. Process Creation \u2751A process in operating systems is an active instance of a program in execution, encompassing its code, data, resources, and execution state. \u2751 Creation allows the system to generate new processes, enabling multitasking, resource sharing, and efficient program execution. Parent Process Spawns Child Child Process Child\u2019s Child Process Process Creation \u2751 Process creation typically occurs when an existing process (parent) spawns a new one (child), forming a hierarchical tree structure. \u2751 This hierarchy helps manage dependencies and resource allocation. \u2751 The operating system oversees creation by allocating resources like memory, CPU time, and I/O devices. \u2751 Each new process receives a unique Process Identifier (PID) and a Process Control Block (PCB) to track its state, program counter, and registers Process Creation \u2751How Resource Allocation, Resource Sharing, Address Space and Execution happens when child processes are created? \u2751 When a child process is created, it needs resources like CPU time, memory, files, and I/O devices to execute. \u2751 The child process can obtain resources directly from the OS \u2751 The child process can inherit a subset of resources from the parent process. (Resources are limited) \u2751 The parent process may need to partition its resources among its children (Resources are limited) Process Execution \u2751After process creation, \u2751 The parent can run concurrently with the child or \u2751 Wait for its one or all child\u2019s execution completion. \u2751 When a new (child) process is created, there are two possibilities for its address space Duplicate Address New Address Space Space \u2022 The child process has \u2022 The child process has a the same program and new program loaded data as the parent into it, with its own process separate address space. Process Execution - Mechanisms The fork() system call creates a child as an exact duplicate of the Unix-like parent, returning 0 to the child and the child's PID to the parent. Systems Following fork(), the exec() system call can replace the child's image with a new program The CreateProcess() function combines creation and program Windows Systems loading, initializing memory and loading the specified executable Alternatives include vfork() for shared memory until exec() or Other Methods posix_spawn() for attribute-specified creation Process Execution \u2013 Mechanisms \u2013 Unix Example \u2022 This system call creates a new child process by duplicating the parent process. The child is an exact copy of the parent, including memory and fork() state, but with a unique process ID (PID). \u2022 It returns 0 to the child and the child's PID to the parent \u2022 After forking, the child often uses exec() (or variants like execv(), execl()) to replace its process image with a new"
  },
  {
    "id": 51,
    "source": "Module 2.txt",
    "text": "loading, initializing memory and loading the specified executable Alternatives include vfork() for shared memory until exec() or Other Methods posix_spawn() for attribute-specified creation Process Execution \u2013 Mechanisms \u2013 Unix Example \u2022 This system call creates a new child process by duplicating the parent process. The child is an exact copy of the parent, including memory and fork() state, but with a unique process ID (PID). \u2022 It returns 0 to the child and the child's PID to the parent \u2022 After forking, the child often uses exec() (or variants like execv(), execl()) to replace its process image with a new program. This loads and runs a exec() different executable in the child's address space, without creating a new process \u2022 The parent process can call wait() (or waitpid()) to suspend its execution wait() until the child terminates. This allows the parent to collect the child's exit status and ensures proper cleanup \u2022 This terminates the process, passing an exit status to the parent. It exit() decrements the process count and releases resources. The child typically calls exit() after completing its task Process Execution \u2013 Mechanisms \u2013 Unix Example Process Execution \u2013 Mechanisms \u2013 Zombie Process Process Execution \u2013 Mechanisms \u2013 Zombie Process Parent is not waiting to acknowledge child process termination Process Execution \u2013 Mechanisms \u2013 Zombie Process Zombie Process \u2751 A zombie process occurs when a child process completes execution (calls exit()) but remains in the process table because the parent hasn't called wait() to collect its exit status. \u2751 The child is \"dead\" but not fully removed, holding a PID and minimal resources until the parent acknowledges. \u2751 Resolution - The parent eventually calls wait(), or if the parent terminates, the zombie becomes an orphan and is adopted by the init process (PID 1), which cleans it up Process Execution \u2013 Mechanisms \u2013 Orphan Process Process Execution \u2013 Mechanisms \u2013 Orphan Process Parent exited before child process termination Orphan Child Process Execution \u2013 Mechanisms \u2013 Orphan Process Orphan Process \u2751 An orphan process is a running child process whose parent terminates before it does, leaving the child without a parent. \u2751 The kernel automatically re-parents it to the init process (PID 1), which handles cleanup. Thread \u2751 A thread is the smallest unit of execution within a process, representing a sequence of programmed instructions that the operating system can manage independently. \u2751 It operates in the context of a process, sharing the process's resources like memory and open files, but each thread has its own stack, program counter, and registers. \u2751 Threads are often called lightweight processes because they enable concurrency without the overhead of creating separate processes. \u2751 Example - In a web browser, a single process might run multiple threads: one for rendering the user interface, another for handling network requests, and a third for executing JavaScript. This allows the browser to remain responsive while performing background tasks. Thread \u2751 A process is an executing program that can contain one or more threads, forming a container"
  },
  {
    "id": 52,
    "source": "Module 2.txt",
    "text": "a process, sharing the process's resources like memory and open files, but each thread has its own stack, program counter, and registers. \u2751 Threads are often called lightweight processes because they enable concurrency without the overhead of creating separate processes. \u2751 Example - In a web browser, a single process might run multiple threads: one for rendering the user interface, another for handling network requests, and a third for executing JavaScript. This allows the browser to remain responsive while performing background tasks. Thread \u2751 A process is an executing program that can contain one or more threads, forming a container for threads to run concurrently. \u2751 All threads within a process share the same address space, code, data, and resources, but they can execute independently to improve efficiency and responsiveness. Components shared Code Section Data Section Open Files Signals Code Section Data Section Open Files Signals by Threads Program Thread ID Thread ID Thread ID Thread ID Register Set Stack Unique to each Counter Program Program Program Thread Counter Counter Counter Register Set Register Set Register Set Stack Stack Stack Thread \u2013 Single thread \u2751 Only one thread runs at a time, executing tasks sequentially. \u2751 This can lead to inefficiencies, such as waiting for I/O operations, resulting in higher idle time and reduced responsiveness. \u2751 For example, a single-threaded file downloader processes one file at a time, pausing if it encounters delays. Thread \u2013 Multi thread \u2751 Multiple threads run concurrently within the same process, allowing parallel task execution and better resource utilization, especially on multi-core processors. \u2751 This minimizes idle time and improves performance, but it introduces complexities like synchronization. Thread \u2013 Single Vs Multi thread Aspect Single Thread Multi-Thread Concurrent, multiple tasks Execution Sequential, one task at a time simultaneously Efficiency Higher idle time, less scalable Minimal idle time, better scalability Responsiveness Can freeze during waits Remains responsive even during delays Basic calculator app handling one Web server handling multiple client Example operation requests Synchronization Not Required Heavily Required Thread \u2013 Multithreading Models Thread \u2013 Multithreading Models User Threads Kernel Threads Managed by user-level libraries: created Managed by the operating system: created and managed entirely within a user and managed by the kernel application Lightweight: Context switching between Heavier weight: Context switching between user threads is typically faster as it kernel threads is slower as it involves involves only switching between user-level saving and restoring kernel-level state as registers and stacks well. No direct OS support: The kernel is Direct OS support: The kernel is aware of unaware of user threads. kernel threads and can schedule them independently. Susceptible to blocking: If one user thread Resistant to blocking: If one kernel thread performs a blocking system call, the entire is blocked, other threads within the same process is blocked, including all other user process can continue execution. threads. Thread \u2013 Multithreading Models \u2751 Multithreading models define how user-level threads (managed by applications) map to kernel-level threads (managed by the operating system). \u2751 These models balance concurrency, efficiency,"
  },
  {
    "id": 53,
    "source": "Module 2.txt",
    "text": "as registers and stacks well. No direct OS support: The kernel is Direct OS support: The kernel is aware of unaware of user threads. kernel threads and can schedule them independently. Susceptible to blocking: If one user thread Resistant to blocking: If one kernel thread performs a blocking system call, the entire is blocked, other threads within the same process is blocked, including all other user process can continue execution. threads. Thread \u2013 Multithreading Models \u2751 Multithreading models define how user-level threads (managed by applications) map to kernel-level threads (managed by the operating system). \u2751 These models balance concurrency, efficiency, and overhead Thread \u2013 Multithreading Models \u2751 Many-to-One Model \u2751 Multiple user threads map to a single kernel thread, with management in user space for efficiency. \u2751 However, a blocking call halts the entire process, limiting parallelism. \u2751 Efficient for processes that don't perform I/O- bound tasks \u2751 Example: Green threads in some Java implementations, Early versions of UNIX Thread \u2013 Multithreading Models \u2751 One-to-One Model \u2751 Each user thread maps to a dedicated kernel thread, enabling true concurrency and parallel execution on multiprocessors. \u2751 Each user thread can be independently scheduled and blocked \u2751 Creating a kernel thread for each user thread can be resource-intensive and be overhead. \u2751 Example: Used in Linux and Windows Thread \u2013 Multithreading Models \u2751 Many-to-Many Model \u2751 Multiple user threads multiplex onto a smaller or equal number of kernel threads, allowing flexible concurrency without excessive overhead. \u2751 It supports parallelism and handles blocking calls well. \u2751 Multiple user threads can continue execution even if one kernel thread is blocked. \u2751 Requires careful management of thread mapping and scheduling \u2751 Example: Database servers managing numerous queries across limited kernel threads. Thread \u2013 Multithreading Models Aspect Many to One One to One Many to Many One user thread per kernel Multiple user threads to Multiple user threads to one Mapping thread multiple kernel threads kernel thread High; supports parallel Balanced; enables parallelism Limited; no true parallelism on Concurrency execution without excessive overhead multiprocessors Blocking One thread's block halts the Only the blocking thread is Kernel schedules around Behavior process affected blocks Overhead Low; user-space management High; kernel thread creation Moderate; flexible thread for each user thread allocation Limited by kernel thread High; supports many threads High for simple tasks but poor Scalability limits efficiently on multicore systems CPU-bound tasks needing Complex applications with Lightweight, non-parallel Best Use Cases concurrency variable workloads applications Thread \u2013 Multithreading Models Aspect Many to One One to One Many to Many Efficiency and low overhead, as Strong concurrency and Scalability and balanced Advantage thread creation avoids kernel multiprocessor utilization resource use, avoiding full involvement process blocks Performance burdens from Added complexity in Lack of multiprocessor support Disadvantage excessive kernel threads implementation and process-wide blocking Multicore Programming \u2751 A multicore processor is a single physical processor that contains multiple processing cores. \u2751 Each core can execute instructions independently, allowing for multiple tasks to be performed simultaneously. Multicore Programming - Execution Parallel"
  },
  {
    "id": 54,
    "source": "Module 2.txt",
    "text": "Best Use Cases concurrency variable workloads applications Thread \u2013 Multithreading Models Aspect Many to One One to One Many to Many Efficiency and low overhead, as Strong concurrency and Scalability and balanced Advantage thread creation avoids kernel multiprocessor utilization resource use, avoiding full involvement process blocks Performance burdens from Added complexity in Lack of multiprocessor support Disadvantage excessive kernel threads implementation and process-wide blocking Multicore Programming \u2751 A multicore processor is a single physical processor that contains multiple processing cores. \u2751 Each core can execute instructions independently, allowing for multiple tasks to be performed simultaneously. Multicore Programming - Execution Parallel Execution Concurrent Execution Concurrency execution refers to the ability of a program to perform multiple tasks or threads simultaneously, but not Parallel execution refers to the necessarily at the same instant. simultaneous execution of multiple tasks or threads on multiple processing cores. Concurrency is achieved through context switching, where the processor switches between tasks quickly. Multicore Programming - Execution Multicore Programming \u2013 Parallelism Type Multicore Programming \u2013 Parallelism Type \u2751Parallelism is a technique to achieve faster execution by dividing a task into smaller sub-tasks that can be executed concurrently. \u2751 Data Parallelism \u2751 Focuses on distributing a large dataset across multiple processor cores and having each core perform the exact same operation on its subset of the data. \u2751 One operation, many pieces of data \u2751 Imagine you have a massive image and you want to increase the brightness of every pixel. Instead of one core processing all pixels one by one, you can split the image into four sections. You then assign each section to a different core, and all four cores run the \"increase brightness\" function simultaneously. Multicore Programming \u2013 Parallelism Type \u2751Task Parallelism \u2751 Focuses on distributing different, independent tasks across multiple cores to be performed concurrently. The tasks can operate on the same or different data \u2751 Different tasks, running at the same time \u2751 In a video game engine, one core might be dedicated to rendering graphics, a second core could be handling the game's physics calculations, a third could be running the AI for computer-controlled characters, and a fourth could be managing audio processing. Multicore Programming \u2013 Parallelism Type Data Parallelism Task Parallelism Used when Dealing with large datasets Performing different operations on different data elements Performing identical operations on each data Executing independent tasks concurrently element Using SIMD (Single Instruction, Multiple Using MIMD (Multiple Instruction, Multiple Data) instructions Data) instructions Examples Examples \u2022 Matrix multiplication \u25aa Web server handling multiple requests \u2022 Image processing \u25aa Compiling multiple source files \u2022 Scientific simulations \u25aa Scientific simulations with different parameters Tutorial Questions \u2751 Consider a web server application designed to handle many simultaneous client requests on a multi-core processor. If one of these requests requires a slow I/O operation (e.g., reading a large file from a disk), why might the entire application freeze and become unresponsive if it uses User-Level Threads, while an identical application using Kernel-Level Threads would continue to handle other requests smoothly?"
  },
  {
    "id": 55,
    "source": "Module 2.txt",
    "text": "(Multiple Instruction, Multiple Data) instructions Data) instructions Examples Examples \u2022 Matrix multiplication \u25aa Web server handling multiple requests \u2022 Image processing \u25aa Compiling multiple source files \u2022 Scientific simulations \u25aa Scientific simulations with different parameters Tutorial Questions \u2751 Consider a web server application designed to handle many simultaneous client requests on a multi-core processor. If one of these requests requires a slow I/O operation (e.g., reading a large file from a disk), why might the entire application freeze and become unresponsive if it uses User-Level Threads, while an identical application using Kernel-Level Threads would continue to handle other requests smoothly? Tutorial Questions \u2751 The fundamental reason for this difference in behavior lies in whether the operating system's kernel is aware of the individual threads. With User-Level Threads, the kernel is not aware of them and sees the entire application as a single task; with Kernel-Level Threads, the kernel manages each thread independently \u2751 Scenario with User-Level Threads (Application Freezes) \u2751 In this model, thread management is handled by a library in the user space, not by the operating system. The kernel only sees one entity: the main process itself. The kernel maps all the user-level threads of the application to a single kernel-level thread or process. To the kernel, your entire multi-threaded application is just one unit of execution. When one user thread makes a \"blocking\" system call (like reading a file from a slow disk), the kernel sees that its single unit of execution has requested a blocking operation. Consequently, the kernel puts the entire process to sleep until the I/O operation is complete. Since the kernel is unaware of the other user threads, it cannot schedule them to run. The result is that all other threads\u2014even those that are ready to process other client requests\u2014are paused, causing the entire application to freeze Tutorial Questions \u2751 Scenario with Kernel-Level Threads (Application Stays Responsive) \u2751In this model, the operating system kernel directly manages every thread. Each thread is a distinct unit that the kernel can schedule independently. The kernel is aware of every single thread within the application. It sees them as separate, schedulable tasks. When one kernel thread makes a blocking I/O call, the kernel identifies that only that specific thread cannot proceed. The kernel marks just the one blocking thread as \"waiting\" and removes it from the queue of runnable threads. The OS scheduler is then free to assign the CPU cores to other, ready-to- run threads from the same application. This allows other client requests to be processed concurrently, and the application remains responsive Tutorial Questions \u2751 A software development team is designing a new high-performance application that needs to execute numerous tasks concurrently on a multi-core server. The team is evaluating different multi-threading models to determine the most suitable one. If the application chooses the Many-to-One multi-threading model, and one of its user-level threads initiates a complex database query that takes a significant amount of time to complete, why would this choice likely lead to performance"
  },
  {
    "id": 56,
    "source": "Module 2.txt",
    "text": "assign the CPU cores to other, ready-to- run threads from the same application. This allows other client requests to be processed concurrently, and the application remains responsive Tutorial Questions \u2751 A software development team is designing a new high-performance application that needs to execute numerous tasks concurrently on a multi-core server. The team is evaluating different multi-threading models to determine the most suitable one. If the application chooses the Many-to-One multi-threading model, and one of its user-level threads initiates a complex database query that takes a significant amount of time to complete, why would this choice likely lead to performance bottlenecks and an unresponsive application, even if other CPU cores are idle? Conversely, explain which multi-threading model would be better suited for maintaining application responsiveness and utilizing available CPU resources in the same scenario and why? Tutorial Questions \u2751 In the Many-to-One model, multiple user-level threads are mapped to a single kernel-level thread . The operating system (kernel) is only aware of this single kernel thread, not the individual user threads within the application. The kernel views the entire multi-threaded application as a single unit of execution because all user threads share one kernel thread. When one user-level thread executes a blocking system call (like a slow database query), the associated single kernel thread also blocks. Since the kernel perceives only that one kernel thread as blocked, it puts the entire process to sleep, regardless of whether other user-level threads are ready to run or if other CPU cores are available . The kernel cannot schedule any other user threads because it is unaware of their existence as separate entities, leading to the application becoming unresponsive. Tutorial Questions \u2751Scenario with Many-to-Many Model (Responsiveness and Resource Utilization) \u2751The kernel is aware of multiple kernel threads for the application, and user-level threads are mapped to these available kernel threads. When one user-level thread makes a blocking system call, only the specific kernel thread it's currently using might block. Crucially, the operating system can then schedule other ready user-level threads onto other available kernel threads within the same process, or even create new kernel threads if needed . This allows other tasks within the application to continue running concurrently, preventing the entire application from freezing . This model effectively utilizes multi-core processors, enabling true parallelism and maintaining responsiveness even when some threads are blocked . Tutorial Questions \u2751Imagine a multi-tasking operating system running two independent processes: Process A, a video editor currently rendering a file, and Process B, a simple command-line shell waiting for user input. Suddenly, a hardware interrupt occurs (e.g., a timer ticks), and the operating system's scheduler decides to perform a context switch from the CPU- intensive Process A to the idle Process B. Explain the precise, step-by-step role of the Process Table, the Process Control Block (PCB), and the concept of a Process Memory Layout in this context switch. Specifically, detail how the operating system ensures that Process A\u2019s complex memory state is safely preserved, so it can resume"
  },
  {
    "id": 57,
    "source": "Module 2.txt",
    "text": "two independent processes: Process A, a video editor currently rendering a file, and Process B, a simple command-line shell waiting for user input. Suddenly, a hardware interrupt occurs (e.g., a timer ticks), and the operating system's scheduler decides to perform a context switch from the CPU- intensive Process A to the idle Process B. Explain the precise, step-by-step role of the Process Table, the Process Control Block (PCB), and the concept of a Process Memory Layout in this context switch. Specifically, detail how the operating system ensures that Process A\u2019s complex memory state is safely preserved, so it can resume rendering flawlessly later, while correctly activating the simple, waiting state of Process B. Tutorial Questions \u2751 The context switch is initiated by a hardware interrupt. This immediately causes the CPU to stop executing Process A's code and switch from user mode to kernel mode. This gives the operating system's kernel full control of the hardware. \u2751The kernel consults the Process Table, which is an array or linked list containing entries for every active process. It uses the ID of the currently running process (Process A) to quickly locate the memory address of its Process Control Block (PCB). \u2751The kernel saves the current state of the CPU's registers directly into designated fields within Process A's PCB. \u2751This is where the PCB saves the pointers and metadata that define Process A's virtual address space. This information, stored in the PCB, is essential for reconstructing its memory environment later. \u2751Finally, the kernel updates the process state field in Process A's PCB from \"Running\" to \"Ready,\" indicating it is ready to run but not currently using the CPU. Tutorial Questions \u2751Now, the scheduler selects Process B to run. The kernel prepares to load its context. \u2751 The scheduler picks Process B from the queue of ready processes. The kernel again uses the Process Table to find the memory location of Process B's PCB. \u2751 The kernel does the reverse of Step 2. It loads the values from Process B\u2019s PCB into the CPU's registers. Since Process B was idle and waiting, its saved Program Counter points to the instruction that checks for user input. \u2751The kernel uses the memory management information stored in Process B\u2019s PCB to reconfigure the Memory Management Unit (MMU). The MMU's registers are updated to point to Process B's page tables. This instantly makes Process B's memory space (its code, data, stack, and heap) the active one, effectively hiding Process A's memory from view. \u2751The state of Process B in its PCB is changed from \"Ready\" to \"Running\u201d. \u2751 The kernel loads the saved Program Counter from Process B's PCB into the CPU's PC register. It then switches the CPU back from kernel mode to user mode. Execution seamlessly resumes within Process B, which now runs as if it had always been in control."
  },
  {
    "id": 58,
    "source": "Module 2.txt",
    "text": "data, stack, and heap) the active one, effectively hiding Process A's memory from view. \u2751The state of Process B in its PCB is changed from \"Ready\" to \"Running\u201d. \u2751 The kernel loads the saved Program Counter from Process B's PCB into the CPU's PC register. It then switches the CPU back from kernel mode to user mode. Execution seamlessly resumes within Process B, which now runs as if it had always been in control."
  },
  {
    "id": 59,
    "source": "M4-2.txt",
    "text": "Concurrency Module 4 Dr. Naveenkumar J Associate Professor, PRP- 217 - 4 Module 4 - Concurrency Inter-Process Communication (IPC) \u25aa A mechanism provided by operating systems that allows processes to communicate with each other. \u25aa This is essential for processes to exchange data, synchronize their actions, and coordinate activities while running in parallel. \u25aa IPC ensures efficient operation and resource sharing in multi-processing environments. 10-09-2025 2 Module 4 - Concurrency Inter-Process Communication (IPC) \u25aa Why is IPC Needed? \u25aa Sharing data between processes that have related tasks. \u25aa Coordinating activities so processes can work together. \u25aa Managing resources among multiple concurrent programs. \u25aa Achieving modularity by separating functionalities into different processes. 10-09-2025 3 Module 4 - Concurrency Inter-Process Communication (IPC) \u25aa Example \u25aa Copy and Paste (Using the Clipboard) - When you copy text from your web browser (Process 1) and paste it into a word processor like Microsoft Word (Process 2), you are using an IPC mechanism called the clipboard. 10-09-2025 4 Module 4 - Concurrency Inter-Process Communication (IPC) IPC Mechanisms Shared Message Message Pipes Semaphores Sockets Memory Passing Queues 10-09-2025 5 Module 4 - Concurrency IPC Mechanisms \u2013 Shared Memory \u25aa It allows multiple independent processes to access the same block of physical memory, enabling them to exchange large amounts of data with minimal overhead. \u25aa Working \u25aa Shared memory works by creating a special segment in the computer's RAM that the operating system maps into the virtual address space of two or more processes. \u25aa This means that although each process \"sees\" the memory at a potentially different address, they are all reading from and writing to the same physical location. 10-09-2025 6 Module 4 - Concurrency IPC Mechanisms \u2013 Shared Memory \u25aa It allows multiple independent processes to access the same block of physical memory, enabling them to exchange large amounts of data with minimal overhead. \u25aa Working \u25aa Shared memory works by creating a special segment in the computer's RAM that the operating system maps into the virtual address space of two or more processes. \u25aa This means that although each process \"sees\" the memory at a potentially different address, they are all reading from and writing to the same physical location. 10-09-2025 7 Module 4 - Concurrency IPC Mechanisms \u2013 Shared Memory \u25aa How is the shared memory segment identified and whether it is associated with a file on the file system? \u25aa Two Implementation styles of Shared Memory \u25aa Anonymous Two Implementation styles of Shared \u25aa Named or Mapped Memory Anonymous Named or Mapped 10-09-2025 8 Module 4 - Concurrency IPC Mechanisms \u2013 Shared Memory Anonymous Mapped or Named Anonymous shared memory is a region of memory Mapped shared memory, also known as memory- that is not associated with any file in the mapped files, is a region of memory that is directly filesystem. It is \"anonymous\" because it doesn't have associated with a specific file on the file system. a name or path that other, unrelated processes can Processes communicate by mapping"
  },
  {
    "id": 60,
    "source": "M4-2.txt",
    "text": "Implementation styles of Shared Memory \u25aa Anonymous Two Implementation styles of Shared \u25aa Named or Mapped Memory Anonymous Named or Mapped 10-09-2025 8 Module 4 - Concurrency IPC Mechanisms \u2013 Shared Memory Anonymous Mapped or Named Anonymous shared memory is a region of memory Mapped shared memory, also known as memory- that is not associated with any file in the mapped files, is a region of memory that is directly filesystem. It is \"anonymous\" because it doesn't have associated with a specific file on the file system. a name or path that other, unrelated processes can Processes communicate by mapping this same file use to find it. into their respective address spaces This type of shared memory is typically used between Processes access the shared memory by using a a parent process and its child processes. The parent common name\u2014the path to the file. This allows creates the anonymous segment, and when it forks a completely unrelated processes to communicate, if child, the child inherits the file descriptor or handle they both know the file path and have the to that memory. necessary permissions Since there is no public name, it is more secure. Unrelated processes cannot easily discover and It can be used to share data between processes that attach to the memory segment, preventing are not part of the same family unauthorized access It is ideal for tightly coupled processes, such as a It is perfect for applications that need to share data main application and its worker processes, where the between unrelated processes, or when the shared parent can directly pass the memory handle to its data needs to be saved and persist beyond the children. lifetime of the processes. 10-09-2025 9 Module 4 - Concurrency IPC Mechanisms \u2013 Shared Memory Merits Demerits Synchronization Complexity: Processes must Speed: Since processes are accessing the same manually manage access to the shared memory to physical memory, there is no need for the kernel to prevent race conditions and data corruption. This mediate or copy data between them. This avoids requires implementing complex synchronization system call overhead. primitives like semaphores or mutexes, which can be difficult to get right. Efficiency: Data is not duplicated for each process; Security Risks: If not properly secured, a shared a single copy is shared among all, reducing overall memory segment could be accessed by unauthorized memory consumption. processes, leading to data leaks or corruption. Potential for Deadlocks: Improperly implemented Large Data Volumes: It is ideal for transferring large synchronization can lead to deadlocks, where two or amounts of data, such as video frames, large more processes are stuck waiting for each other to datasets, or scientific computing results, where other release a lock on the shared memory, bringing the methods would be too slow. system to a halt. 10-09-2025 10 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing \u25aa Processes communicate without sharing the same address space. Instead, they exchange discrete messages managed by the operating system. \u25aa Think of it as processes"
  },
  {
    "id": 61,
    "source": "M4-2.txt",
    "text": "Potential for Deadlocks: Improperly implemented Large Data Volumes: It is ideal for transferring large synchronization can lead to deadlocks, where two or amounts of data, such as video frames, large more processes are stuck waiting for each other to datasets, or scientific computing results, where other release a lock on the shared memory, bringing the methods would be too slow. system to a halt. 10-09-2025 10 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing \u25aa Processes communicate without sharing the same address space. Instead, they exchange discrete messages managed by the operating system. \u25aa Think of it as processes talking to each other by sending letters through a postal service (the OS) rather than writing on a shared whiteboard \u25aa The implementation relies on the operating system's kernel to act as an intermediary. 10-09-2025 11 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing \u25aa How Message Passing is Implemented in an OS? Establish a Communication Link Send Operation Receive Operation 10-09-2025 12 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing \u25aa How Message Passing is Implemented in an OS? \u25aa Establish a Communication Link: Before communication can begin, a link must be established between the processes. This can be: \u25aa Direct Communication: The sender and receiver explicitly name each other (e.g., send(Process_B, message)). This creates a one-to-one link. \u25aa Indirect Communication: Messages are sent to and received from a central \"mailbox\" or \"port.\" Multiple processes can use the same mailbox, allowing for more flexible, many-to-many communication. 10-09-2025 13 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing \u25aa The send Operation: \u25aa A process packages its data into a message and executes a send system call. The kernel takes control, copies the message from the sender's private memory into a secure kernel buffer, and queues it for delivery. \u25aa The receive Operation: \u25aa The receiving process executes a receive system call. The kernel then copies the message from its buffer into the receiver's private memory space. 10-09-2025 14 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing \u25aa This process can be either synchronous (blocking) or asynchronous (non-blocking) \u25aa Synchronous (Blocking): The sender is blocked until the receiver has successfully received the message. This ensures the message was delivered but can make the sender wait. \u25aa Asynchronous (Non-Blocking): The sender hands the message to the OS and continues its own execution immediately, without waiting for the receiver. This is more efficient for the sender but provides no guarantee of when (or if) the message is read. 10-09-2025 15 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing Merits Demerits Simpler to Implement for Programmers: It Slower Performance: The involvement of the avoids the complexities of synchronization (like kernel in every send and receive operation adds mutexes and semaphores) because the kernel significant overhead. Data must be copied from handles the message transfer. This makes it the sender's memory to the kernel, and then from easier to write correct concurrent code. the kernel to the receiver's memory. This"
  },
  {
    "id": 62,
    "source": "M4-2.txt",
    "text": "This is more efficient for the sender but provides no guarantee of when (or if) the message is read. 10-09-2025 15 Module 4 - Concurrency IPC Mechanisms \u2013 Message Passing Merits Demerits Simpler to Implement for Programmers: It Slower Performance: The involvement of the avoids the complexities of synchronization (like kernel in every send and receive operation adds mutexes and semaphores) because the kernel significant overhead. Data must be copied from handles the message transfer. This makes it the sender's memory to the kernel, and then from easier to write correct concurrent code. the kernel to the receiver's memory. This is much slower than directly accessing shared memory. Enhanced Safety and Isolation: Since processes Overhead and Latency: The packaging, sending, don't share memory, there is no risk of one and unboxing of messages introduces latency, process accidentally corrupting another's data. which can be a problem for performance-critical Each process operates in its own protected applications. address space. Ideal for Distributed Systems: Because it Not Ideal for Large Data: The copying process doesn't require a shared physical memory, makes it inefficient for transferring very large message passing is the natural choice for amounts of data compared to shared memory. communication between processes running on different computers across a network. 10-09-2025 16 Module 4 - Concurrency IPC Mechanisms \u2013 Message Queues \u25aa It allows processes to communicate asynchronously by exchanging messages through a shared queue structure managed by the operating system's kernel \u25aa The sender doesn't have to wait for the receiver to be ready, and the receiver doesn't need to know anything about the sender, only where the mailbox is. 10-09-2025 17 Module 4 - Concurrency IPC Mechanisms \u2013 Message Queues \u25aa The queue acts as an intermediary, allowing Sender and Receiver processes to operate independently. \u25aa A process can send a message and continue with its work without waiting for a response. The message will be stored securely in the queue until a receiver is ready to process it. \u25aa The system/kernel preserves message boundaries, ensuring that what one process sends is exactly what another receives. \u25aa A single queue can have multiple senders and multiple receivers, making it a flexible tool for complex application architectures. 10-09-2025 18 Module 4 - Concurrency IPC Mechanisms \u2013 Message Queues \u25aa POSIX Message Queues: POSIX queues are identified by names (like file paths) and offer additional features like message prioritization and asynchronous notifications when a new message arrives. 10-09-2025 19 Module 4 - Concurrency IPC Mechanisms \u2013 PIPES \u25aa They create a unidirectional communication channel that allows the output of one process to be fed directly as the input to another process. \u25aa Pipes are implemented and managed by the operating system's kernel. \u25aa A single process (the parent) makes a pipe() system call. The OS doesn't create a file on disk; instead, it creates a small, in-memory buffer and returns two file descriptors to the process: \u25aa A file descriptor for the read end of the pipe. \u25aa A file descriptor for"
  },
  {
    "id": 63,
    "source": "M4-2.txt",
    "text": "when a new message arrives. 10-09-2025 19 Module 4 - Concurrency IPC Mechanisms \u2013 PIPES \u25aa They create a unidirectional communication channel that allows the output of one process to be fed directly as the input to another process. \u25aa Pipes are implemented and managed by the operating system's kernel. \u25aa A single process (the parent) makes a pipe() system call. The OS doesn't create a file on disk; instead, it creates a small, in-memory buffer and returns two file descriptors to the process: \u25aa A file descriptor for the read end of the pipe. \u25aa A file descriptor for the write end of the pipe. 10-09-2025 20 Module 4 - Concurrency IPC Mechanisms \u2013 PIPES \u25aa The sending process uses a standard write() system call on its file descriptor, and the receiving process uses a read() system call on its file descriptor. \u25aa The OS kernel handles the data transfer through the in-memory buffer, ensuring that the reader process waits if the pipe is empty and the writer process waits if the pipe's buffer is full. 10-09-2025 21 Module 4 - Concurrency Summary \u2013 Shared Memory 10-09-2025 22 Module 4 - Concurrency Synchronization \u25aa Synchronization is the coordination of multiple processes or threads so that they can work together without interfering with each other. \u25aa It ensures that when multiple processes access shared resources (like variables, files, or devices), they do so in an orderly manner to prevent conflicts or errors. 10-09-2025 23 Module 4 - Concurrency Synchronization \u2013 Critical Section \u25aa The \"critical section\" is the part of the program where the shared resource is accessed. \u25aa Critical Section Problem \u25aa The critical section problem occurs when multiple processes need to access and modify shared data simultaneously \u25aa The problem is to design a way so that only one process can be inside its critical section at a time, preventing inconsistent or corrupted data. \u25aa If two or more processes enter their critical sections simultaneously, it can cause race conditions, where the outcome depends on the unpredictable timing of processes. 10-09-2025 24 Module 4 - Concurrency Critical Section \u2013 Race Condition \u25aa If two or more processes or threads run int shared_counter = 0; // shared resource increment() simultaneously, the following void increment() can happen: { // Critical Section starts int temp = shared_counter; // read shared value \u25aa Thread A reads shared_counter as 0. temp = temp + 1; // modify value \u25aa Before Thread A writes back, Thread B shared_counter = temp; // write back to shared value // Critical Section ends also reads shared_counter as 0. } \u25aa Both increment to 1 independently. \u25aa The block of code: \u25aa Both write 1 back to shared_counter. int temp = shared_counter; \u25aa Instead of two increments making temp = temp + 1; shared_counter 2, it stays 1 due to shared_counter = temp; is the critical section because it accesses simultaneous access. and modifies the shared variable. \u25aa This unexpected behavior is a race The problem is that"
  },
  {
    "id": 64,
    "source": "M4-2.txt",
    "text": "+ 1; // modify value \u25aa Before Thread A writes back, Thread B shared_counter = temp; // write back to shared value // Critical Section ends also reads shared_counter as 0. } \u25aa Both increment to 1 independently. \u25aa The block of code: \u25aa Both write 1 back to shared_counter. int temp = shared_counter; \u25aa Instead of two increments making temp = temp + 1; shared_counter 2, it stays 1 due to shared_counter = temp; is the critical section because it accesses simultaneous access. and modifies the shared variable. \u25aa This unexpected behavior is a race The problem is that multiple threads can condition because threads are \"racing\" to enter this critical section at the same time. access and modify the shared data. This leads to incorrect or inconsistent values like the race condition above. 10-09-2025 25 Module 4 - Concurrency Critical Section \u2013 Race Condition int shared_var = 0; // Shared variable Thread 1 / Process 1 Thread 2 / Process 2 Main Program void* process1(void* arg) { void* process2(void* arg) { pthread_t t1, t2; // Critical Section starts // Critical Section starts pthread_create(&t1, NULL, process1, int temp = shared_var; // Read shared int temp = shared_var; // Read shared NULL); variable variable pthread_create(&t2, NULL, process2, temp = temp + 1; // Modify temp = temp + 1; // Modify NULL); shared_var = temp; // Write back shared_var = temp; // Write back printf(\"Process 1 updated shared_var to printf(\"Process 2 updated shared_var to pthread_join(t1, NULL); %d\\n\", shared_var); %d\\n\", shared_var); pthread_join(t2, NULL); // Critical Section ends // Critical Section ends return NULL; return NULL; printf(\"Final value of shared_var: %d\\n\", shared_var); \u25aa shared_var is accessed by both processes, representing critical shared data. \u25aa The lines reading, modifying, and writing shared_var form the critical section that must be accessed atomically. \u25aa Without synchronization, both threads could read the same original value before writing, causing one increment to be lost. \u25aa The final value of shared_var may incorrectly be 1 instead of 2, due to overlapping critical sections and race condition. 10-09-2025 26 Module 4 - Concurrency Critical Section \u2013 Race Condition - Analogy \u25aa Imagine you and a family member share a joint bank account with a current balance of $1,000. On the same day, at almost the exact same time, you both decide to withdraw $100 from different ATMs. ATM A (CHENNAI) ATM B (DELHI) At the exact same moment, before ATM A can Reads the account balance. It sees $1,000. update the balance, ATM B also reads the account balance. It also sees $1,000. Calculates the new balance: $1,000 - $100 = It doesn't know about ATM A's update. It performs $900. It updates the account balance to $900. its own calculation based on the balance it originally read: $1,000 - $100 = $900. It updates the account balance to $900. \u25aa Even though a total of $200 was withdrawn, the final account balance is incorrectly recorded as $900 instead of the correct $800. The bank has lost $100 because"
  },
  {
    "id": 65,
    "source": "M4-2.txt",
    "text": "ATM A can Reads the account balance. It sees $1,000. update the balance, ATM B also reads the account balance. It also sees $1,000. Calculates the new balance: $1,000 - $100 = It doesn't know about ATM A's update. It performs $900. It updates the account balance to $900. its own calculation based on the balance it originally read: $1,000 - $100 = $900. It updates the account balance to $900. \u25aa Even though a total of $200 was withdrawn, the final account balance is incorrectly recorded as $900 instead of the correct $800. The bank has lost $100 because the second transaction overwrote the result of the first one. 10-09-2025 27 Module 4 - Concurrency Critical Section Primitives \u25aa Entry Section: The part of the program where a process requests permission to enter its critical section. \u25aa Critical Section: The code section where the process accesses shared resources exclusively. \u25aa Exit Section: The part where the process signals it has finished its critical section, allowing others to enter. \u25aa Remainder Section: The rest of the code outside the critical section, where no shared resource access occurs. Every process repeatedly executes these sections, and the challenge is to ensure only one process enters its critical section at a time, preventing race conditions. 10-09-2025 28 Module 4 - Concurrency Synchronization Working \u25aa Synchronization works by using special tools or mechanisms like locks, semaphores, or monitors to control access: \u25aa When a process wants to enter its critical section, it requests permission from the synchronization tool. \u25aa If no other process is in the critical section, permission is granted. \u25aa If the critical section is occupied, other processes wait until the resource is free. \u25aa After finishing, the process releases the lock or signals the synchronization tool, allowing others to enter. \u25aa These mechanisms ensure that processes take turns safely accessing shared resources without conflicts, preserving data accuracy and system stability. 10-09-2025 29 Module 4 - Concurrency Requirements of Synchronization \u25aa To solve the critical section problem, synchronization mechanisms must satisfy these three main requirements: \u25aa Mutual Exclusion: Only one process can be inside the critical section at any time. \u25aa Progress: If no process is in the critical section, and some processes want to enter, only those not in their remainder section (non-critical part) can decide who enters next immediately, ensuring no indefinite waiting. \u25aa Bounded Waiting: A process that wants to enter its critical section must have a limit on how many times others can enter before it is allowed, preventing starvation (waiting forever). 10-09-2025 30 Module 4 - Concurrency Solution 1 \u2013 Peterson\u2019s Solution \u25aa Peterson\u2019s solution is a classic software-based method to solve the critical section problem for two processes. \u25aa It uses two shared variables: \u25aa An array flag to indicate if a process wants to enter the critical section. \u25aa A variable turn to indicate which process\u2019s turn it is to enter. \u25aa Concept: \u25aa Each process announces its intention to enter by setting its flag"
  },
  {
    "id": 66,
    "source": "M4-2.txt",
    "text": "to enter its critical section must have a limit on how many times others can enter before it is allowed, preventing starvation (waiting forever). 10-09-2025 30 Module 4 - Concurrency Solution 1 \u2013 Peterson\u2019s Solution \u25aa Peterson\u2019s solution is a classic software-based method to solve the critical section problem for two processes. \u25aa It uses two shared variables: \u25aa An array flag to indicate if a process wants to enter the critical section. \u25aa A variable turn to indicate which process\u2019s turn it is to enter. \u25aa Concept: \u25aa Each process announces its intention to enter by setting its flag to true. \u25aa Then it gives turn to the other process to give it a chance. \u25aa Next, each process waits if the other wants to enter and it\u2019s the other\u2019s turn. \u25aa When the other process leaves the critical section, the waiting process proceeds. 10-09-2025 31 Module 4 - Concurrency Solution 1 \u2013 Peterson\u2019s Solution Process 0 Process 1 Action Explanation flag[0] = true; flag[1] = true; Each process signals its intention to enter the critical section by setting its own flag to true. turn = 1; turn = 0; Each process gives preference to the other by setting turn to the opponent\u2019s ID. This means it is willing to wait if the other process wants to enter. while (flag[1] == true && while (flag[0] == true && turn \u25aa Each process checks if the other process wants to turn == 1) == 0) enter (flag[other] == true) and if it is the other\u2019s Busy wait Busy wait turn (turn == other). \u25aa If both are true, the process waits (busy-waits) before entering the critical section. // Critical Section starts // // Critical Section starts // \u25aa When the while condition is false, the process access shared resources access shared resources here enters the critical section to access shared here // Critical Section // Critical Section ends resources exclusively. ends \u25aa No other process can enter because flag[other] or turn conditions block them. flag[0] = false; flag[1] = false; \u25aa After exiting the critical section, the process resets its flag to false, signaling it no longer needs exclusive access. \u25aa This allows the other process to enter if it is waiting. 10-09-2025 32 Module 4 - Concurrency Solution 2 \u2013 Bakery Algorithm \u25aa A classic solution to the critical section problem for N processes \u25aa It ensures mutual exclusion, meaning only one process can enter its critical section at a time; while also ensuring fairness by serving the processes in the order they request access. \u25aa Concept: \u25aa Each process wanting to enter its critical section picks a number. \u25aa The process with the smallest number gets to enter the critical section first. \u25aa If two processes have the same number, the one with the smaller process ID is given priority. \u25aa Numbers are assigned by taking one more than the maximum number currently held by any process. \u25aa When a process exits the critical section, it resets its"
  },
  {
    "id": 67,
    "source": "M4-2.txt",
    "text": "meaning only one process can enter its critical section at a time; while also ensuring fairness by serving the processes in the order they request access. \u25aa Concept: \u25aa Each process wanting to enter its critical section picks a number. \u25aa The process with the smallest number gets to enter the critical section first. \u25aa If two processes have the same number, the one with the smaller process ID is given priority. \u25aa Numbers are assigned by taking one more than the maximum number currently held by any process. \u25aa When a process exits the critical section, it resets its number to zero to indicate it no longer needs access. 10-09-2025 33 Module 4 - Concurrency Solution 2 \u2013 Bakery Algorithm \u25aa Data Structures Used: \u25aa choosing[i]: Boolean array indicating whether process i is choosing a number. \u25aa number[i]: Integer array holding the ticket number for process i. 10-09-2025 34 Module 4 - Concurrency Solution 2 \u2013 Bakery Algorithm Algorithm Explanation choosing[i] = true; \u25aa A process sets choosing[i] to true to indicate it is picking number[i] = 1 + max(number[0...N-1]); a number. choosing[i] = false; \u25aa It sets its number to one more than the maximum number taken by any process, ensuring unique increasing numbers. \u25aa Sets choosing[i] to false after picking the number. for (j = 0; j < N; j++) { \u25aa Waits for other processes: while (choosing[j]) { /* busy wait */ } \u25aa It waits if another process is currently choosing a number. while (number[j] != 0 && ( \u25aa It waits if another process has a smaller number, or the number[j] < number[i] || same number but a lower process ID (to break ties). (number[j] == number[i] && j < i) )) { /* busy wait */ } // Critical Section \u25aa When no other process with higher priority is waiting, it // (Access shared resource here) enters the critical section. // After critical section \u25aa After execution, it resets its number to 0, signaling it has number[i] = 0; left the critical section. 10-09-2025 35 Module 4 - Concurrency Solution 2 \u2013 Bakery Algorithm \u25aa The algorithm ensures mutual exclusion as no two processes can have the smallest number simultaneously. \u25aa It provides progress and bounded waiting, so no process has to wait indefinitely. \u25aa Suitable for any number of processes. \u25aa It uses only shared memory and no special hardware instructions. \u25aa However, it involves busy waiting (spinning) while waiting. 10-09-2025 36 Module 4 - Concurrency Solution 3 \u2013 H/w Based \u2013 Test & Set \u25aa Test and Set is a hardware atomic instruction that reads a memory location and sets it to 1 simultaneously. \u25aa It returns the original value before setting it. \u25aa Used to implement spinlocks or simple mutexes. \u25aa Working \u25aa A lock variable initialized to 0 means unlocked. \u25aa A process executes Test_and_Set(&lock): \u25aa If the returned value is 0, the process acquires the lock. \u25aa If it returns 1, the lock is already held, so the"
  },
  {
    "id": 68,
    "source": "M4-2.txt",
    "text": "However, it involves busy waiting (spinning) while waiting. 10-09-2025 36 Module 4 - Concurrency Solution 3 \u2013 H/w Based \u2013 Test & Set \u25aa Test and Set is a hardware atomic instruction that reads a memory location and sets it to 1 simultaneously. \u25aa It returns the original value before setting it. \u25aa Used to implement spinlocks or simple mutexes. \u25aa Working \u25aa A lock variable initialized to 0 means unlocked. \u25aa A process executes Test_and_Set(&lock): \u25aa If the returned value is 0, the process acquires the lock. \u25aa If it returns 1, the lock is already held, so the process keeps trying (busy waits). \u25aa This atomicity avoids race conditions on the lock variable. 10-09-2025 37 Module 4 - Concurrency Solution 3 \u2013 H/w Based \u2013 Test & Set int lock = 0; // 0 means unlocked, 1 means int TestAndSet(int *lock) { locked int old = *lock; // Read old value *lock = 1; // Set lock to 1 void acquire_lock() { return old; // Return old value while (TestAndSet(&lock) == 1) { } // Busy wait (spin) until lock becomes \u25aa When a process calls acquire_lock(), it repeatedly available calls TestAndSet(&lock). } // Lock acquired \u25aa If another process holds the lock (lock == 1), it keeps spinning. } \u25aa When the lock becomes free (lock == 0), the process void release_lock() { successfully sets it to 1 and enters the critical lock = 0; // Release the lock section. } \u25aa After finishing, it calls release_lock() to set lock back to 0. 10-09-2025 38 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap \u25aa It Is an atomic hardware instruction widely used in multithreading and multiprocessing environments to perform synchronization without locks. \u25aa CAS takes three parameters: \u25aa A memory location- p \u25aa An expected old value- old \u25aa A new value- new 10-09-2025 39 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap \u25aa It compares the current value at memory location p with old. \u25aa If the current value is equal to old, it swaps (updates) the value at p with new. \u25aa If the current value is not equal to old (meaning another thread/process changed it), it does nothing. \u25aa The operation executes atomically, guaranteeing no other thread can interrupt it during execution. \u25aa CAS returns a boolean or the original value indicating whether the swap was successful. 10-09-2025 40 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap \u25aa The CAS (Compare and Swap) hardware mechanism is an atomic instruction used to protect critical sections in concurrent programming. \u25aa It works by comparing the contents of a memory location with an expected old value, and if they are the same, it atomically swaps the memory location with a new value. \u25aa This operation is done as a single atomic step, ensuring no other thread can interfere during the comparison and update. 10-09-2025 41 Module 4 - Concurrency Solution 4 \u2013"
  },
  {
    "id": 69,
    "source": "M4-2.txt",
    "text": "swap was successful. 10-09-2025 40 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap \u25aa The CAS (Compare and Swap) hardware mechanism is an atomic instruction used to protect critical sections in concurrent programming. \u25aa It works by comparing the contents of a memory location with an expected old value, and if they are the same, it atomically swaps the memory location with a new value. \u25aa This operation is done as a single atomic step, ensuring no other thread can interfere during the comparison and update. 10-09-2025 41 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap \u25aa Suppose there is a shared memory location called lock which is initially 0 (meaning the critical section is free). \u25aa A thread wants to enter the critical section, so it expects the value in lock to be 0. \u25aa The thread executes CAS with these parameters: \u25aa expected old value = 0, \u25aa new value = 1 (meaning the thread wants to acquire the lock). \u25aa CAS checks if the current value of lock is indeed 0. \u25aa If yes, CAS atomically sets lock to 1 and returns success. \u25aa If no (another thread already changed it), CAS returns failure, and the thread must try again. 10-09-2025 42 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap lock = 0 // initial state function tryEnterCriticalSection() { expected = 0 newValue = 1 // Atomic CAS operation: // if (lock == expected) then lock = newValue else do nothing success = CAS(&lock, expected, newValue) if success { // Enter critical section ... // Release lock after critical section lock = 0 } else { // Failed to acquire lock, retry later } } 10-09-2025 43 Module 4 - Concurrency Solution 4 \u2013 H/w Based \u2013 Compare & Swap atomic_int lock = 0; // shared lock variable Thread 1/ Process 1 Thread 1 / Process 1 Wants to enter critical section Wants to enter critical section Calls CAS: Compare if lock == 0 Calls CAS: Compare if lock == 0 Reads lock, which is currently 0 Reads lock, which is currently 0 Compares it to expected old value 0 (match) Compares it to expected old value 0 (match) CAS operation is queued by CPU but not yet Since match, atomically swaps lock to 1 executed because CPU handles one at a time Reads updated lock, now 1 from Thread 1's Returns success (true), Thread 1 acquired lock successful CAS Enters critical section CAS fails because lock \u2260 0 (it's 1) Works inside critical section; other thread blocked Thread 2 retries CAS in a loop (busy waiting) Finishes critical section Thread 2 continuously retries CAS Sets lock back to 0 to release lock Eventually reads lock as 0 10-09-2025 44 Module 4 - Concurrency Solution 6 : Monitors \u25aa A monitor is a high-level synchronization tool designed to prevent conflicts when multiple processes or threads try to access a shared resource at"
  },
  {
    "id": 70,
    "source": "M4-2.txt",
    "text": "time Reads updated lock, now 1 from Thread 1's Returns success (true), Thread 1 acquired lock successful CAS Enters critical section CAS fails because lock \u2260 0 (it's 1) Works inside critical section; other thread blocked Thread 2 retries CAS in a loop (busy waiting) Finishes critical section Thread 2 continuously retries CAS Sets lock back to 0 to release lock Eventually reads lock as 0 10-09-2025 44 Module 4 - Concurrency Solution 6 : Monitors \u25aa A monitor is a high-level synchronization tool designed to prevent conflicts when multiple processes or threads try to access a shared resource at the same time. \u25aa Monitors simplify synchronization by bundling shared data and the procedures that operate on that data into a single unit, similar to a class in object-oriented programming. \u25aa It data is not accessed directly from outside the monitor; it can only be manipulated through the monitor's own procedures 10-09-2025 45 Module 4 - Concurrency Solution 6 : Monitors \u25aa The core principle of a monitor is mutual exclusion, which is enforced automatically. \u25aa This means a monitor allows only one thread or process to be active within it at any point in time. \u25aa If a second thread tries to enter the monitor while it's already occupied, it will be blocked and placed in an \"entry queue\" until the first thread exits. 10-09-2025 46 Module 4 - Concurrency Solution 6 : Monitors - Key Components \u2022 These are the variables or resources that need to be Shared Data protected from simultaneous access. This data is private to the monitor. \u2022 These are the functions that a process can call to interact with the shared data. A process outside the Procedures monitor can't access the data directly but can call these procedures. \u2022 This is a block of code that runs only once when the Initialization Code monitor is first created. It's used to set up the initial state of the shared data. \u2022 These are special variables used within the monitor Condition to manage the synchronization of processes. They Variables allow a process to wait for a specific condition to become true before proceeding 10-09-2025 47 Module 4 - Concurrency Solution 6 : Monitors - Key Components \u25aa While mutual exclusion prevents multiple threads from executing in the monitor simultaneously, condition variables handle more complex synchronization scenarios. They support two main operations: \u2022 When a process inside the monitor calls wait() on a condition variable, it is suspended and moved out of the monitor, allowing another process to enter. wait() The suspended process waits until another process signals that the condition it was waiting for has been met. \u2022 When a process calls signal() on a condition variable, it wakes up one of the processes that was suspended by a wait() call on the same condition signal() variable. The awakened process can then re-enter the monitor to continue its execution when the monitor is free. 10-09-2025 48 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher //"
  },
  {
    "id": 71,
    "source": "M4-2.txt",
    "text": "the monitor calls wait() on a condition variable, it is suspended and moved out of the monitor, allowing another process to enter. wait() The suspended process waits until another process signals that the condition it was waiting for has been met. \u2022 When a process calls signal() on a condition variable, it wakes up one of the processes that was suspended by a wait() call on the same condition signal() variable. The awakened process can then re-enter the monitor to continue its execution when the monitor is free. 10-09-2025 48 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Called by a philosopher to release forks // N is the number of philosophers // Private helper procedure to check if a philosopher can eat procedure putdown(int i) { procedure test(int i) { #define N 5 state[i] = THINKING; if ((state[LEFT] != EATING) && (state[i] #define LEFT (i + N - 1) % N // Check if neighbors can now == HUNGRY) && (state[RIGHT] != EATING)) { #define RIGHT (i + 1) % N eat state[i] = EATING; test(LEFT); // Define philosopher states // Signal the philosopher that they can now proceed test(RIGHT); self[i].signal(); } enum { THINKING, HUNGRY, EATING } } } state[N]; } // The code for each philosopher process // Monitor to manage the dining philosophers // Called by a philosopher to request forks procedure philosopher(int i) { monitor DiningSolution { procedure pickup(int i) { while (true) { state[i] = HUNGRY; // Condition variable for each philosopher think(); condition self[N]; // Philosopher is thinking // See if forks are available DiningSolution.pickup(i); test(i); // Request forks // Initialization: all philosophers start by thinking eat(); initialization_code() { // If not able to eat, wait // Philosopher is eating if (state[i] != EATING) { DiningSolution.putdown(i); for (int i = 0; i < N; i++) { self[i].wait(); // Release forks state[i] = THINKING; } } } } } } 10-09-2025 49 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // N is the number of philosophers #define N 5 Philosopher 5 Philosopher1 #define LEFT (i + N - 1) % N #define RIGHT (i + 1) % N // Define philosopher states An array tracks the state of each philosopher: enum { THINKING, HUNGRY, EATING } THINKING, HUNGRY, or EATING. This shared Philosopher 4 Philosopher 2 data is protected by the monitor. state[N]; // Monitor to manage the dining philosophers Philosopher 3 monitor DiningSolution { A high-level structure that encapsulates the shared state array and the procedures that // Condition variable for each philosopher modify it, ensuring mutual exclusion. condition self[N]; An array of condition variables, one for each philosopher. A philosopher who is hungry but cannot get forks will wait on their own condition // Initialization: all philosophers start by thinking variable. initialization_code() { Thinking Thinking Thinking Thinking Thinking for (int i = 0; i < N; i++) { state[i] = THINKING; } } 10-09-2025 50 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure"
  },
  {
    "id": 72,
    "source": "M4-2.txt",
    "text": "Philosopher 3 monitor DiningSolution { A high-level structure that encapsulates the shared state array and the procedures that // Condition variable for each philosopher modify it, ensuring mutual exclusion. condition self[N]; An array of condition variables, one for each philosopher. A philosopher who is hungry but cannot get forks will wait on their own condition // Initialization: all philosophers start by thinking variable. initialization_code() { Thinking Thinking Thinking Thinking Thinking for (int i = 0; i < N; i++) { state[i] = THINKING; } } 10-09-2025 50 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure to check if a philosopher P2 becomes hungry: can eat P2 calls pickup(2). Inside the monitor, state becomes procedure test(int i) { Philosopher 4 Philosopher 0 HUNGRY. if ((state[LEFT] != EATING) && (state[i] == HUNGRY) && (state[RIGHT] != EATING)) { state[i] = EATING; // Signal the philosopher that they can now proceed Philosopher 3 self[i].signal(); Philosopher 1 } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Thinking Hungry Thinking Thinking self[i].wait(); } } 10-09-2025 51 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure to check if a philosopher P2 becomes hungry: can eat P2 calls pickup(2). Inside the monitor, state becomes procedure test(int i) { Philosopher 4 Philosopher 0 HUNGRY. if ((state[LEFT] != EATING) && (state[i] == test(2) is called. It checks P1 and P3, who are THINKING HUNGRY) && (state[RIGHT] != EATING)) { state[i] = EATING; // Signal the philosopher that they can now proceed Philosopher 3 self[i].signal(); Philosopher 1 } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Thinking Hungry Thinking Thinking self[i].wait(); } } 10-09-2025 52 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure to check if a philosopher P2 becomes hungry: can eat P2 calls pickup(2). Inside the monitor, state becomes procedure test(int i) { Philosopher 4 Philosopher 0 HUNGRY. if ((state[LEFT] != EATING) && (state[i] == test(2) is called. It checks P1 and P3, who are THINKING HUNGRY) && (state[RIGHT] != EATING)) { The condition is true, so state is set to EATING, state[i] = EATING; and self.signal() is called (which has no effect as P2 isn't waiting). // Signal the philosopher that they can now proceed Philosopher 3 self[i].signal(); Philosopher 1 P2 exits the monitor and starts EATING. } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Thinking Eating Thinking Thinking self[i].wait(); } } 10-09-2025 53 Module 4 - Concurrency Solution 6: Monitors \u2013"
  },
  {
    "id": 73,
    "source": "M4-2.txt",
    "text": "is true, so state is set to EATING, state[i] = EATING; and self.signal() is called (which has no effect as P2 isn't waiting). // Signal the philosopher that they can now proceed Philosopher 3 self[i].signal(); Philosopher 1 P2 exits the monitor and starts EATING. } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Thinking Eating Thinking Thinking self[i].wait(); } } 10-09-2025 53 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure to check if a philosopher P1 becomes hungry: can eat P1 calls pickup(1). Inside the monitor, state becomes procedure test(int i) { Philosopher 4 Philosopher 0 HUNGRY. if ((state[LEFT] != EATING) && (state[i] == HUNGRY) && (state[RIGHT] != EATING)) { state[i] = EATING; // Signal the philosopher that they can now proceed Philosopher 3 self[i].signal(); Philosopher 1 } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Hungry Eating Thinking Thinking self[i].wait(); } } 10-09-2025 54 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure to check if a philosopher P1 becomes hungry: can eat P1 calls pickup(1). Inside the monitor, state becomes procedure test(int i) { Philosopher 4 Philosopher 0 HUNGRY. if ((state[LEFT] != EATING) && (state[i] == test(1) is called. It checks P0 (THINKING) and P2 HUNGRY) && (state[RIGHT] != EATING)) { (EATING). state[i] = EATING; The condition state != EATING is false. Nothing happens. // Signal the philosopher that they can now proceed Philosopher 3 self[i].signal(); Philosopher 1 } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Hungry Eating Thinking Thinking self[i].wait(); } } 10-09-2025 55 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Private helper procedure to check if a philosopher P1 becomes hungry: can eat P1 calls pickup(1). Inside the monitor, state becomes procedure test(int i) { Philosopher 4 Philosopher 0 HUNGRY. if ((state[LEFT] != EATING) && (state[i] == test(1) is called. It checks P0 (THINKING) and P2 HUNGRY) && (state[RIGHT] != EATING)) { (EATING). state[i] = EATING; The condition state != EATING is false. Nothing happens. Back in pickup(1), // Signal the philosopher that they can now proceed state is still HUNGRY, Philosopher 3 self[i].signal(); Philosopher 1 so P1 calls self.wait(). P1 is now blocked and waiting. } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Hungry Eating Thinking Thinking self[i].wait(); } } 10-09-2025"
  },
  {
    "id": 74,
    "source": "M4-2.txt",
    "text": "(THINKING) and P2 HUNGRY) && (state[RIGHT] != EATING)) { (EATING). state[i] = EATING; The condition state != EATING is false. Nothing happens. Back in pickup(1), // Signal the philosopher that they can now proceed state is still HUNGRY, Philosopher 3 self[i].signal(); Philosopher 1 so P1 calls self.wait(). P1 is now blocked and waiting. } } Philosopher 2 // Called by a philosopher to request forks procedure pickup(int i) { state[i] = HUNGRY; // See if forks are available test(i); // If not able to eat, wait if (state[i] != EATING) { Thinking Hungry Eating Thinking Thinking self[i].wait(); } } 10-09-2025 56 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Called by a philosopher to release forks P2 finishes eating: procedure putdown(int i) { P2 calls putdown(2) Inside the monitor, state becomes state[i] = THINKING; Philosopher 4 Philosopher 0 THINKING // Check if neighbors can now eat putdown(2) calls test(LEFT), which is test(1) test(LEFT); The condition is now true. state is set to test(RIGHT); EATING self.signal() is called, which wakes up } the waiting P1. } Philosopher 3 Philosopher 1 // The code for each philosopher process procedure philosopher(int i) { Philosopher 2 while (true) { think(); // Philosopher is thinking DiningSolution.pickup(i); // Request forks eat(); // Philosopher is eating DiningSolution.putdown(i); Thinking Hungry Thinking Thinking Thinking // Release forks } } 10-09-2025 57 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Called by a philosopher to release forks P2 finishes eating: procedure putdown(int i) { P2 calls putdown(2) Inside the monitor, state becomes state[i] = THINKING; Philosopher 4 Philosopher 0 THINKING // Check if neighbors can now eat putdown(2) calls test(LEFT), which is test(1) test(LEFT); test(1) runs. test(RIGHT); P1 is HUNGRY, and its neighbors P0 and P2 are THINKING. } } Philosopher 3 Philosopher 1 // The code for each philosopher process procedure philosopher(int i) { Philosopher 2 while (true) { think(); // Philosopher is thinking DiningSolution.pickup(i); // Request forks eat(); // Philosopher is eating DiningSolution.putdown(i); Thinking Hungry Thinking Thinking Thinking // Release forks } } 10-09-2025 58 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Called by a philosopher to release forks P2 finishes eating: procedure putdown(int i) { P2 calls putdown(2) Inside the monitor, state becomes state[i] = THINKING; Philosopher 4 Philosopher 0 THINKING // Check if neighbors can now eat putdown(2) calls test(LEFT), which is test(1) test(LEFT); test(1) runs. test(RIGHT); P1 is HUNGRY, and its neighbors P0 and P2 are THINKING. } putdown(2) then calls test(RIGHT), } which is test(3). Philosopher 3 Philosopher 1 Assuming P3 is not hungry, nothing happens. // The code for each philosopher process P2 exits the monitor and returns to THINKING procedure philosopher(int i) { Philosopher 2 while (true) { think(); // Philosopher is thinking DiningSolution.pickup(i); // Request forks eat(); // Philosopher is eating DiningSolution.putdown(i); Thinking Hungry Thinking Thinking Thinking // Release forks } } 10-09-2025 59 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Called by a philosopher to release forks P1 resumes: procedure"
  },
  {
    "id": 75,
    "source": "M4-2.txt",
    "text": "HUNGRY, and its neighbors P0 and P2 are THINKING. } putdown(2) then calls test(RIGHT), } which is test(3). Philosopher 3 Philosopher 1 Assuming P3 is not hungry, nothing happens. // The code for each philosopher process P2 exits the monitor and returns to THINKING procedure philosopher(int i) { Philosopher 2 while (true) { think(); // Philosopher is thinking DiningSolution.pickup(i); // Request forks eat(); // Philosopher is eating DiningSolution.putdown(i); Thinking Hungry Thinking Thinking Thinking // Release forks } } 10-09-2025 59 Module 4 - Concurrency Solution 6: Monitors \u2013 Dining-Philosopher // Called by a philosopher to release forks P1 resumes: procedure putdown(int i) { P1 wakes up from its wait call, re-acquires the monitor lock, state[i] = THINKING; Philosopher 4 Philosopher 0 and its pickup(1) procedure completes. P1 now starts EATING. // Check if neighbors can now eat test(LEFT); test(RIGHT); } } Philosopher 3 Philosopher 1 // The code for each philosopher process procedure philosopher(int i) { Philosopher 2 while (true) { think(); // Philosopher is thinking DiningSolution.pickup(i); // Request forks eat(); // Philosopher is eating DiningSolution.putdown(i); Thinking Eating Thinking Thinking Thinking // Release forks } } 10-09-2025 60 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa To use a monitor to solve the Dining Philosophers problem, you encapsulate the shared resources (the forks) and the operations on them within a monitor. \u25aa This provides automatic mutual exclusion, ensuring only one philosopher can access the monitor's code at a time. \u25aa The monitor also uses condition variables to manage the state of each philosopher, preventing deadlock by only allowing a philosopher to pick up both forks simultaneously when they become available. 10-09-2025 67 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa Monitor structure \u25aa States: An array state[5] tracks each philosopher's state (0-4), which can be THINKING, HUNGRY, or EATING. \u25aa Condition variables: An array of condition variables self[5] blocks a philosopher if they are HUNGRY but cannot eat. \u25aa Monitor functions: The monitor includes three main functions: \u25aa pickup(i): Called by philosopher i when they want to eat. \u25aa putdown(i): Called by philosopher i when they finish eating. \u25aa test(i): An internal helper function that checks if philosopher i can start eating. 10-09-2025 68 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa Initial State \u25aa All five philosophers (P0-P4) are THINKING. \u25aa All five forks (F0-F4) are available. \u25aa The state array is all THINKING. \u25aa The self condition variables are all empty. 10-09-2025 69 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher monitor DiningPhilosophers enum state {THINKING, HUNGRY, EATING}; state philosopher_state[5]; condition self[5]; 10-09-2025 70 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher // Helper function to check and allow eating private procedure test(i) // Check if philosopher `i` is hungry and both neighbors are not eating if (philosopher_state[i] == HUNGRY AND philosopher_state[(i + 4) % 5] != EATING AND philosopher_state[(i + 1) % 5] != EATING) then philosopher_state[i] = EATING;"
  },
  {
    "id": 76,
    "source": "M4-2.txt",
    "text": "\u25aa The state array is all THINKING. \u25aa The self condition variables are all empty. 10-09-2025 69 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher monitor DiningPhilosophers enum state {THINKING, HUNGRY, EATING}; state philosopher_state[5]; condition self[5]; 10-09-2025 70 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher // Helper function to check and allow eating private procedure test(i) // Check if philosopher `i` is hungry and both neighbors are not eating if (philosopher_state[i] == HUNGRY AND philosopher_state[(i + 4) % 5] != EATING AND philosopher_state[(i + 1) % 5] != EATING) then philosopher_state[i] = EATING; self[i].signal(); // Wake up philosopher `i` if they are waiting end if end procedure 10-09-2025 71 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher // Called by a philosopher `i` when they want to eat public procedure pickup(i) philosopher_state[i] = HUNGRY; test(i); // Attempt to start eating if (philosopher_state[i] != EATING) then self[i].wait(); // Wait if unable to eat end if end procedure 10-09-2025 72 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher // Called by a philosopher `i` when they finish eating public procedure putdown(i) philosopher_state[i] = THINKING; test((i + 4) % 5); // Check if the left neighbor can now eat test((i + 1) % 5); // Check if the right neighbor can now eat end procedure 10-09-2025 73 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher // Initialization block initialization_code() for i = 0 to 4 philosopher_state[i] = THINKING; end for end initialization_code end monitor 10-09-2025 74 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher ### Philosopher process The pseudocode for each philosopher's individual process shows how it interacts with the monitor to coordinate its eating and thinking cycle. ```pseudocode procedure philosopher(i) while (true) // The philosopher is thinking think(); // The philosopher is hungry and wants to eat DiningPhilosophers.pickup(i); // The philosopher is eating eat(); // The philosopher has finished eating DiningPhilosophers.putdown(i); end while end procedure ``` 10-09-2025 75 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa monitor DiningPhilosophers: This declares the beginning of a monitor named DiningPhilosophers. The monitor is a high-level synchronization construct that provides automatic mutual exclusion, meaning only one process or thread can be active inside the monitor's code at any given time. \u25aa enum state {THINKING, HUNGRY, EATING};: This line defines an enumeration to represent the three possible states of each philosopher. \u25aa THINKING: The philosopher is not trying to eat. \u25aa HUNGRY: The philosopher wants to eat and is trying to acquire the forks. \u25aa EATING: The philosopher has acquired both forks and is eating. \u25aa state philosopher_state[5];: An array philosopher_state is declared to store the state of each of the five philosophers. \u25aa condition self[5];: An array of condition variables self is declared, one for each philosopher. A condition variable is a queue for threads that are waiting for a specific condition to become true. In this case, a philosopher waits on self[i]"
  },
  {
    "id": 77,
    "source": "M4-2.txt",
    "text": "the three possible states of each philosopher. \u25aa THINKING: The philosopher is not trying to eat. \u25aa HUNGRY: The philosopher wants to eat and is trying to acquire the forks. \u25aa EATING: The philosopher has acquired both forks and is eating. \u25aa state philosopher_state[5];: An array philosopher_state is declared to store the state of each of the five philosophers. \u25aa condition self[5];: An array of condition variables self is declared, one for each philosopher. A condition variable is a queue for threads that are waiting for a specific condition to become true. In this case, a philosopher waits on self[i] when they are hungry but cannot eat 10-09-2025 76 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa private procedure test(i): This helper procedure is an internal monitor function. It is not called directly by the philosophers outside the monitor. The function checks if a philosopher i can transition from the HUNGRY to the EATING state. \u25aa if (philosopher_state[i] == HUNGRY AND ...): This is the safety check that prevents deadlock. It tests three conditions: \u25aa philosopher_state[i] == HUNGRY: Checks if the philosopher i is trying to eat. \u25aa philosopher_state[(i + 4) % 5] != EATING: Checks if the philosopher's left neighbor is not eating. The modulo operator % 5 handles the circular table. For philosopher 0, the left neighbor is philosopher 4. \u25aa philosopher_state[(i + 1) % 5] != EATING: Checks if the philosopher's right neighbor is not eating. For philosopher 4, the right neighbor is philosopher 0. \u25aa then philosopher_state[i] = EATING;: If all conditions are met, the philosopher's state is updated to EATING. This signals that the philosopher can now acquire both forks. \u25aa self[i].signal();: This operation wakes up a single thread waiting on the self[i] condition variable. If the philosopher i was waiting to eat, this signal allows them to proceed. If they weren't waiting, the signal has no effect. 10-09-2025 77 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa public procedure pickup(i): This procedure is called by philosopher i when they become hungry and wish to eat. \u25aa philosopher_state[i] = HUNGRY;: The philosopher's state is immediately updated to HUNGRY. \u25aa test(i);: The test procedure is called to see if the philosopher can begin eating. \u25aa if (philosopher_state[i] != EATING) then: If the test(i) call failed (because a neighbor was eating), this condition will be true. \u25aa self[i].wait();: The philosopher is blocked and put in a waiting queue on the condition variable self[i]. This also releases the monitor's lock, allowing another philosopher to enter the monitor. 10-09-2025 78 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa public procedure putdown(i): This procedure is called by philosopher i when they finish eating. \u25aa philosopher_state[i] = THINKING;: The philosopher's state is updated to THINKING. \u25aa test((i + 4) % 5);: The test procedure is called for the left neighbor. If that neighbor was HUNGRY and now can eat, they will be signaled. \u25aa test((i + 1) % 5);:"
  },
  {
    "id": 78,
    "source": "M4-2.txt",
    "text": "philosopher is blocked and put in a waiting queue on the condition variable self[i]. This also releases the monitor's lock, allowing another philosopher to enter the monitor. 10-09-2025 78 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa public procedure putdown(i): This procedure is called by philosopher i when they finish eating. \u25aa philosopher_state[i] = THINKING;: The philosopher's state is updated to THINKING. \u25aa test((i + 4) % 5);: The test procedure is called for the left neighbor. If that neighbor was HUNGRY and now can eat, they will be signaled. \u25aa test((i + 1) % 5);: The test procedure is called for the right neighbor, potentially allowing them to eat. 10-09-2025 79 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher \u25aa procedure philosopher(i): This outlines the behavior of an individual philosopher. \u25aa think();: Represents the time a philosopher spends thinking. \u25aa DiningPhilosophers.pickup(i);: The philosopher calls the monitor procedure to acquire the forks. This call will block if the forks are not available. \u25aa eat();: Once the pickup(i) call completes, the philosopher has both forks and can eat. \u25aa DiningPhilosophers.putdown(i);: After eating, the philosopher releases the forks by calling the putdown procedure inside the monitor. 10-09-2025 80 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: All philosophers become hungry \u25aa P0 calls pickup(0): \u25aa P0 enters the monitor and acquires the lock. \u25aa state[0] is set to HUNGRY. \u25aa The test(0) function is called. \u25aa test(0) checks if P0's neighbors are eating. \u25aa Since P4 and P1 are THINKING, the condition is met. \u25aa state[0] is set to EATING, and the self[0].signal() operation is called. Since P0 is not waiting, the signal has no effect. \u25aa P0 now has both forks (F0 and F1) and exits the monitor, releasing the lock. 10-09-2025 81 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: All philosophers become hungry \u25aa P1 calls pickup(1): \u25aa P1 enters the monitor. \u25aa state[1] is set to HUNGRY. \u25aa test(1) is called. \u25aa test(1) checks if P0 and P2 are eating. Since P0 is EATING, the condition is not met. \u25aa P1 calls self[1].wait() and is blocked. \u25aa It releases the monitor lock and is added to the waiting queue for self[1]. 10-09-2025 82 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: All philosophers become hungry \u25aa P2 calls pickup(2): \u25aa P2 enters the monitor. \u25aa state[2] is set to HUNGRY. \u25aa test(2) is called. \u25aa test(2) checks if P1 and P3 are eating. Since P1 is HUNGRY, the condition is not met. \u25aa P2 calls self[2].wait(), releases the lock, and is blocked on self[2]. 10-09-2025 83 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: All philosophers become hungry \u25aa P3 calls pickup(3): \u25aa P3 enters the monitor. \u25aa state[3] is set to HUNGRY. \u25aa test(3) is called. \u25aa test(3) checks if P2 and P4 are eating. Since P2 is HUNGRY, the condition is not met."
  },
  {
    "id": 79,
    "source": "M4-2.txt",
    "text": "\u25aa P2 enters the monitor. \u25aa state[2] is set to HUNGRY. \u25aa test(2) is called. \u25aa test(2) checks if P1 and P3 are eating. Since P1 is HUNGRY, the condition is not met. \u25aa P2 calls self[2].wait(), releases the lock, and is blocked on self[2]. 10-09-2025 83 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: All philosophers become hungry \u25aa P3 calls pickup(3): \u25aa P3 enters the monitor. \u25aa state[3] is set to HUNGRY. \u25aa test(3) is called. \u25aa test(3) checks if P2 and P4 are eating. Since P2 is HUNGRY, the condition is not met. \u25aa P3 calls self[3].wait(), releases the lock, and is blocked on self[3]. 10-09-2025 84 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: All philosophers become hungry \u25aa P4 calls pickup(4): \u25aa P4 enters the monitor. \u25aa state[4] is set to HUNGRY. \u25aa test(4) is called. \u25aa test(4) checks if P3 and P0 are eating. Since P0 is EATING, the condition is not met. \u25aa P4 calls self[4].wait(), releases the lock, and is blocked on self[4]. 10-09-2025 85 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: P0 finishes eating P0 calls putdown(0): P0 enters the monitor and acquires the lock. state[0] is set to THINKING. test() is called for P0's neighbors: test(4): P4 is HUNGRY and its neighbors (P3 and P0) are now THINKING and THINKING, respectively. P4's condition is met. state[4] is set to EATING. self[4].signal() is called, which wakes up P4 from its waiting queue. test(1): P1 is HUNGRY and its neighbors (P0 and P2) are THINKING and HUNGRY, respectively. P1's condition is not met yet, so no signal is sent. P0 exits the monitor, releasing the lock. P4 now has both forks (F4 and F0) and begins eating. 10-09-2025 86 Module 4 - Concurrency Solution 5 \u2013 Monitors \u2013 Dining Philosopher Scenario: P4 finishes eating P4 calls putdown(4): P4 enters the monitor. state[4] is set to THINKING. test() is called for P4's neighbors: test(3): P3 is HUNGRY and its neighbors (P2 and P4) are HUNGRY and THINKING, respectively. The condition is not yet met. test(0): P0 is THINKING and is not hungry. No action needed. P4 exits the monitor. 10-09-2025 87"
  },
  {
    "id": 80,
    "source": "M7_1.txt",
    "text": "Storage Management, Protection and Security Module 7 Dr. Naveenkumar J Associate Professor, PRP- 217 - 4 Module 6 - Virtualization System Threats and Security \u2751Operating System (OS) security refers to the measures taken to protect a computer system's resources\u2014such as the CPU, memory, disks, and data\u2014from unauthorized access, malicious attacks, and interference \u2751 Security systems aim to ensure confidentiality, integrity, and availability of system resources against external threats. 06-11-2025 2 Module 6 - Virtualization System Threats and Security \u2751 System Threats \u2751 A system threat is any program or event that has the potential to cause damage to a computer system. \u2751 These threats can be intentional or accidental and are typically launched by non-users or unauthorized programs from outside the operating system. 06-11-2025 3 Module 6 - Virtualization System Threats and Security \u2751A virus is a self-replicating piece of code that attaches itself to legitimate programs. When the host program is executed, the virus also runs, potentially corrupting or destroying files and spreading to other programs. \u2751A Trojan horse is a malicious program disguised as useful software. It performs a hidden, destructive function while appearing to do something benign. Unlike viruses, Trojans do not replicate themselves. \u2751Trap Door (or Backdoor) is a secret entry point into a program that allows someone to gain access while bypassing normal security measures. It may be left by the original developer for testing or intentionally placed for malicious purposes 06-11-2025 4 Module 6 - Virtualization System Threats and Security \u2751Logic Bomb This is a piece of code intentionally inserted into a software system that will trigger a malicious function when specified conditions are met. The trigger could be a specific date, time, or the absence of a particular file \u2751Buffer Overflow This occurs when a program attempts to write more data into a fixed- length memory block (a buffer) than it can hold. The excess data overwrites adjacent memory, which can corrupt data, crash the program, or, in a targeted attack, execute malicious code with the privileges of the compromised application. \u2751Privilege Escalation This attack involves a user with limited permissions exploiting a vulnerability to gain elevated access (e.g., administrator or root privileges). 06-11-2025 5 Module 6 - Virtualization Policy vs. Mechanism \u2751 In the context of OS security, policy and mechanism are distinct concepts that work together to enforce security rules Policy Mechanism A security policy defines what needs to be A security mechanism is the implementation or secured. It is a set of high-level rules and tool used to enforce a policy. It provides the objectives that specify the desired security how\u2014the specific methods, functions, and goals for a system. Policies are about the goals, hardware used to achieve the security goals not the implementation. defined by the policy. A company policy might state, \"Only To enforce the policy, the operating system employees from the Finance department are could use an Access Control List (ACL) as a allowed to view and modify payroll files.\" This mechanism. The ACL for the payroll"
  },
  {
    "id": 81,
    "source": "M7_1.txt",
    "text": "security mechanism is the implementation or secured. It is a set of high-level rules and tool used to enforce a policy. It provides the objectives that specify the desired security how\u2014the specific methods, functions, and goals for a system. Policies are about the goals, hardware used to achieve the security goals not the implementation. defined by the policy. A company policy might state, \"Only To enforce the policy, the operating system employees from the Finance department are could use an Access Control List (ACL) as a allowed to view and modify payroll files.\" This mechanism. The ACL for the payroll files rule does not specify how to enforce this would be configured to grant read and write restriction. permissions only to users who are members of the \"Finance\" group. 06-11-2025 6 Module 6 - Virtualization Access vs. Authentication \u2751 Authentication and access (via authorization) are sequential steps in a secure process. Authentication confirms identity, while access control determines what an identified user can do. Authentication Authorization ( Governs Access) To determine the permissions and access rights of an To verify a user's identity and ensure they are who they authenticated user . It answers the question, \"What are claim to be . It answers the question, \"Who are you?\" you allowed to do?\" Occurs after successful authentication . The system Occurs before authorization . The user provides checks the user's permissions against the requested credentials that the system validates. resource. Requires information like a username/password, Relies on policies, roles, or permissions (e.g., read, biometric data (fingerprint, retina scan), or a security write, execute) assigned to the user's identity token/key . Users can typically manage their own authentication Permissions are granted by a system administrator or credentials, such as changing a password . resource owner and cannot be changed by the user . \u2751Access Control is the practical application of authorization. Once a user is authenticated and their authorization level is determined, the access control mechanism enforces this policy, either granting or denying the user's request to access a resource. 06-11-2025 7 Module 7 \u2013 System Protection System Protection \u2751System protection is concerned with controlling access to resources within a computer system. \u2751 The goal is to ensure that processes, users, and programs can only access the objects (like files, devices, or memory segments) for which they have been granted authorization, and only in the manner specified. \u2751Two fundamental models for this are the \u2751Access Matrix and \u2751Capability-Based Systems. 06-11-2025 8 Module 6 - Virtualization System Protection : Access Matrix \u2751 An Access Matrix is a conceptual model used to define the access rights of subjects to objects. \u2751Subjects: These are the active entities that request access, such as users or processes. The rows of the matrix represent subjects (often grouped into domains of execution). \u2751Objects: These are the passive resources that need protection, such as files, printers, or other devices. The columns of the matrix represent objects. \u2751Access Rights: The cells of the matrix, Access(i, j), contain the set of"
  },
  {
    "id": 82,
    "source": "M7_1.txt",
    "text": "this are the \u2751Access Matrix and \u2751Capability-Based Systems. 06-11-2025 8 Module 6 - Virtualization System Protection : Access Matrix \u2751 An Access Matrix is a conceptual model used to define the access rights of subjects to objects. \u2751Subjects: These are the active entities that request access, such as users or processes. The rows of the matrix represent subjects (often grouped into domains of execution). \u2751Objects: These are the passive resources that need protection, such as files, printers, or other devices. The columns of the matrix represent objects. \u2751Access Rights: The cells of the matrix, Access(i, j), contain the set of operations that a subject in domain i can perform on object j. Examples of rights include read, write, execute, and own. 06-11-2025 9 Module 6 - Virtualization System Protection : Access Matrix \u2751Imagine a small company with three users (acting as subjects) and four resources (objects): \u2751Subjects/Domains: \u2751D1: Alice (a project manager) \u2751D2: Bob (a finance analyst) \u2751D3: Charlie (a developer) \u2751Objects: \u2751File1: Project_Plan.docx \u2751File2: Financials.xlsx \u2751File3: Source_Code.c \u2751Object1: Printer 06-11-2025 10 Module 6 - Virtualization System Protection : Access Matrix \u2751The security policy can be represented by the following Access Matrix: Domain / Object File1 (Project Plan) File2 (Financials) File3 (Source Code) Object1 (Printer) D1: Alice read,write read print D2: Bob read read,write print D3: Charlie read read,write \u2751 The Access Matrix itself is an abstract model. In a real OS, it is too large and sparse to be stored as a simple table. Instead, it is implemented in one of two primary ways: Access Control Lists or Capability Lists 06-11-2025 11 Module 6 - Virtualization System Protection : Access Matrix \u2751 Access Control Lists (ACLs) - A Column-based View: The matrix is broken down by columns (objects). Each object has a list (an ACL) attached to it that specifies which subjects have what rights. \u2751Example: The Financials.xlsx file would have an ACL like this: (D1:Alice, {read}), (D2:Bob, {read, write}). \u2751When Alice tries to write to the file, the system checks this list, sees she only has read permission, and denies the operation. 06-11-2025 12 Module 6 - Virtualization System Protection : Capability Lists \u2751Capability Lists - A Row-based View: The matrix is broken down by rows (subjects). \u2751 Each subject has a list of \"capabilities,\" where each capability specifies an object and the access rights for it. This forms the basis for capability-based systems. \u2751Example: Alice would possess a capability list: (File1, {read, write}), (File2, {read}), (Object1, {print}). 06-11-2025 13 Module 6 - Virtualization Capability-Based Systems \u2751A capability-based system takes the concept of a capability list and makes it the central security mechanism. In this model, a capability is like a key that gives the holder specific rights to an object. \u2751 A capability is a data structure that contains two key pieces of information: \u2751 A unique pointer or identifier to an object. \u2751 The set of access rights for that object (e.g., read, write). \u2751The operating system kernel ensures that capabilities cannot be forged or modified"
  },
  {
    "id": 83,
    "source": "M7_1.txt",
    "text": "a capability list: (File1, {read, write}), (File2, {read}), (Object1, {print}). 06-11-2025 13 Module 6 - Virtualization Capability-Based Systems \u2751A capability-based system takes the concept of a capability list and makes it the central security mechanism. In this model, a capability is like a key that gives the holder specific rights to an object. \u2751 A capability is a data structure that contains two key pieces of information: \u2751 A unique pointer or identifier to an object. \u2751 The set of access rights for that object (e.g., read, write). \u2751The operating system kernel ensures that capabilities cannot be forged or modified by user-level processes. The core principle is: possession of a capability is proof of the right to access the object. 06-11-2025 14"
  },
  {
    "id": 84,
    "source": "M5.txt",
    "text": "Memory Management Module 5 Dr. Naveenkumar J Associate Professor, PRP- 217 - 4 Module 5 - Memory Management Memory Management \uf0a7 The OS function/Service responsible for controlling and coordinating a computer's primary memory (RAM) \uf0a7 It ensures efficient allocation and deallocation of memory spaces to various processes while they are running, so that each process has the memory it needs without conflicting with others. \uf0a7 Memory management keeps track of which parts of memory are in use, by which processes, and which parts are free, facilitating optimal utilization of the available memory resources. 23/09/202 2 5 Module 5 - Memory Management Memory Management \u2013 Why Necessary? \uf0a7 To allocate and free memory space to processes before and after their execution. \uf0a7 To keep track of allocated and free memory locations. \uf0a7 To minimize fragmentation and ensure proper utilization of the main memory. \uf0a7 To maintain data integrity and prevent one process from corrupting the memory of another. \uf0a7 To enable multitasking, allowing multiple processes to reside in memory and run concurrently. 23/09/202 3 5 Module 5 - Memory Management Memory (RAM) \u2013 How it is organized Primary memory or RAM (Random Access Memory) is organized into a large array of \uf0a7 storage cells, each capable of holding a fixed number of bits (usually 8 bits or 1 byte). These cells are grouped into words (e.g., 16, 32, or 64 bits) for storage and retrieval. \uf0a7 RAM is essentially an addressable memory array with a series of rows and columns \uf0a7 Each memory cell has a unique address that the CPU or memory controller uses for \uf0a7 read/write operations. Memory addressing uses binary numbers where the number of address lines \uf0a7 determines the total addressable memory size (e.g., 16 address lines allow addressing 2^16 memory locations). Internally, RAM does not distinguish between code or data; it only sees bits stored at \uf0a7 particular addresses. The basic operations are reading (retrieving bits from a given address) and writing \uf0a7 (storing bits at a given address). 23/09/202 4 5 Module 5 - Memory Management Memory (RAM) \u2013 How it is organized \uf0a7 The OS sees RAM as a large pool of continuous addressable memory. \uf0a7 It manages memory by dividing RAM into fixed or variable-sized partitions allocated to processes. \uf0a7 The OS treats RAM using abstractions like logical/virtual addresses which it maps to physical addresses in RAM via memory management units. \uf0a7 It uses techniques like paging and segmentation to provide efficient, protected, and sometimes non-contiguous use of RAM. 23/09/202 5 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection \uf0a7 Memory hardware protection is a mechanism used by operating systems and hardware to prevent a process from accessing memory segments that are outside its allocated range. \uf0a7 This protects the system from bugs or malicious code that could corrupt or access other processes' memory or the OS kernel's memory. 23/09/202 6 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection - Working \uf0a7 The system uses special hardware"
  },
  {
    "id": 85,
    "source": "M5.txt",
    "text": "uses techniques like paging and segmentation to provide efficient, protected, and sometimes non-contiguous use of RAM. 23/09/202 5 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection \uf0a7 Memory hardware protection is a mechanism used by operating systems and hardware to prevent a process from accessing memory segments that are outside its allocated range. \uf0a7 This protects the system from bugs or malicious code that could corrupt or access other processes' memory or the OS kernel's memory. 23/09/202 6 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection - Working \uf0a7 The system uses special hardware registers, primarily the Base Register and Limit Register, to enforce protection: \uf0a7 Base Register (Relocation Register): Holds the starting physical address of the memory block allocated to a process. The base address tells where the process begins in memory. \uf0a7 Limit Register: Holds the size (or range) of the allocated memory block. The value in the limit register is typically the length of the addressable memory region for that process, starting from the base address. The limit tells how much memory (in terms of size) the process can access, thus defining the upper boundary of accessible memory as base + limit 23/09/202 7 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection - Working \uf0a7 When CPU generates a logical address during Process execution, it is first checked against the limit register. \uf0a7 The logical address must be less than the limit value; if the logical address is equal to or exceeds the limit, it indicates an invalid memory access, and the hardware raises a protection fault (such as segmentation or memory violation error). \uf0a7 If the address is valid (less than the limit), the contents of the base register are added to the logical address to form the physical address. Physical Address in RAM = Value in base Register + logical address \uf0a7 This physical address is then used by the memory to read or write data. 23/09/202 8 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection - Working 23/09/202 9 5 Module 5 - Memory Management Memory (RAM) \u2013 Hardware Protection - Working \uf0a7 The base register serves as a relocation register marking where in physical memory the process\u2019s memory segment begins. \uf0a7 The limit register defines the maximum valid offset for logical addresses that the process can use. \uf0a7 The protection check happens before the address translation (logical to physical). \uf0a7 This hardware-based mechanism keeps processes from accessing memory outside their assigned range, ensuring memory protection and isolation. 23/09/202 10 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Address Binding in an operating system is the process of mapping the addresses used in a program (logical or symbolic addresses) to actual physical memory addresses where the program's instructions and data reside. \uf0a7 This mapping can happen at different stages depending on when the final physical location is decided. \uf0a7 Compile-Time Address Binding \uf0a7 Load-Time Address Binding \uf0a7 Execution-Time"
  },
  {
    "id": 86,
    "source": "M5.txt",
    "text": "happens before the address translation (logical to physical). \uf0a7 This hardware-based mechanism keeps processes from accessing memory outside their assigned range, ensuring memory protection and isolation. 23/09/202 10 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Address Binding in an operating system is the process of mapping the addresses used in a program (logical or symbolic addresses) to actual physical memory addresses where the program's instructions and data reside. \uf0a7 This mapping can happen at different stages depending on when the final physical location is decided. \uf0a7 Compile-Time Address Binding \uf0a7 Load-Time Address Binding \uf0a7 Execution-Time (Run-Time) Address Binding 23/09/202 11 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Compile-Time Address Binding \uf0a7 The binding of addresses is done when the program is compiled. \uf0a7 The compiler translates symbolic addresses into absolute physical addresses. \uf0a7 This means the program can only run at a fixed memory location. \uf0a7 Example: If the compiler decides the program starts at memory location 1000, all addresses are fixed relative to that location. If the program needs to be moved, it must be recompiled. \uf0a7 Good for systems where memory layout is fixed and known, but inflexible. 23/09/202 12 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Load-Time Address Binding \uf0a7 Address binding happens when the program is loaded into memory before execution starts. \uf0a7 The compiler generates relocatable addresses (relative addresses), not absolute. \uf0a7 The loader (part of OS) assigns the starting physical address and updates addresses accordingly. \uf0a7 Example: If the program is loaded starting at physical address 5000, all logical addresses are adjusted relative to 5000 at load time. The program can run anywhere in memory without recompilation. \uf0a7 Flexible and useful if the program's starting location in memory can vary but does not move after loading. 23/09/202 13 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Execution-Time (Run-Time) Address Binding \uf0a7 Address binding happens during program execution dynamically. \uf0a7 The program's addresses are logical until resolved by hardware (Memory Management Unit) during instruction execution. \uf0a7 This allows the process to move or have memory allocated dynamically, such as in virtual memory systems. \uf0a7 Example: The program uses logical addresses; the OS/hardware translates these into physical addresses on the fly. \uf0a7 The program memory can be relocated or paged without the program being aware. \uf0a7 Most modern OSes use this for flexibility and memory protection. 23/09/202 14 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding Binding Type When Address Address Type Flexibility Example Usage Binding Happens Embedded During No (fixed, must Compile-Time Absolute systems, simple compilation recompile to move) OS During loading Yes (program can Batch processing Load-Time Relocatable before execution load anywhere) systems Modern During program Logical (virtual) Yes (dynamic multitasking Execution-Time execution addresses relocation) OSes, virtual memory 23/09/202 15 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Suppose a program uses an array: int arr[3] = {5,"
  },
  {
    "id": 87,
    "source": "M5.txt",
    "text": "modern OSes use this for flexibility and memory protection. 23/09/202 14 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding Binding Type When Address Address Type Flexibility Example Usage Binding Happens Embedded During No (fixed, must Compile-Time Absolute systems, simple compilation recompile to move) OS During loading Yes (program can Batch processing Load-Time Relocatable before execution load anywhere) systems Modern During program Logical (virtual) Yes (dynamic multitasking Execution-Time execution addresses relocation) OSes, virtual memory 23/09/202 15 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding \uf0a7 Suppose a program uses an array: int arr[3] = {5, 10, 15}; Compile-Time Binding Load-Time Binding Execution-Time Binding \u2022 The compiler decides absolute \u2022 During compilation, the compiler \u2022 The program uses logical addresses for arr, arr, and generates relative addresses for addresses (offsets) as above. array elements like arr offset 0, arr arr. offset 4, arr offset 8. \u2022 The OS and hardware memory management unit translate logical For example: \u2022 When loaded by the OS, the addresses (e.g., offset 0, 4, 8) into \u2022 arr at physical address 1000, program\u2019s base physical address is physical memory dynamically \u2022 arr at 1004, assigned, e.g., 5000. during execution. \u2022 arr at 1008 (assuming 4 bytes \u2022 The loader adds this base address per int). Suppose physical frame 7000 is to each offset: assigned, then: \u2022 arr physical address = 5000 + \u2022 arr \u2192 physical address 7000, \u2022 These addresses are fixed in 0 = 5000, \u2022 arr = 5000 + 4 = 5004, \u2022 arr \u2192 7004, the generated code. \u2022 arr = 5000 + 8 = 5008. \u2022 arr \u2192 7008, \u2022 all resolved at runtime. \u2022 The program must load at \u2022 The program can now run anywhere address 1000 exactly, or it in memory as the loader adjusts \u2022 This allows the program to move won\u2019t run properly. addresses accordingly. or be swapped in memory without affecting execution. 23/09/202 16 5 Module 5 - Memory Management Memory (RAM) \u2013 Address Binding Done when \uf071 The compiler translates the source program into an object file by assigning symbolic or, in some cases, absolute addresses to variables and instructions. \uf071 Compile-time binding occurs here if the compiler sets all final memory addresses. The program must then always load at the same spot in memory. \uf071 The linker combines object files and libraries to create an executable file, but addresses inside the executable are typically relocatable. \uf071 Load-time binding happens when the \uf071 The loader places the program in loader assigns the executable a base memory along with any dynamically address in memory and adjusts all linked libraries. addresses based on this starting point. \uf071 The program can occupy different \uf071 Execution-time binding refers to memory locations at each run, as computing physical addresses at addresses are fixed only upon loading. runtime. \uf071 The program uses logical or virtual addresses, and these are mapped to physical memory as the instructions execute\u2014allowing advanced features like address space relocation and virtual"
  },
  {
    "id": 88,
    "source": "M5.txt",
    "text": "addresses inside the executable are typically relocatable. \uf071 Load-time binding happens when the \uf071 The loader places the program in loader assigns the executable a base memory along with any dynamically address in memory and adjusts all linked libraries. addresses based on this starting point. \uf071 The program can occupy different \uf071 Execution-time binding refers to memory locations at each run, as computing physical addresses at addresses are fixed only upon loading. runtime. \uf071 The program uses logical or virtual addresses, and these are mapped to physical memory as the instructions execute\u2014allowing advanced features like address space relocation and virtual memory. 23/09/202 17 5 Module 5 - Memory Management Memory (RAM) \u2013 Virtual/Logical Address Space \uf0a7 This is the set of all addresses that a process or program can use as references to memory locations. \uf0a7 These addresses are generated by the CPU during program execution but do not represent actual physical memory locations. \uf0a7 They create an abstraction, allowing each process to think it has its own contiguous memory, independent of other processes or the real layout of physical memory. 23/09/202 18 5 Module 5 - Memory Management Memory (RAM) \u2013 Virtual/Logical Address \uf0a7 An address generated by the CPU while a program is running. \uf0a7 It is an abstract address that points to a location in the virtual address space for that process. \uf0a7 Logical addresses are translated to physical addresses by the Memory Management Unit (MMU) before actual access to memory occurs. 23/09/202 19 5 Module 5 - Memory Management Memory (RAM) \u2013 Physical Address Space \uf0a7 This is the actual set of physical memory addresses in the RAM hardware. \uf0a7 It represents all real locations where data and instructions reside physically. 23/09/202 20 5 Module 5 - Memory Management Memory (RAM) \u2013 Physical Address \uf0a7 The real address seen by the memory hardware. \uf0a7 It's where data actually lives in RAM. \uf0a7 After translation from logical addresses, physical addresses are used by the memory controller to fetch or store data. 23/09/202 21 5 Module 5 - Memory Management Memory (RAM) \u2013 Physical Address Address generated by Provides abstraction Virtual (Logical) CPU, refers to location and isolation for Address in virtual memory processes Actual memory Real memory access Physical Address location in RAM and storage of data hardware Complete set of all Creates process- Virtual Address Space logical addresses a specific memory process can use environment Actual addressable Represents real Physical Address Space space in physical RAM physical memory 23/09/202 22 5 Module 5 - Memory Management Memory (RAM) \u2013 Memory Management Unit MMU is a critical hardware component in a computer system that acts as the bridge between the \uf0a7 CPU and the physical RAM. Its primary roles include: Translating Virtual (Logical) Addresses to Physical Addresses: The CPU generates virtual \uf0a7 addresses during program execution. The MMU converts these virtual addresses into actual physical addresses in RAM, enabling the CPU to correctly access data. Memory Protection: The MMU enforces access control, ensuring a process can only"
  },
  {
    "id": 89,
    "source": "M5.txt",
    "text": "addressable Represents real Physical Address Space space in physical RAM physical memory 23/09/202 22 5 Module 5 - Memory Management Memory (RAM) \u2013 Memory Management Unit MMU is a critical hardware component in a computer system that acts as the bridge between the \uf0a7 CPU and the physical RAM. Its primary roles include: Translating Virtual (Logical) Addresses to Physical Addresses: The CPU generates virtual \uf0a7 addresses during program execution. The MMU converts these virtual addresses into actual physical addresses in RAM, enabling the CPU to correctly access data. Memory Protection: The MMU enforces access control, ensuring a process can only access \uf0a7 memory locations it is allowed to, preventing accidental or malicious memory access violations. Support for Virtual Memory: By managing address translation and memory access \uf0a7 permissions, the MMU supports virtual memory systems that allow programs to use more memory than physically available by swapping pages between RAM and disk storage. Memory Segmentation and Paging: MMUs may implement segmentation and paging \uf0a7 techniques that divide memory into blocks (segments or pages) for fine-grained memory management and protection. 23/09/202 23 5 Module 5 - Memory Management Memory (RAM) \u2013 Memory Management Unit \uf0a7 The MMU is hardware within the CPU or as a separate chip that takes the address requested by a running program and figures out where that data or instruction actually resides in physical memory. It checks permissions and ensures the system\u2019s memory is used securely and efficiently. 23/09/202 24 5 Module 5 - Memory Management Memory (RAM) \u2013 Static Loading & Linking Static Loading: \uf0a7 Static loading is the process where the entire program, including all its instructions and library \uf0a7 routines, is loaded into the main memory before execution starts. The whole executable is loaded into memory at once. \uf0a7 Since everything is loaded beforehand, the program starts running immediately after loading. \uf0a7 This method simplifies execution but requires that all code must fit into memory at load time. \uf0a7 Static Linking: \uf0a7 Static linking happens at compile-time or before execution. \uf0a7 The linker combines all object modules and required library routines into a single executable \uf0a7 file. All addresses are resolved and fixed, so the program is self-contained and has no external \uf0a7 dependencies at runtime. Every statically linked program has its own complete copy of the library code it uses. \uf0a7 23/09/202 25 5 Module 5 - Memory Management Memory (RAM) \u2013 Dynamic Loading & Linking \uf0a7 Dynamic loading is a technique where a program loads parts of itself (like functions or libraries) into memory only when they are needed during execution, not before. \uf0a7 At runtime, when a program calls a function or module that is not yet loaded, the OS or program loads the required code from disk into RAM on demand. \uf0a7 Saves memory by only loading what is necessary. \uf0a7 Speeds up program startup as not everything is loaded initially. \uf0a7 Useful for large programs or modular systems (e.g., plugins). 23/09/202 26 5 Module 5 - Memory Management Memory (RAM)"
  },
  {
    "id": 90,
    "source": "M5.txt",
    "text": "& Linking \uf0a7 Dynamic loading is a technique where a program loads parts of itself (like functions or libraries) into memory only when they are needed during execution, not before. \uf0a7 At runtime, when a program calls a function or module that is not yet loaded, the OS or program loads the required code from disk into RAM on demand. \uf0a7 Saves memory by only loading what is necessary. \uf0a7 Speeds up program startup as not everything is loaded initially. \uf0a7 Useful for large programs or modular systems (e.g., plugins). 23/09/202 26 5 Module 5 - Memory Management Memory (RAM) \u2013 Dynamic Loading & Linking \uf0a7 Dynamic linking refers to the process of linking external libraries or modules to programs at runtime rather than during compile time. \uf0a7 Instead of copying library code into each program executable (like static linking), a dynamic linker loads shared libraries (e.g., DLLs or SO files) into memory, resolves references, and binds the libraries to the running program as needed. \uf0a7 Saves memory since multiple programs can share a single copy of a library in RAM. \uf0a7 Reduces executable size as libraries are not duplicated in each program. \uf0a7 Updates to shared libraries apply to all programs immediately without recompilation. 23/09/202 27 5 Module 5 - Memory Management Contiguous Memory \uf0a7 It refers to a block of memory addresses that are adjacent and sequential. \uf0a7 When a process or program is allocated memory in a contiguous manner, all the memory cells allocated to it form one continuous chunk without gaps. \uf0a7 Example: If a process is allocated memory addresses from 0x1000 to 0x1FFF, these addresses are consecutive and form a contiguous memory block. \uf0a7 In contiguous memory allocation, the whole program or data segment resides as one continuous block in RAM. 23/09/202 28 5 Module 5 - Memory Management Partitioning of Memory Or Memory Allocation \uf0a7 Partitioning is the technique of dividing the main memory into blocks or partitions to allocate to different processes. \uf0a7 Partitioning helps in organizing memory so that multiple processes can reside in memory simultaneously without overlapping. Fixed-size Variable-size partitioning partitioning The memory is Memory is divided divided into fixed-size dynamically based on chunks or partitions. the process size at These partitions are runtime. Partitions static in size and vary in size. predetermined. 23/09/202 29 5 Module 5 - Memory Management Contiguous Memory - Partitioning of Memory \uf0a7 When memory is partitioned, each process is given one contiguous block or partition of memory. \uf0a7 The OS manages these partitions, allocating contiguous memory blocks to processes they require. \uf0a7 Contiguous memory allocation simplifies memory access and address translation because the entire process occupies one continuous physical block. \uf0a7 Drawbacks include fragmentation (both internal and external) and lack of flexibility when processes grow beyond allocated partitions. 23/09/202 30 5 Module 5 - Memory Management Contiguous Memory \u2013 Memory Allocation Strategies Fixed Partitioning (Static Partitioning) \u2022 RAM is divided into several fixed-size partitions during system startup. \u2022 Each process gets an entire partition. If"
  },
  {
    "id": 91,
    "source": "M5.txt",
    "text": "partitioned, each process is given one contiguous block or partition of memory. \uf0a7 The OS manages these partitions, allocating contiguous memory blocks to processes they require. \uf0a7 Contiguous memory allocation simplifies memory access and address translation because the entire process occupies one continuous physical block. \uf0a7 Drawbacks include fragmentation (both internal and external) and lack of flexibility when processes grow beyond allocated partitions. 23/09/202 30 5 Module 5 - Memory Management Contiguous Memory \u2013 Memory Allocation Strategies Fixed Partitioning (Static Partitioning) \u2022 RAM is divided into several fixed-size partitions during system startup. \u2022 Each process gets an entire partition. If the process is smaller than the partition, the rest of the space is wasted (internal fragmentation). \u2022 The number and size of partitions are fixed until reboot. \u2022 Simple but can be inefficient if process size varies widely. Variable Partitioning (Dynamic Partitioning) \u2022 RAM is divided into partitions dynamically based on process requirements. \u2022 Each process gets exactly as much memory as it needs, allocated as a contiguous block. \u2022 As processes are loaded and terminated, \u201choles\u201d (gaps of free memory) form. \u2022 This can lead to external fragmentation\u2014the memory is free, but not in large enough contiguous blocks for bigger processes. 23/09/202 31 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning \uf0a7 Fixed partitioning divides RAM into a fixed number of partitions at system configuration time, after allocating space for the OS. \uf0a7 Fixed Number of Partitions: The count is predetermined and unchanging (e.g., 4 partitions). \uf0a7 Partition Sizes: Can be equal (e.g., all 8MB) or variable (e.g., 4MB, 8MB, 8MB, 16MB) to accommodate different process sizes. \uf0a7 Allocation Rules: Processes are loaded from secondary memory into RAM partitions. Each process must fit entirely into one contiguous partition (no spanning across partitions). Allocation uses strategies like first-fit, best fit, worst fit and Next Fit. 23/09/202 32 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Post Allocation \u2013 P1 P5 - 4MB 8 MB 6 MB 8 MB 8 MB 8 MB 10 MB 8 MB 12 MB 23/09/202 33 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) 8 MB 6 MB 8 MB 8 MB 8 MB 10 MB 8 MB 12 MB 23/09/202 34 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes"
  },
  {
    "id": 92,
    "source": "M5.txt",
    "text": "Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) 8 MB 6 MB 8 MB 8 MB 8 MB 10 MB 8 MB 12 MB 23/09/202 34 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Post Allocation \u2013 P2 8 MB 8 MB 8 MB 10 MB 8 MB 12 MB 23/09/202 35 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) 8 MB 8 MB 8 MB 10 MB 8 MB 12 MB 23/09/202 36 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB Post Allocation \u2013 P3 8 MB 10 MB 8 MB 12 MB 23/09/202 37 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation"
  },
  {
    "id": 93,
    "source": "M5.txt",
    "text": "Allocation \u2013 P3 8 MB 10 MB 8 MB 12 MB 23/09/202 37 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB 10 MB 8 MB 12 MB 23/09/202 38 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB P4 (9 MB) No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 8 MB 12 MB 23/09/202 39 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB P4 (9 MB) No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 Internal Fragmentation (1MB) 8 MB 12 MB 23/09/202 40 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB)"
  },
  {
    "id": 94,
    "source": "M5.txt",
    "text": "P4 (9 MB) No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 Internal Fragmentation (1MB) 8 MB 12 MB 23/09/202 40 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB P4 (9 MB) No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 Internal Fragmentation (1MB) P5 (4 MB) 8 MB Post Allocation \u2013 P5 12 MB P5 (4 MB) Post Allocation \u2013 P4 23/09/202 41 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Internal Fragmentation (3MB) Fragmentation (1MB) P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB P4 (9 MB) No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 Internal Fragmentation (1MB) P5 (4 MB) 8 MB Post Allocation \u2013 P5 12 MB P5 (4 MB) Post Allocation \u2013 P4 Internal Fragmentation (1MB) Internal Fragmentation (8MB) 23/09/202 42 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 If a Process P6 and P7 Internal Fragmentation (3MB) arrives of size each 10 Fragmentation (1MB) MB and 12 MB P3 (7 MB) respectively still the - 8 MB P3 (7 MB) total free space across Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 partitions is sufficient, Internal Fragmentation (1MB) a new process can't be Fragmentation (1MB) allocated if no single contiguous partition 8 MB P4 (9 MB)"
  },
  {
    "id": 95,
    "source": "M5.txt",
    "text": "\u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 If a Process P6 and P7 Internal Fragmentation (3MB) arrives of size each 10 Fragmentation (1MB) MB and 12 MB P3 (7 MB) respectively still the - 8 MB P3 (7 MB) total free space across Post Allocation \u2013 P3 8 MB Internal Post Allocation \u2013 P3 partitions is sufficient, Internal Fragmentation (1MB) a new process can't be Fragmentation (1MB) allocated if no single contiguous partition 8 MB P4 (9 MB) fits it. No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 Internal Fragmentation (1MB) P5 (4 MB) 8 MB Post Allocation \u2013 P5 12 MB P5 (4 MB) Post Allocation \u2013 P4 Internal Fragmentation (1MB) Internal Fragmentation (8MB) 23/09/202 43 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) P2 (5 MB) 8 MB P2 (5 MB) Post Allocation \u2013 P2 6 MB Internal Post Allocation \u2013 P2 Assume P2 and Internal Fragmentation (3MB) P4 Terminates Fragmentation (1MB) then Holes are P3 (7 MB) 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB created Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB P4 (9 MB) No Allocation \u2013 P4 10 MB Post Allocation \u2013 P4 Internal Fragmentation (1MB) P5 (4 MB) 8 MB Post Allocation \u2013 P5 12 MB P5 (4 MB) Post Allocation \u2013 P4 Internal Fragmentation (1MB) Internal Fragmentation (8MB) 23/09/202 44 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) 8 MB Post Allocation \u2013 P2 Hole 6 MB Post Allocation \u2013 P2 Hole Assume P2 and P4 Terminates P3 (7 MB) then Holes are 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB created Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB No Allocation \u2013 P4 Hole 10 MB Post Allocation \u2013 P4 Hole P5 (4 MB) 8 MB P5 (4 MB) Post Allocation \u2013 P5 12 MB Internal Post Allocation \u2013 P4 Internal Fragmentation (1MB) Fragmentation (8MB) 23/09/202 45 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2"
  },
  {
    "id": 96,
    "source": "M5.txt",
    "text": "Terminates P3 (7 MB) then Holes are 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB created Internal Post Allocation \u2013 P3 Internal Fragmentation (1MB) Fragmentation (1MB) 8 MB No Allocation \u2013 P4 Hole 10 MB Post Allocation \u2013 P4 Hole P5 (4 MB) 8 MB P5 (4 MB) Post Allocation \u2013 P5 12 MB Internal Post Allocation \u2013 P4 Internal Fragmentation (1MB) Fragmentation (8MB) 23/09/202 45 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Static Partitioning Processes Fixed Partitioning with Equal Partition Sizes Fixed Partitioning with Variable Partition Sizes P1 - 3MB Memory Memory P2 - 5MB Blocks Blocks (Predefined) (Predefined) P3 - 7MB P1 (3 MB) 8 MB P1 (3 MB) P4 - 9MB Post Allocation \u2013 P1 4 MB Internal Post Allocation \u2013 P1 Internal Fragmentation (5MB) P5 - 4MB Fragmentation (1MB) 8 MB Post Allocation \u2013 P2 Hole 6 MB Post Allocation \u2013 P2 Hole Assume P9 and P10 arrives with P3 (7 MB) each 16 MB 8 MB P3 (7 MB) Post Allocation \u2013 P3 8 MB size, still they Internal Post Allocation \u2013 P3 Internal cannot be Fragmentation (1MB) Fragmentation (1MB) allocated to 8 MB memory which No Allocation \u2013 P4 Hole 10 MB Post Allocation \u2013 P4 Hole is External Fragmentation. P5 (4 MB) 8 MB P5 (4 MB) Post Allocation \u2013 P5 12 MB Internal Post Allocation \u2013 P4 Internal Fragmentation (1MB) Fragmentation (8MB) 23/09/202 46 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB 39 23/09/202 47 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB P2 5MB Post Allocation \u2013 P2 39 23/09/202 48 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB P2 5MB Post Allocation \u2013 P2 P3 7MB Post Allocation \u2013 P3 39 23/09/202 49 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB P2 5MB Post Allocation \u2013 P2 P3 7MB Post Allocation \u2013 P3 P4 9MB Post Allocation \u2013 P4 39 23/09/202 50 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB P2 5MB Post Allocation \u2013 P2 P3 7MB Post"
  },
  {
    "id": 97,
    "source": "M5.txt",
    "text": "Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB P2 5MB Post Allocation \u2013 P2 P3 7MB Post Allocation \u2013 P3 P4 9MB Post Allocation \u2013 P4 39 23/09/202 50 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB P2 5MB Post Allocation \u2013 P2 P3 7MB Post Allocation \u2013 P3 P4 9MB No Allocation \u2013 P4 P5 4MB Post Allocation \u2013 P5 Hole 12MB 39 23/09/202 51 5 Module 5 - Memory Management Contiguous Memory Allocation \u2013 Variable/Dynamic Partitioning Processes Variable/Dynamic Partitioning P1 - 3MB P2 - 5MB Memory Memory Blocks Blocks P3 - 7MB 0 Post Allocation \u2013 P1 P1 3MB P4 - 9MB P5 - 4MB Hole 5M Post Allocation \u2013 P2 P3 7MB Assume P2 and If P6 with size 14M Post Allocation \u2013 P3 P4 Terminates arrives it cannot be then Holes are allocated hence still Hole 9M No Allocation \u2013 P4 created External Fragmentation Exists. But no Internal Fragmentation. P5 4MB Post Allocation \u2013 P5 Hole 12MB 39 23/09/202 52 5 Module 5 - Memory Management Contiguous Memory \u2013 Memory Fragmentation \uf0a7 Memory fragmentation is a problem that occurs when free memory in the system is broken into small, scattered pieces (fragments), making it difficult or impossible to allocate large contiguous blocks even though there may be enough total free memory. \uf0a7 When processes are loaded and unloaded dynamically, memory space gets divided into allocated blocks (partitions) and holes (free spaces) scattered throughout RAM. 23/09/202 53 5 Module 5 - Memory Management Contiguous Memory \u2013 Memory Fragmentation Internal Fragmentation External Fragmentation Occurs when allocated memory blocks are Occurs when free memory is scattered into larger than needed by processes. small non-contiguous chunks (holes). The unused memory inside these allocated Even if the total free memory is enough for a blocks results in wasted space. process, it may not be allocated because no single free block is large enough. Example: If a 50 KB fixed partition is Example: 100 MB of free memory is available allocated but a process needs only 40 KB, 10 but divided into small pieces of 10 MB, 20 KB is wasted inside that partition. MB, 30 MB \u2013 none large enough for a 40 MB allocation request. Happens in fixed partitioning or paging Happens in dynamic partitioning and systems. contiguous allocation systems. 23/09/202 54 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning \u2022 Scan memory from the beginning and allocate the first available First Fit: hole that is big enough for the process. \u2022 Fast and simple but can lead to more fragmentation near the beginning of memory. \u2022 Search for the smallest available hole that will fit the process. Best Fit: \u2022 Tries"
  },
  {
    "id": 98,
    "source": "M5.txt",
    "text": "KB is wasted inside that partition. MB, 30 MB \u2013 none large enough for a 40 MB allocation request. Happens in fixed partitioning or paging Happens in dynamic partitioning and systems. contiguous allocation systems. 23/09/202 54 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning \u2022 Scan memory from the beginning and allocate the first available First Fit: hole that is big enough for the process. \u2022 Fast and simple but can lead to more fragmentation near the beginning of memory. \u2022 Search for the smallest available hole that will fit the process. Best Fit: \u2022 Tries to minimize leftover space but can create many small unusable holes (more external fragmentation). \u2022 Search for the largest available hole and allocate from it. Worst Fit: \u2022 The idea is to leave the largest leftover holes, hoping they\u2019ll be usable for future requests; however, it can waste large spaces inefficiently. \u2022 Similar to First Fit, but instead of starting from the beginning every time, it resumes the search from the location where it left off last Next Fit time. \u2022 This reduces the search overhead for large memory but still allocates contiguous blocks. 23/09/202 55 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Algorithm How it allocates memory Advantage Disadvantage Allocates first large enough Can lead to fragmentation First Fit Fast, simple block found at start Allocates smallest suitable Minimizes leftover Slow, may create many Best Fit block space small holes Allocates largest block Avoids tiny leftover Worst Fit Can waste large blocks available holes Like First Fit, but search Faster than first fit Next Fit starts from last allocation May still fragment memory theoretically point 23/09/202 56 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning \uf0a7 Exercise 1 - Consider six memory partitions of size 200 KB, 400 KB, 600 KB, 500 KB, 300 KB and 250 KB. These partitions need to be allocated to four processes of sizes 357 KB, 210 KB, 468 KB and 491 KB in that order. Perform the allocation of processes using- First, Best and Worst Fit 23/09/202 57 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE P3 \u2013 468KB 200KB P4 \u2013 491KB P1 400KB 600KB 500KB 300KB 250KB 23/09/202 58 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE P3 \u2013 468KB 200KB P4 \u2013 491KB P1 400KB P2 600KB 500KB 300KB 250KB 23/09/202 59 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE P3 \u2013 468KB 200KB P4 \u2013 491KB P1 400KB P2 600KB P3 500KB 300KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 60 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable"
  },
  {
    "id": 99,
    "source": "M5.txt",
    "text": "Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE P3 \u2013 468KB 200KB P4 \u2013 491KB P1 400KB P2 600KB 500KB 300KB 250KB 23/09/202 59 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE P3 \u2013 468KB 200KB P4 \u2013 491KB P1 400KB P2 600KB P3 500KB 300KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 60 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB P2 600KB 600KB P3 500KB 500KB 300KB 300KB 250KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 61 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB P2 600KB 600KB P3 500KB 500KB 300KB 300KB 250KB P2 210 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 62 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB P2 600KB 600KB P3 500KB P3 468 500KB 300KB 300KB 250KB P2 210 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 63 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB P2 600KB P4 491 600KB P3 500KB P3 468 500KB 300KB 300KB 250KB P2 210 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 64 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB 400KB P2 600KB P4 491 600KB P1 357 600KB P3 500KB P3 468 500KB 500KB 300KB 300KB 300KB 250KB P2 210 250KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 65 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB 400KB P2 600KB P4 491 600KB P1 357 600KB P3 500KB P3 468 500KB P2 - 210 500KB 300KB 300KB 300KB 250KB P2 210 250KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 66 5 Module 5 -"
  },
  {
    "id": 100,
    "source": "M5.txt",
    "text": "500KB 300KB 300KB 300KB 250KB P2 210 250KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 65 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB 400KB P2 600KB P4 491 600KB P1 357 600KB P3 500KB P3 468 500KB P2 - 210 500KB 300KB 300KB 300KB 250KB P2 210 250KB 250KB P4 \u2013 491KB \u2013 Not Allocated 23/09/202 66 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 357KB P2 \u2013 210KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 468KB 200KB 200KB 200KB P4 \u2013 491KB P1 400KB P1 357 400KB 400KB P2 600KB P4 491 600KB P1 357 600KB P3 500KB P3 468 500KB P2 - 210 500KB 300KB 300KB 300KB 250KB P2 210 250KB 250KB P4 \u2013 491KB \u2013 Not Allocated P3 \u2013 468 KB & P4 \u2013 491KB \u2013 Not Allocated 23/09/202 67 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning \uf0a7 Example 2 \u2013 There are 4 processes requesting for Memory of following size - 300, 25, 125, 50. The current memory snapshot is given to you. It follows Variable Partitioning Partitions SIZE Occupied 50KB \uf0a7 Use First, Best and Worst Fit allocation 150KB Occupied 300KB Schemes for the given Processes. 350KB Occupied 600KB 23/09/202 68 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE P3 \u2013 125KB Occupied 50KB P4 \u2013 50KB 150KB Occupied 300KB P1 - 300 300KB Hole 50KB Occupied 600KB 23/09/202 69 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE P3 \u2013 125KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB Hole 125KB Occupied 300KB P1 - 300 300KB Hole 50KB Occupied 600KB 23/09/202 70 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE P3 \u2013 125KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P3 - 125 125KB Occupied 300KB P1 - 300 300KB Hole 50KB Occupied 600KB 23/09/202 71 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE P3 \u2013 125KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P3 - 125 125KB Occupied 300KB P1 - 300 300KB P4 - 50 50KB Occupied 600KB 23/09/202 72 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied"
  },
  {
    "id": 101,
    "source": "M5.txt",
    "text": "P1 - 300 300KB Hole 50KB Occupied 600KB 23/09/202 71 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE P3 \u2013 125KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P3 - 125 125KB Occupied 300KB P1 - 300 300KB P4 - 50 50KB Occupied 600KB 23/09/202 72 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB 150KB P3 - 125 125KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB Hole 50KB P4 - 50 50KB Occupied 600KB Occupied 600KB 23/09/202 73 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB 150KB P3 - 125 125KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P2 - 25 25KB P4 - 50 50KB Hole 25KB Occupied 600KB Occupied 600KB 23/09/202 74 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB P3 - 125 125KB Hole 25KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P4 - 50 50KB P2 - 25 25KB Occupied 600KB Hole 25KB Occupied 600KB 23/09/202 75 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB P3 - 125 125KB Hole 25KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P4 - 50 50KB P2 - 25 25KB Occupied 600KB Hole 25KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 76 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB 150KB P3 - 125 125KB Hole 25KB Occupied 300KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P1 - 300 300KB Hole 50KB P4 - 50 50KB P2 - 25 25KB Occupied 600KB Occupied 600KB Hole 25KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 77 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT"
  },
  {
    "id": 102,
    "source": "M5.txt",
    "text": "WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB 150KB P3 - 125 125KB Hole 25KB Occupied 300KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P1 - 300 300KB Hole 50KB P4 - 50 50KB P2 - 25 25KB Occupied 600KB Occupied 600KB Hole 25KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 77 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB P2 - 25 25KB P3 - 125 125KB Hole 25KB Hole 125KB Occupied 300KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P1 - 300 300KB P4 - 50 50KB P2 - 25 25KB Hole 50KB Occupied 600KB Hole 25KB Occupied 600KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 78 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB P2 - 25 25KB P3 - 125 125KB Hole 25KB P3 - 125 125KB Occupied 300KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P1 - 300 300KB P4 - 50 50KB P2 - 25 25KB Hole 50KB Occupied 600KB Hole 25KB Occupied 600KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 79 5 Module 5 - Memory Management Memory Allocation Algorithms For Variable Partitioning Processes FIRST FIT BEST FIT WORST FIT P1 \u2013 300KB P2 \u2013 25KB Partitions SIZE Partitions SIZE Partitions SIZE P3 \u2013 125KB Occupied 50KB Occupied 50KB Occupied 50KB P4 \u2013 50KB P2 - 25 25KB P2 - 125 125KB P2 - 25 25KB P3 - 125 125KB Hole 25KB P3 - 125 125KB Occupied 300KB Occupied 300KB Occupied 300KB P1 - 300 300KB P1 - 300 300KB P1 - 300 300KB P4 - 50 50KB P2 - 25 25KB P4 - 50 50KB Occupied 600KB Hole 25KB Occupied 600KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 80 5 Module 5 - Memory Management Non-Contiguous Memory \uf0a7 Non-contiguous memory is a memory management technique where a single process is allocated physical memory in separate, non-adjacent blocks. \uf0a7 Instead of requiring one large, continuous chunk of memory, the operating system can scatter the parts of a process throughout available free spaces in RAM. \uf0a7 This approach addresses the major limitations of contiguous memory allocation, which often leads to significant wasted space due to external fragmentation. External fragmentation occurs when free memory is divided into small, non-contiguous blocks,"
  },
  {
    "id": 103,
    "source": "M5.txt",
    "text": "Occupied 600KB Occupied 600KB P4 \u2013 50KB \u2013 Not Allocated 23/09/202 80 5 Module 5 - Memory Management Non-Contiguous Memory \uf0a7 Non-contiguous memory is a memory management technique where a single process is allocated physical memory in separate, non-adjacent blocks. \uf0a7 Instead of requiring one large, continuous chunk of memory, the operating system can scatter the parts of a process throughout available free spaces in RAM. \uf0a7 This approach addresses the major limitations of contiguous memory allocation, which often leads to significant wasted space due to external fragmentation. External fragmentation occurs when free memory is divided into small, non-contiguous blocks, making it impossible to allocate a large process even if the total free memory is sufficient. 23/09/202 81 5 Module 5 - Memory Management Virtual Memory \u2013 A memory management technique \uf0a7 Virtual memory is a memory management technique that provides an illusion of a very large main memory to programs, even if the actual physical memory (RAM) is much smaller. \uf0a7 It allows processes to use more memory than physically available by temporarily transferring data between RAM and disk storage, enabling efficient multitasking and memory utilization. 23/09/202 82 5 Module 5 - Memory Management Virtual Memory \uf0a7 Programs use virtual addresses without worrying about physical memory limits. \uf0a7 The OS and hardware work together to map virtual addresses to physical addresses transparently. \uf0a7 When a required data page is not in physical memory, it is paged in from disk (swap space), and less-used pages are paged out, making room for active data. 23/09/202 83 5 Module 5 - Memory Management Virtual Memory \uf0a7 When physical memory runs out, less-used parts of programs (pages) are temporarily moved to disk storage, allowing systems to run larger programs or multiple programs simultaneously without requiring all of them to be fully loaded in RAM at once. \uf0a7 The Translation Lookaside Buffer (TLB) is a small, fast cache inside the Memory Management Unit (MMU) that stores recent virtual-to-physical address translations to speed up virtual memory access. 23/09/202 84 5 Module 5 - Memory Management Virtual Memory \u2013 Why required? \uf0a7 Allows execution of programs larger than physical memory. \uf0a7 Increases the degree of multiprogramming by running multiple processes concurrently. \uf0a7 Optimizes memory usage by loading only needed program parts into RAM. \uf0a7 Provides memory protection and isolation between processes. \uf0a7 Reduces I/O operations since programs do not need full memory allocation at once. 23/09/202 85 5 Module 5 - Memory Management Paging \uf0a7 Paging is the key technique used to implement virtual memory. \uf0a7 It divides the process's address space and the physical memory into fixed-size blocks: \uf0a7 Pages: Fixed-size blocks of the process's virtual address space. \uf0a7 Frames: Corresponding fixed-size blocks in physical memory, same size as pages. 23/09/202 86 5 Module 5 - Memory Management Paging \uf0a7 The operating system maintains a page table for each process, which maps the virtual page numbers to physical frame numbers. \uf0a7 This table is used by the Memory Management Unit (MMU) to translate virtual addresses"
  },
  {
    "id": 104,
    "source": "M5.txt",
    "text": "85 5 Module 5 - Memory Management Paging \uf0a7 Paging is the key technique used to implement virtual memory. \uf0a7 It divides the process's address space and the physical memory into fixed-size blocks: \uf0a7 Pages: Fixed-size blocks of the process's virtual address space. \uf0a7 Frames: Corresponding fixed-size blocks in physical memory, same size as pages. 23/09/202 86 5 Module 5 - Memory Management Paging \uf0a7 The operating system maintains a page table for each process, which maps the virtual page numbers to physical frame numbers. \uf0a7 This table is used by the Memory Management Unit (MMU) to translate virtual addresses into physical addresses at runtime. 23/09/202 87 5 Module 5 - Memory Management Virtual Memory \u2013 Hardware Support 23/09/202 88 5 Module 5 - Memory Management Virtual Memory \u2013 Hardware Support \uf0a7 The CPU generates a virtual address consisting of a page number (P) and an offset (D). \uf0a7 The TLB is checked first to see if it contains the page number. \uf0a7 If TLB hit (page number found): The corresponding frame number (F) is retrieved quickly. \uf0a7 Combine frame number (F) with the offset (D) to get the physical address. \uf0a7 Access memory at that physical address. \uf0a7 If TLB miss (page number not found): \uf0a7 Page table is accessed in main memory to find the frame number. \uf0a7 The TLB is updated with this translation for future quick access. \uf0a7 The physical address is formed, and memory is accessed. 23/09/202 89 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging \uf0a7 Paging is a memory management technique used by operating systems to efficiently manage the physical memory available to programs. \uf0a7 It allows programs to use more memory than physically available and ensures efficient use of memory without requiring contiguous allocation. 23/09/202 90 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Working \uf0a7 Division into Pages and Frames: \uf0a7 The logical address space (program memory) is divided into fixed-size blocks called pages (e.g., 4KB each). \uf0a7 The physical memory (RAM) is divided into blocks of the same size as pages called frames. \uf0a7 Page Mapping: \uf0a7 Each page can be loaded into any frame in physical memory. \uf0a7 This allows non-contiguous allocation of memory, avoiding the problem of external fragmentation. 23/09/202 91 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Working \uf0a7 Page Table: \uf0a7 The OS keeps a page table which maps each virtual page to a physical frame. \uf0a7 When the CPU accesses memory using a virtual (logical) address, the page number is looked up in the page table to find the frame number. \uf0a7 The physical address is computed by combining the frame number with the offset within the page. 23/09/202 92 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Working \uf0a7 Paging and Virtual Memory \uf0a7 If a required page is not in physical memory (a page fault), it is loaded from"
  },
  {
    "id": 105,
    "source": "M5.txt",
    "text": "Page Table: \uf0a7 The OS keeps a page table which maps each virtual page to a physical frame. \uf0a7 When the CPU accesses memory using a virtual (logical) address, the page number is looked up in the page table to find the frame number. \uf0a7 The physical address is computed by combining the frame number with the offset within the page. 23/09/202 92 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Working \uf0a7 Paging and Virtual Memory \uf0a7 If a required page is not in physical memory (a page fault), it is loaded from secondary storage (disk). \uf0a7 This allows processes to run as if there is more RAM than physically present, seamlessly using disk as extended memory. 23/09/202 93 5 Module 5 - Memory Management Paging - Working \uf0a7 CPU is executing the Process P1 which of size \uf0e0 4Bytes. \uf0a7 The Page Size \uf0e0 2Bytes \uf0a7 The main memory or RAM total size is \uf0e0 16Bytes \uf0a7 Rules: \uf0a7 Page Size = Frame Size; hence Frame Size \uf0e0 2Bytes \uf0a7 The logical address space(LAS) is designed to accommodate exactly the process size; hence LAS \uf0e0 4Bytes 23/09/202 94 5 Module 5 - Memory Management Paging \u2013 Working - \u2013 Logical Address \uf0a7 When CPU executes a Process it generates Logical Address for it \uf0a7 The Logical Address contains \uf0e0 Page No. and Page offset \uf0a7 Now find the No. of Pages for this P1 and represent it in Bits. \uf0a7 Representing the Pages in bits \uf0a7 Representing the offset in bits \uf0a7 Logical address generated by CPU for this process will have \uf0e0 2bits, The MSB (most significant bit) represents the page number, while the LSB (least significant bit) represents the offset within the page. 23/09/202 95 5 Module 5 - Memory Management Paging \u2013 Working \u2013 Logical Address \uf0a7 So P1 will be divided into two pages shown below. Page No. (1 Bit) Bytes in Page Offset at 0 bit Offset at 1 bit Page 0 Byte 0 | Byte 1 Offset at 0 bit Offset at 1 bit Page 1 Byte 2 | Byte 3 \uf0a7 The logical address generated by CPU for P1 will be like: Page No. (1 Bit) Page Offset (1 Bit) Since 1 bit either Page 0 0 or 1 Since 1 bit either Page 1 0 or 1 23/09/202 96 5 Module 5 - Memory Management Paging \u2013 Working \u2013 Physical Memory & Physical Address \uf0a7 Now the RAM is of total \uf0e0 16Bytes and Frame Size \uf0e0 2Bytes \uf0a7 Finding the No. of Frames in RAM \uf0a7 The Physical Address contains \uf0e0 Frame No. and Page offset \uf0a7 Representing the Frames in bi ts \uf0a7 Representing the offset in bits \uf0a7 Physical address will have \uf0e0 3bits MSB (most significant bit) representing the frame number, while the 1bit LSB (least significant bit) representing the offset. 23/09/202 97 5 Module 5 - Memory Management Paging - Working \u2013 Physical Memory & Physical Address \uf0a7 The"
  },
  {
    "id": 106,
    "source": "M5.txt",
    "text": "Management Paging \u2013 Working \u2013 Physical Memory & Physical Address \uf0a7 Now the RAM is of total \uf0e0 16Bytes and Frame Size \uf0e0 2Bytes \uf0a7 Finding the No. of Frames in RAM \uf0a7 The Physical Address contains \uf0e0 Frame No. and Page offset \uf0a7 Representing the Frames in bi ts \uf0a7 Representing the offset in bits \uf0a7 Physical address will have \uf0e0 3bits MSB (most significant bit) representing the frame number, while the 1bit LSB (least significant bit) representing the offset. 23/09/202 97 5 Module 5 - Memory Management Paging - Working \u2013 Physical Memory & Physical Address \uf0a7 The Frame Structure of RAM will be like: Frame No Byte Addresses (3 Bits ) Byte 0 | Byte 1 0 0 0 Offset at 0 bit Offset at 1 bit Byte 2 | Byte 3 0 0 1 Offset at 0 bit Offset at 1 bit Byte 4 | Byte 5 0 1 0 Offset at 0 bit Offset at 1 bit Byte 6 | Byte 7 0 1 1 Offset at 0 bit Offset at 1 bit Byte 8 | Byte 9 1 0 0 Offset at 0 bit Offset at 1 bit Byte 10 | Byte 11 1 0 1 Offset at 0 bit Offset at 1 bit Byte 12 | Byte 13 1 1 0 Offset at 0 bit Offset at 1 bit Byte 14 | Byte 15 1 1 1 Offset at 0 bit Offset at 1 bit 23/09/202 98 5 Module 5 - Memory Management Paging - Working \u2013 Page Table \uf0a7 The page table maps logical pages to physical frames. \uf0a7 For every Process one page table is created. Page Table Structure \uf0a7 For this process P1 Page table will have the Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) following: Page 0 0 0 0 \uf0a7 Number of entries \uf0e0 2 (As per Logical Page 1 0 0 1 Page) Page 2 0 1 0 \uf0a7 Frames represented as 3 bits Page 3 0 1 1 Page 4 1 0 0 \uf0a7 So, Total page table size: Page 5 1 0 1 No. of Entries x Entry Size \uf0e0 2 \u00d7 3 = 6 bits Page 6 1 1 0 Page 7 1 1 1 23/09/202 99 5 Module 5 - Memory Management Paging - Working \u2013 Page Table \uf0a7 If the Operating System allocates: Page 0 \u2192 Frame 3, Page 1 \u2192 Frame Page Table Structure 6. The page table would look like Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) \uf0a7 Use this equation to find the physical 0 0 0 0 0 1 Address 0 1 0 Page 0 0 1 1 1 R/W = 1 0 Physical Address = (Frame 1 0 0 1 0 1 Number \u00d7 Page Size) + Offset Page 1 1 1 0 1 R/W = 1 0 1 1 1 23/09/202 100 5 Module 5 - Memory Management"
  },
  {
    "id": 107,
    "source": "M5.txt",
    "text": "Page 0 \u2192 Frame 3, Page 1 \u2192 Frame Page Table Structure 6. The page table would look like Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) \uf0a7 Use this equation to find the physical 0 0 0 0 0 1 Address 0 1 0 Page 0 0 1 1 1 R/W = 1 0 Physical Address = (Frame 1 0 0 1 0 1 Number \u00d7 Page Size) + Offset Page 1 1 1 0 1 R/W = 1 0 1 1 1 23/09/202 100 5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping \uf0a7 CPU runs P1 and request for 1st byte data of it. \uf0a7 logical Address generated by CPU \uf0e0 1 0 Page Table Structure \uf0a7 Page no.\uf0e0 1 & Page Offset \uf0e0 0 Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) P1 0 0 0 Logical address for 1st Byte 0 0 1 Page Offset 0 1 0 No. No. CPU Page 0 0 1 1 1 R/W = 1 0 1 0 1 0 0 1 0 1 Page 1 1 1 0 1 R/W = 1 0 1 1 1 23/09/202 101 5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping \uf0a7 Perform Lookup in Page Table structure using Page No. as index Page Table Structure Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) P1 0 0 0 Logical address for 1st Byte 0 0 1 0 1 0 Page Offset No. No. CPU Page 0 0 1 1 1 R/W = 1 0 1 0 1 0 0 1 0 1 Page 1 1 1 0 1 R/W = 1 0 1 1 1 23/09/202 102 5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping \uf0a7 Extract the Frame No. from the lookup process in page table. Page Table Structure Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) P1 0 0 0 Logical address for 1st Byte 0 0 1 0 1 0 Page Offset No. No. CPU Page 0 0 1 1 1 R/W = 1 0 1 0 1 0 0 1 0 1 Page 1 1 1 0 1 R/W = 1 0 1 1 1 23/09/202 103 5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping \uf0a7 Extract the Frame No. from the lookup process in page table. Page Table Structure Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) P1 0 0 0 Logical address for 1st Byte 0 0 1 0 1 0 Page Offset No. No. CPU Page 0 0 1 1 1 R/W = 1 0 1 0 1 0 0 1 0 1 Frame Offset No. (3 No. (1 Page 1 1 1 0 1 R/W = 1 0 bits) bit) 1"
  },
  {
    "id": 108,
    "source": "M5.txt",
    "text": "5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping \uf0a7 Extract the Frame No. from the lookup process in page table. Page Table Structure Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) P1 0 0 0 Logical address for 1st Byte 0 0 1 0 1 0 Page Offset No. No. CPU Page 0 0 1 1 1 R/W = 1 0 1 0 1 0 0 1 0 1 Frame Offset No. (3 No. (1 Page 1 1 1 0 1 R/W = 1 0 bits) bit) 1 1 1 1 1 0 23/09/202 104 5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping \uf0a7 Combine the offset with the extracted Frame no. Page Table Structure Present Protection Dirty Frame No Bit Bits Bits (3 Bits ) (1 bit) R/W/E (1 Bit) P1 0 0 0 Logical address for 1st Byte 0 0 1 0 1 0 Page Offset No. No. CPU Page 0 0 1 1 1 R/W = 1 0 1 0 1 0 0 1 0 1 Frame Offset No. (3 No. (1 Page 1 1 1 0 1 R/W = 1 0 bits) bit) 1 1 1 1 1 0 0 23/09/202 105 5 Module 5 - Memory Management Paging - Working \u2013 Address Translation/Mapping Combine the offset with the extracted Frame no. and look in the RAM for the memory Location in terms of Byte which is Byte 12. \uf0a7 Frame No Byte Addresses (3 Bits ) Byte 0 | Byte 1 Page Table Structure 0 0 0 Offset at 0 bit Offset at 1 bit Byte 2 | Byte 3 0 0 1 Offset at 0 bit Offset at 1 bit Present Protection Dirty Frame No Bit Bits Bits 0 1 0 Byte 4 | Byte 5 (3 Bits ) (1 bit) R/W/E (1 Bit) Offset at 0 bit Offset at 1 bit P1 0 1 1 Byte 6 | Byte 7 0 0 0 Offset at 0 bit Offset at 1 bit Logical address for 1st Byte 8 | Byte 9 1 0 0 Byte 0 0 1 Offset at 0 bit Offset at 1 bit Byte 10 | Byte 11 1 0 1 0 1 0 Page Offset Offset at 0 bit Offset at 1 bit No. No. Byte 12 | Byte 13 CPU 1 1 0 Page 0 0 1 1 1 R/W = 1 0 Offset at 0 bit Offset at 1 bit 1 0 Byte 14 | Byte 15 1 1 1 1 0 0 Offset at 0 bit Offset at 1 bit 1 0 1 Frame Offset No. (3 No. (1 Page 1 1 1 0 1 R/W = 1 0 bits) bit) P1 1 1 1 1 1 0 0 Physical address for 1st Byte 23/09/202 106 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Equations \uf0a7 For Main Memory \uf0a7 Physical Address Space = Size"
  },
  {
    "id": 109,
    "source": "M5.txt",
    "text": "Page 0 0 1 1 1 R/W = 1 0 Offset at 0 bit Offset at 1 bit 1 0 Byte 14 | Byte 15 1 1 1 1 0 0 Offset at 0 bit Offset at 1 bit 1 0 1 Frame Offset No. (3 No. (1 Page 1 1 1 0 1 R/W = 1 0 bits) bit) P1 1 1 1 1 1 0 0 Physical address for 1st Byte 23/09/202 106 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Equations \uf0a7 For Main Memory \uf0a7 Physical Address Space = Size of main memory \uf0a7 Size of main memory = Total number of frames x Page size \uf0a7 Frame size = Page size \uf0a7 If number of frames in main memory = 2X, then number of bits in frame number = X bits \uf0a7 If Page size = 2X Bytes, then number of bits in page offset = X bits \uf0a7 If size of main memory = 2X Bytes, then number of bits in physical address = X bits 23/09/202 117 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Equations \uf0a7 For Process- For Page Table- \uf0a7 Virtual Address Space = Size of \uf0a7 Size of page table = Number of process entries in page table x Page table \uf0a7 Number of pages the process is entry size divided = Process size / Page size \uf0a7 Number of entries in pages table = \uf0a7 If process size = 2X bytes, then Number of pages the process is number of bits in virtual address divided space = X bits \uf0a7 Page table entry size = Number of bits in frame number + Number of bits used for optional fields if any 23/09/202 118 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Equations In general, if the given address consists of \u2018n\u2019 bits, then using \u2018n\u2019 bits, 2n locations are possible. Then, size of memory = 2n x Size of one location. If the memory is byte-addressable, then size of one location = 1 byte. Thus, size of memory = 2n bytes. If the memory is word-addressable where 1 word = m bytes, then size of one location = m bytes. Thus, size of memory = 2n x m bytes. 23/09/202 119 5 Module 5 - Memory Management How to Implement Virtual Memory \u2013 Paging - Equations 21 2 220 1M 22 4 230 1G 23 8 240 1T 24 16 25 32 26 64 27 128 28 256 29 512 210 1K 23/09/202 120 5 Module 5 - Memory Management Paging - Problem Given Logical Address Space \u2013 4GB, Physical Address Space \u2013 64 MB, Page Size \u2013 4 KB. Find the \uf0a7 No. of Pages, \uf0a7 No. of Frames, \uf0a7 No. of Entries in Page Table, \uf0a7 Size of Page Table 23/09/202 121 5 Module 5 - Memory Management Paging - Problem \uf0a7 32 bits Page No. Page Offset"
  },
  {
    "id": 110,
    "source": "M5.txt",
    "text": "Virtual Memory \u2013 Paging - Equations 21 2 220 1M 22 4 230 1G 23 8 240 1T 24 16 25 32 26 64 27 128 28 256 29 512 210 1K 23/09/202 120 5 Module 5 - Memory Management Paging - Problem Given Logical Address Space \u2013 4GB, Physical Address Space \u2013 64 MB, Page Size \u2013 4 KB. Find the \uf0a7 No. of Pages, \uf0a7 No. of Frames, \uf0a7 No. of Entries in Page Table, \uf0a7 Size of Page Table 23/09/202 121 5 Module 5 - Memory Management Paging - Problem \uf0a7 32 bits Page No. Page Offset Size 23/09/202 122 5 Module 5 - Memory Management Paging - Problem \uf0a7 32 32 bits bits Page No. Page Offset Page No. Page Offset Size 20 bits Size 12 bits 23/09/202 123 5 Module 5 - Memory Management Paging - Problem \uf0a7 26 bits Frame No. Frame Offset Size 23/09/202 124 5 Module 5 - Memory Management Paging - Problem \uf0a7 26 bits Frame No. Frame Offset Size 23/09/202 125 5 Module 5 - Memory Management Paging - Problem \uf0a7 23/09/202 126 5 Module 5 - Memory Management Paging \u2013 With Hardware (TLB) Support \uf0a7 TLB is a high-speed cache that stores recent page table entries to speed up address translation \uf0a7 Without TLB: Every memory access requires 2 memory accesses \uf0a7 Access page table entry \uf0a7 Access actual data \uf0a7 With TLB: Most translations are cached \uf0a7 TLB Hit: Translation found in cache (fast) \uf0a7 TLB Miss: Must access page table (slower) 23/09/202 127 5 Module 5 - Memory Management Paging \u2013 With Hardware (TLB) Support \uf0a7 PTBR (Page Table Base Register) is a special hardware register that stores the base physical address of the current process's page table in main memory. \uf0a7 It acts as a pointer that tells the Memory Management Unit (MMU) where to find the page table for address translation. 23/09/202 128 5 Module 5 - Memory Management Paging \u2013 With Hardware (TLB) Support \uf0a7 Effective Access Time for TLB = Hit ratio of TLB x [Access time of TLB + Access time of RAM] + Miss ratio of TLB x [Access time of TLB + (2 x Access time of RAM) ] TLB Miss ratio = 1 \u2013 TLB Hit ratio 23/09/202 129 5 Module 5 - Memory Management Paging \u2013 Multi-Level Paging \uf0a7 Multi-level paging is a hierarchical memory management technique that organizes page tables into multiple levels instead of using a single flat page table. \uf0a7 In this scheme, the page table is divided into a tree-like structure where: \uf0a7 Higher-level page tables contain pointers to lower-level page tables \uf0a7 Lower-level page tables eventually contain actual frame numbers \uf0a7 Page Table Base Register (PTBR) points to the outermost (first-level) page table \uf0a7 Address translation occurs in stages, with each level providing part of the translation 23/09/202 130 5 Module 5 - Memory Management Paging \u2013 Multi-Level Paging \u2013 When Required? \uf0a7 23/09/202 131 5 Module 5 - Memory Management Paging \u2013"
  },
  {
    "id": 111,
    "source": "M5.txt",
    "text": "organizes page tables into multiple levels instead of using a single flat page table. \uf0a7 In this scheme, the page table is divided into a tree-like structure where: \uf0a7 Higher-level page tables contain pointers to lower-level page tables \uf0a7 Lower-level page tables eventually contain actual frame numbers \uf0a7 Page Table Base Register (PTBR) points to the outermost (first-level) page table \uf0a7 Address translation occurs in stages, with each level providing part of the translation 23/09/202 130 5 Module 5 - Memory Management Paging \u2013 Multi-Level Paging \u2013 When Required? \uf0a7 23/09/202 131 5 Module 5 - Memory Management Paging \u2013 Multi-Level Paging \u2013 Problem \uf0a7 Page No. Page Offset Frame No. Frame Offset 32 bits 44 bits 12 bits 12 bits 23/09/202 132 5 Module 5 - Memory Management Paging \u2013 Multi-Level Paging \u2013 Problem \uf0a7 23/09/202 133 5 Module 5 - Memory Management Paging \u2013 Multi-Level Paging \u2013 Problem \uf0a7 Now, we can observe- \uf0a7 The size of outer page table is same as frame size (4 KB). \uf0a7 Thus, outer page table can be stored in a single frame. \uf0a7 So, for given system, we will have two levels of page table. \uf0a7 Page Table Base Register (PTBR) will store the base address of the outer page table. 23/09/202 134 5 Module 5 - Memory Management Demand Paging \uf0a7 Demand paging is a memory management technique used in modern operating systems where pages of a process are only loaded into main memory when they are actually needed (on demand), rather than loading the entire process at once. \uf0a7 When a program starts, no or few pages are loaded initially. \uf0a7 If the program tries to access a page not in memory, a page fault occurs. \uf0a7 The operating system: \uf0a7 Pauses the program, \uf0a7 Locates the required page on disk, \uf0a7 Loads it into a free memory frame, \uf0a7 Updates the page table to reflect its new location. \uf0a7 The CPU resumes running the program as if the page had always been in memory. 23/09/202 135 5 Module 5 - Memory Management Demand Paging \u2013 Why? \uf0a7 Allows systems to run programs larger than physical memory. \uf0a7 Optimizes memory usage by keeping only frequently used pages in RAM. \uf0a7 Is transparent to user programs\u2014handled entirely by the OS. \uf0a7 Page Fault: Happens when a requested page isn\u2019t in main memory. \uf0a7 Secondary Storage: Where non-resident pages are stored (e.g., disk). \uf0a7 Page Replacement: If no free memory frame is available, the OS uses a page replacement algorithm to decide which existing page to evict. 23/09/202 136 5 Module 5 - Memory Management Page Fault & Page Replacement \uf0a7 A page fault occurs when a program accesses a memory page that is not currently present in physical memory (RAM) and must be loaded from secondary storage (like a hard disk or SSD). \uf0a7 The memory management unit (MMU) detects this condition and interrupts the program so the operating system (OS) can handle it. \uf0a7 Once the required page is"
  },
  {
    "id": 112,
    "source": "M5.txt",
    "text": "(e.g., disk). \uf0a7 Page Replacement: If no free memory frame is available, the OS uses a page replacement algorithm to decide which existing page to evict. 23/09/202 136 5 Module 5 - Memory Management Page Fault & Page Replacement \uf0a7 A page fault occurs when a program accesses a memory page that is not currently present in physical memory (RAM) and must be loaded from secondary storage (like a hard disk or SSD). \uf0a7 The memory management unit (MMU) detects this condition and interrupts the program so the operating system (OS) can handle it. \uf0a7 Once the required page is fetched into RAM, the process resumes as if the page had always been present. 23/09/202 137 5 Module 5 - Memory Management Page Fault & Page Replacement - Working \uf0a7 The MMU detects access to a missing page and raises an exception. \uf0a7 The OS checks if the access is valid. If not, the process may be terminated; if valid, the OS finds a free page frame in memory. \uf0a7 If no free frames are available, the OS must use a page replacement algorithm to choose a \"victim\" page to evict. \uf0a7 If the victim page has been modified, it's written back to disk (swap/page file). \uf0a7 The required page is loaded from secondary storage into the frame. \uf0a7 The page tables are updated and program execution continues. 23/09/202 138 5 Module 5 - Memory Management Page Replacement - Working \uf0a7 When main memory is full, the OS must decide which page to replace to make space for the new page. The page replacement policy directly affects system performance by influencing the page fault rate. \uf0a7 Select a victim page using an algorithm. \uf0a7 Evict the victim from RAM. \uf0a7 Load the required new page in its place. 23/09/202 139 5 Module 5 - Memory Management Page Replacement - Working \uf0a7 When main memory is full, the OS must decide which page to replace to make space for the new page. The page replacement policy directly affects system performance by influencing the page fault rate. \uf0a7 Select a victim page using an algorithm. \uf0a7 Evict the victim from RAM. \uf0a7 Load the required new page in its place. Algorithm How It Works Strengths Weaknesses Removes the oldest page in FIFO Simple Suffers from Belady\u2019s anomaly memory Removes page unused for Needs tracking/history LRU (Least Recently Used) Near-optimal longest time mechanism Removes page not needed for Optimal Lowest faults Requires future knowledge the longest future time Most/Most May not match real usage MFU/MRU Tunable Frequently/Recently Used patterns 23/09/202 140 5 Module 5 - Memory Management Page Replacement - Working 23/09/202 141 5"
  },
  {
    "id": 113,
    "source": "M5.txt",
    "text": "Removes page not needed for Optimal Lowest faults Requires future knowledge the longest future time Most/Most May not match real usage MFU/MRU Tunable Frequently/Recently Used patterns 23/09/202 140 5 Module 5 - Memory Management Page Replacement - Working 23/09/202 141 5"
  },
  {
    "id": 114,
    "source": "M7.txt",
    "text": "Storage Management, Protection and Security Module 7 Dr. Naveenkumar J Associate Professor, PRP- 217 - 4 Module 6 - Virtualization Hard Disk Drive \u2751A Hard Disk Drive (HDD) is a non-volatile data storage device commonly found in computers, laptops, servers, and many electronic devices. \u2751It uses magnetic storage to store and retrieve digital information by spinning multiple rigid disks (platters) coated with magnetic material. 02-11-2025 2 Module 6 - Virtualization HDD Key Characteristics \u2751Function: Stores operating systems, applications, documents, media files, and virtually all user and system data. \u2751Structure: Contains spinning platters, read/write heads, actuator arm, controller electronics, and is housed in a protective casing. \u2751Working Principle: Data is encoded as tiny magnetized zones on the platters. As platters spin, read/write heads move very close to the surface to read or change the magnetization, representing binary data (0s and 1s). \u2751Capacity: Modern HDDs range from hundreds of gigabytes (GB) to multiple terabytes (TB) 02-11-2025 3 Module 6 - Virtualization HDD \u2013 Simple Working \u2751 The platters spin at high speed (5400\u201315000 RPM). \u2751 Read/write heads attached to arms quickly move to locate the right place on the platter. \u2751 The controller board manages all operations and communicates with the computer via a connector (SATA, IDE, SCSI, etc.). \u2751 Data stays intact even when the device is powered off (non-volatile). 02-11-2025 4 Module 6 - Virtualization HDD \u2013 Components 02-11-2025 5 Module 6 - Virtualization HDD \u2013 Components \u2751Platters \u2751 Flat, circular disks (usually made of aluminum or glass, coated with magnetic material) where data is actually stored. \u2751 A standard HDD contains multiple platters stacked vertically; each surface can store data. \u2751Spindle \u2751 The rotating axis/wheel holding the platters in place. \u2751 Spins the platters at high speeds (typically 5400, 7200, or 10,000+ RPM), enabling rapid data access. 02-11-2025 6 Module 6 - Virtualization HDD \u2013 Components \u2751Read/Write Arm (Actuator Arm) \u2751 A thin, pivoting metallic arm that moves across the platters. \u2751 Positions the read/write head over the correct track on the platter for reading or writing data. \u2751Actuator (Voice Coil Motor) \u2751 The device (often an electromagnet) that controls the precise movement of the read/write arm. \u2751 Allows rapid, accurate movement to the required data location. 02-11-2025 7 Module 6 - Virtualization HDD \u2013 Components \u2751 Read/Write Head \u2751 Microscopic electromagnetic devices located at the tip of each actuator arm. \u2751 Flies just above the platter surface, reading or altering the magnetic fields to store/retrieve bits. \u2751 Each platter surface has its own dedicated head. \u2751Controller Board (PCB) \u2751 The electronic circuit board mounted on the underside of the drive. \u2751 Manages all internal communication, power regulation, and data transfer between the HDD and host (computer). 02-11-2025 8 Module 6 - Virtualization HDD \u2013 Components \u2751 Connector Interface \u2751 External data and power connectors (SATA, PATA/IDE, SCSI, etc.) for connecting to the motherboard and power supply. 02-11-2025 9 Module 6 - Virtualization HDD \u2013 Components Component Function/Role Casing Protection and physical support Platters Magnetic data storage Spindle Rotates"
  },
  {
    "id": 115,
    "source": "M7.txt",
    "text": "or altering the magnetic fields to store/retrieve bits. \u2751 Each platter surface has its own dedicated head. \u2751Controller Board (PCB) \u2751 The electronic circuit board mounted on the underside of the drive. \u2751 Manages all internal communication, power regulation, and data transfer between the HDD and host (computer). 02-11-2025 8 Module 6 - Virtualization HDD \u2013 Components \u2751 Connector Interface \u2751 External data and power connectors (SATA, PATA/IDE, SCSI, etc.) for connecting to the motherboard and power supply. 02-11-2025 9 Module 6 - Virtualization HDD \u2013 Components Component Function/Role Casing Protection and physical support Platters Magnetic data storage Spindle Rotates platters for data access Read/Write Arm Positions the heads over data tracks Actuator Moves the arm precisely Read/Write Head Reads/writes magnetic data Controller Board Controls logic, communication, power Connector Connects power/data to computer 02-11-2025 10 Module 6 - Virtualization Disk Scheduling Algorithm \u2751A disk scheduling algorithm is a strategy used by operating systems to decide the order in which disk I/O (input/output) requests are serviced. \u2751 These algorithms are vital in multi-tasking systems or environments with heavy disk usage, because multiple read/write requests may arrive simultaneously or out of sequence. 02-11-2025 11 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Why important \u2751 Minimizes overall waiting time for disk accesses. \u2751 Reduces mechanical movement of the disk\u2019s read/write heads, decreasing seek time and improving performance. \u2751 Prevents starvation and ensures fairness among requests. 02-11-2025 12 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Goals \u2751 Optimize: \u2751 Seek time (time to move disk head between tracks) \u2751 Rotational latency (time for required sector to rotate under head) \u2751 Throughput (number of serviced requests per time unit) \u2751 Fairness (avoid neglecting distant requests) 02-11-2025 13 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Seek Time \u2751 The time taken for the disk\u2019s read/write head to move from its current position to the track containing the desired data. \u2751Example \u2751 Suppose the head is at track 20 and your data is at track 80. If each track jump takes 0.5 ms, total seek time = 60 tracks \u00d7 0.5 ms = 30 ms. 02-11-2025 14 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Rotational Latency \u2751The time it takes for the desired disk sector to rotate under the read/write head after the head has reached the correct track. It depends on disk rotation speed. \u2751Example: \u2751 A disk spins at 7200 RPM (Revolutions Per Minute). One full rotation takes 60sec/7200 = 0.0083sec = 8.3ms \u2751 The average rotational latency is half of this: 4.15 ms. 02-11-2025 15 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Transfer Time \u2751 The time required to actually transfer (read/write) the data once the head is positioned correctly. \u2751Example: If the disk can transfer data at 100 MB/s and you want to read MB 1 1 MB, Transfer Time = = 0.01 sec = 10 ms. MB/s 100 02-11-2025 16 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Throughput \u2751 The number of"
  },
  {
    "id": 116,
    "source": "M7.txt",
    "text": "7200 RPM (Revolutions Per Minute). One full rotation takes 60sec/7200 = 0.0083sec = 8.3ms \u2751 The average rotational latency is half of this: 4.15 ms. 02-11-2025 15 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Transfer Time \u2751 The time required to actually transfer (read/write) the data once the head is positioned correctly. \u2751Example: If the disk can transfer data at 100 MB/s and you want to read MB 1 1 MB, Transfer Time = = 0.01 sec = 10 ms. MB/s 100 02-11-2025 16 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Throughput \u2751 The number of disk requests serviced in a given period (requests per second). \u2751Example: \u2751If 70 I/O requests are completed in 10 seconds, 70 Throughput = = 7 requests/sec. 10 02-11-2025 17 Module 6 - Virtualization Disk Scheduling Algorithm \u2013 Metrics \u2751Fairness \u2751 The extent to which requests\u2014whether near or far from the current head position\u2014are serviced without undue delay or starvation. \u2751Example: \u2751 SSTF may be fast for short seeks, but is less fair (may starve far requests), while SCAN and C-SCAN offer balanced fairness. 02-11-2025 18 Module 6 - Virtualization Disk Scheduling AlgorithmS Algorithm Definition FCFS (First-Come, First- Requests are processed in the exact order they arrive. Simple and fair, but not Served) always efficient. SSTF (Shortest Seek Time The request closest to the current position of the disk arm is handled first, which First) minimizes immediate head movement. The disk arm moves in one direction, servicing all requests along its path. When it SCAN (Elevator) reaches the end, it reverses and services requests on the way back, like an elevator. The disk arm moves in one direction, servicing requests until it reaches the end. C-SCAN (Circular SCAN) Then, it quickly returns to the start (without servicing on the way back) and repeats. The disk arm moves only as far as the last request in each direction, then reverses, LOOK instead of going all the way to the physical end of the disk. The arm goes from the current position to the furthest request in one direction, then C-LOOK jumps to the furthest request in the other direction and continues, skipping over empty parts of the disk. 02-11-2025 19 Module 6 - Virtualization Disk Scheduling Algorithm \u2751Suppose the head of moving disk with 200 tracks, numbered 0 to 199 is currently serving a request at track 132 and has just finished a request at track 120. \u2751 If the queue of requests is kept in the FIFO order 38, 55, 86, 123, 147, 91, 177, 115, 94, 150, 100, 175 and 130, 185. \u2751What is total head movement to satisfy these request for the all- disk scheduling algorithms? 02-11-2025 20 Module 6 - Virtualization Disk Scheduling Algorithm \u2751Given Data \u2751 Tracks: 0 to 199 \u2751 Current Head Position: 132 \u2751 Previous Head Position: 120 (Indicates the head is moving towards higher track numbers) \u2751 Request Queue (FIFO): 38, 55, 86, 123, 147, 91, 177, 115, 94, 150, 100, 175, 130, 185"
  },
  {
    "id": 117,
    "source": "M7.txt",
    "text": "a request at track 120. \u2751 If the queue of requests is kept in the FIFO order 38, 55, 86, 123, 147, 91, 177, 115, 94, 150, 100, 175 and 130, 185. \u2751What is total head movement to satisfy these request for the all- disk scheduling algorithms? 02-11-2025 20 Module 6 - Virtualization Disk Scheduling Algorithm \u2751Given Data \u2751 Tracks: 0 to 199 \u2751 Current Head Position: 132 \u2751 Previous Head Position: 120 (Indicates the head is moving towards higher track numbers) \u2751 Request Queue (FIFO): 38, 55, 86, 123, 147, 91, 177, 115, 94, 150, 100, 175, 130, 185 02-11-2025 21 Module 6 - Virtualization Disk Scheduling Algorithm - FCFS \u2751 In the FCFS algorithm, the requests are serviced in the order they appear in the queue. \u2751Path: 132 \u2192 38 \u2192 55 \u2192 86 \u2192 123 \u2192 147 \u2192 91 \u2192 177 \u2192 115 \u2192 94 \u2192 150 \u2192 100 \u2192 175 \u2192 130 \u2192 185 02-11-2025 22 Module 6 - Virtualization Disk Scheduling Algorithm - FCFS Total Step Current Head Position Request Serviced Head Movement Movement 1 132 38 |132 - 38| = 94 94 2 38 55 |38 - 55| = 17 111 3 55 86 |55 - 86| = 31 142 4 86 123 |86 - 123| = 37 179 5 123 147 |123 - 147| = 24 203 6 147 91 |147 - 91| = 56 259 7 91 177 |91 - 177| = 86 345 8 177 115 |177 - 115| = 62 407 9 115 94 |115 - 94| = 21 428 10 94 150 |94 - 150| = 56 484 11 150 100 |150 - 100| = 50 534 12 100 175 |100 - 175| = 75 609 13 175 130 |175 - 130| = 45 654 14 130 185 |130 - 185| = 55 709 02-11-2025 23 Module 6 - Virtualization Disk Scheduling Algorithm - FCFS FCFS Algo 200 185 180 177 175 160 150 147 140 132 130 123 120 115 100 100 94 91 86 80 60 55 40 38 20 0 132 132 38 55 86 123 147 91 177 115 94 150 100 175 130 02-11-2025 24 Module 6 - Virtualization Disk Scheduling Algorithm - SSTF Total Step Current Head Position Request Serviced Head Movement Movement 1 132 130 |132 - 130| = 2 2 2 130 123 |130 - 123| = 7 9 3 123 115 |123 - 115| = 8 17 4 115 100 |115 - 100| = 15 32 5 100 94 |100 - 94| = 6 38 The head moves to the closest pending request. 6 94 91 |94 - 91| = 3 41 7 91 86 |91 - 86| = 5 46 8 86 55 |86 - 55| = 31 77 9 55 38 |55 - 38| = 17 94 10 38 147 |38 - 147| = 109 203 11 147 150 |147 - 150| = 3 206 12 150 175 |150 - 175| = 25 231 13 175 177 |175"
  },
  {
    "id": 118,
    "source": "M7.txt",
    "text": "115 |123 - 115| = 8 17 4 115 100 |115 - 100| = 15 32 5 100 94 |100 - 94| = 6 38 The head moves to the closest pending request. 6 94 91 |94 - 91| = 3 41 7 91 86 |91 - 86| = 5 46 8 86 55 |86 - 55| = 31 77 9 55 38 |55 - 38| = 17 94 10 38 147 |38 - 147| = 109 203 11 147 150 |147 - 150| = 3 206 12 150 175 |150 - 175| = 25 231 13 175 177 |175 - 177| = 2 233 14 177 185 |177 - 185| = 8 241 02-11-2025 25 Module 6 - Virtualization Disk Scheduling SASTF lgorithm - SSTF 200 180 186 177 175 160 150 140 147 132 130 120 123 115 100 100 94 91 80 86 60 55 40 38 20 0 132 132 130 123 115 100 94 91 86 55 38 147 150 175 177 02-11-2025 26 Module 6 - Virtualization Disk Scheduling Algorithm - SCAN Current Total Direction Path Segment & Requests Serviced Justification Head Movement Position Movement Continue in initial 132 \u2191 132 \u2192 147 \u2192 150 \u2192 175 \u2192 177 \u2192 185 |185 - 132| = 53 53 direction (UP). 185 \u2191 185 \u2192 199 Continue to end of disk. |199 - 185| = 14 67 199 \u2193 Reverse Direction At disk end, reverse to \u2193. 0 67 Service all remaining 199 \u2193 199 \u2192 130 \u2192 123 \u2192 ... \u2192 38 |199 - 38| = 161 228 requests. Sweep in one direction servicing all requests, then reverse at the disk's end and sweep back. 02-11-2025 27 Module 6 - Virtualization Disk Scheduling Algorithm - SCAN SCAN 250 200 199 185 177 175 150 150 147 132 130 123 115 100 100 94 91 86 50 55 38 0 132 132 147 150 175 177 185 199 130 123 115 100 94 91 86 55 02-11-2025 28 Module 6 - Virtualization Disk Scheduling Algorithm - CSCAN Current Head Total Direction Path Segment & Requests Serviced Justification Position Movement Movement |185 - 132| = 132 \u2191 132 \u2192 147 \u2192 150 \u2192 175 \u2192 177 \u2192 185 Continue in initial direction (UP). 53 53 |199 - 185| = 185 \u2191 185 \u2192 199 Continue to end of disk. 67 14 |199 - 0| = 199 JUMP 199 \u2192 0 At disk end, jump to start. 266 199 Service requests from start in UP |130 - 0| = 0 \u2191 0 \u2192 38 \u2192 55 \u2192 ... \u2192 130 396 direction. 130 Sweep in one direction only. After reaching the end, jump back to the start and sweep again. 02-11-2025 29 Module 6 - Virtualization Disk Scheduling Algorithm - CSCAN CSCAN 250 200 199 185 177 175 150 150 147 132 130 123 115 100 100 94 91 86 50 55 38 0 132 132 147 150 175 177 185 1099 0 38 55 86 91 94"
  },
  {
    "id": 119,
    "source": "M7.txt",
    "text": "199 \u2192 0 At disk end, jump to start. 266 199 Service requests from start in UP |130 - 0| = 0 \u2191 0 \u2192 38 \u2192 55 \u2192 ... \u2192 130 396 direction. 130 Sweep in one direction only. After reaching the end, jump back to the start and sweep again. 02-11-2025 29 Module 6 - Virtualization Disk Scheduling Algorithm - CSCAN CSCAN 250 200 199 185 177 175 150 150 147 132 130 123 115 100 100 94 91 86 50 55 38 0 132 132 147 150 175 177 185 1099 0 38 55 86 91 94 100 115 123 02-11-2025 30 Module 6 - Virtualization Disk Scheduling Algorithm - Look Current Total Direction Path Segment & Requests Serviced Justification Head Movement Position Movement Continue in initial 132 \u2191 132 \u2192 147 \u2192 150 \u2192 175 \u2192 177 \u2192 185 |185 - 132| = 53 53 direction (UP). At last request (185), 185 \u2193 Reverse Direction 0 53 reverse to \u2193. Service all remaining 185 \u2193 185 \u2192 130 \u2192 123 \u2192 ... \u2192 38 |185 - 38| = 147 200 requests. Like SCAN, but reverse direction after the last request in that direction (don't go to the end). 02-11-2025 31 Module 6 - Virtualization Disk Scheduling Algorithm - Look LOOK 200 180 185 185 177 175 160 150 140 147 132 130 120 123 115 100 100 94 91 80 86 60 55 40 38 20 0 132 132 147 150 175 177 185 185 130 123 115 100 94 91 86 55 02-11-2025 32 Module 6 - Virtualization Disk Scheduling Algorithm - CLook Current Total Direction Path Segment & Requests Serviced Justification Head Movement Position Movement Continue in initial 132 \u2191 132 \u2192 147 \u2192 150 \u2192 175 \u2192 177 \u2192 185 |185 - 132| = 53 53 direction (UP). At last request, 185 JUMP 185 \u2192 38 jump to first in 0 (jump, not sweep) 53 queue. Service remaining 38 \u2191 38 \u2192 55 \u2192 86 \u2192 ... \u2192 130 requests from new |130 - 38| = 92 145 start. Like C-SCAN, but jump from the last request in one direction to the first request in the queue. 02-11-2025 33 Module 6 - Virtualization Disk Scheduling Algorithm - CLook CLOOK 200 180 185 177 175 160 150 140 147 132 130 120 123 115 100 100 94 91 80 86 60 55 40 38 20 0 132 132 147 150 175 177 1085 0 38 55 86 91 94 100 115 123 02-11-2025 34 Module 6 - Virtualization Disk Scheduling Algorithm - All Disk Schedule 250 200 150 100 50 0 132 132 147 150 175 177 185 0 38 55 86 91 94 100 115 123 FCFS SSTF SCAN CSCAN LOOK CLOOK 02-11-2025 35 Module 6 - Virtualization Disk Scheduling Algorithm \u2751Suppose the head of moving disk with 1000 tracks, numbered 0 to 999 is currently serving a request at track 132 and has just finished a request at track 120. \u2751If the"
  },
  {
    "id": 120,
    "source": "M7.txt",
    "text": "20 0 132 132 147 150 175 177 1085 0 38 55 86 91 94 100 115 123 02-11-2025 34 Module 6 - Virtualization Disk Scheduling Algorithm - All Disk Schedule 250 200 150 100 50 0 132 132 147 150 175 177 185 0 38 55 86 91 94 100 115 123 FCFS SSTF SCAN CSCAN LOOK CLOOK 02-11-2025 35 Module 6 - Virtualization Disk Scheduling Algorithm \u2751Suppose the head of moving disk with 1000 tracks, numbered 0 to 999 is currently serving a request at track 132 and has just finished a request at track 120. \u2751If the queue of requests is kept in the FIFO order 358, 555, 861, 123, 147, 911,177, 115, 994, 150 , 100, 175, and 130, 185 \u2751 what is total head movement to satisfy these request for the following disk scheduling algorithms? \u2751(a) FCFS (b)LOOK (C) SCAN 02-11-2025 36"
  },
  {
    "id": 121,
    "source": "Module 3_Example.txt",
    "text": "Operating System Module \u2013 3 Dr. Naveenkumar Jayakumar PRP \u2013 217 4 CPU Scheduling \u2013 Sample Problem \u25aa Consider three processes, P1, P2, and P3, that arrive at time t = 0 with the Process CPU Burst 1(ms) I/O Burst(ms) CPU Burst 2(ms) following characteristics: P1 10 20 5 P2 7 15 3 P3 12 25 8 \u25aa Assume a Round Robin scheduler with a time quantum of 5 ms. Each context switch takes 1 ms. Processes begin with a CPU burst, followed by an I/O burst, and finally another CPU burst. Calculate average waiting time, average turnaround time, Response time, CPU utilization time and No. of Context switch. CPU Scheduling \u2013 Sample Problem At time 0: P1 runs for its first 5ms quantum. P1 0 5 CPU Scheduling \u2013 Sample Problem At time 5: Context switch. P1 CS 0 5 6 CPU Scheduling \u2013 Sample Problem At time 6: P2 runs for its 5ms quantum.. P1 CS P2 0 5 6 11 CPU Scheduling \u2013 Sample Problem At time 11: Context switch. P1 CS P2 CS 0 5 6 11 12 CPU Scheduling \u2013 Sample Problem At time 12: P3 runs for its 5ms quantum. P1 CS P2 CS P3 0 5 6 11 12 17 CPU Scheduling \u2013 Sample Problem At time 17: Context Switch P1 CS P2 CS P3 CS 0 5 6 11 12 17 18 CPU Scheduling \u2013 Sample Problem At time 18: P1 runs for its remaining 5ms of CPU Burst 1, then begins its 20ms I/O burst, which will complete at t = 43ms P1 CS P2 CS P3 CS P1 0 5 6 11 12 17 18 23 CPU Scheduling \u2013 Sample Problem At time 18: P1 runs for its remaining 5ms of CPU Burst 1, then begins its 20ms I/O burst, which will complete at t = 43ms P1 CS P2 CS P3 CS P1 0 5 6 11 12 17 18 23 P1 I/O Burst CPU Scheduling \u2013 Sample Problem At time 23: Context switch. P1 CS P2 CS P3 CS P1 CS 0 5 6 11 12 17 18 23 24 P1 I/O Burst CPU Scheduling \u2013 Sample Problem At time 24: P2 runs for its remaining 2ms of CPU Burst 1, then begins its 15ms I/O burst, which will complete at t = 41ms. P1 CS P2 CS P3 CS P1 CS P2 0 5 6 11 12 17 18 23 24 26 P1 I/O Burst CPU Scheduling \u2013 Sample Problem At time 24: P2 runs for its remaining 2ms of CPU Burst 1, then begins its 15ms I/O burst, which will complete at t = 41ms. P1 CS P2 CS P3 CS P1 CS P2 0 5 6 11 12 17 18 23 24 26 P1 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem At time 26: Context Switch P1 CS P2 CS P3 CS P1 CS P2 CS 0 5 6 11 12 17 18 23 24 26 27 P1 I/O Burst P2 I/O"
  },
  {
    "id": 122,
    "source": "Module 3_Example.txt",
    "text": "17 18 23 24 26 P1 I/O Burst CPU Scheduling \u2013 Sample Problem At time 24: P2 runs for its remaining 2ms of CPU Burst 1, then begins its 15ms I/O burst, which will complete at t = 41ms. P1 CS P2 CS P3 CS P1 CS P2 0 5 6 11 12 17 18 23 24 26 P1 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem At time 26: Context Switch P1 CS P2 CS P3 CS P1 CS P2 CS 0 5 6 11 12 17 18 23 24 26 27 P1 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem At time 27: P3 runs for another 5ms quantum. P1 CS P2 CS P3 CS P1 CS P2 CS P3 0 5 6 11 12 17 18 23 24 26 27 32 P1 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem At time 32: P3 runs for another 2ms quantum. It then begins its 25ms I/O burst, which will complete at t = 59ms. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 0 5 6 11 12 17 18 23 24 26 27 32 34 P1 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem At time 32: P3 runs for another 2ms quantum. It then begins its 25ms I/O burst, which will complete at t = 59ms. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 0 5 6 11 12 17 18 23 24 26 27 32 34 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 34: CPU is IDLE P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 41: P2's I/O completes. It runs for its 3ms CPU Burst 2. P2 finishes at t = 44ms. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 44: Context Switch P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 45: P1's I/O completes (at t=43). It runs for its 5ms CPU Burst 2. P1 finishes at t = 50ms. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS P1 IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 50 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At"
  },
  {
    "id": 123,
    "source": "Module 3_Example.txt",
    "text": "0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 45: P1's I/O completes (at t=43). It runs for its 5ms CPU Burst 2. P1 finishes at t = 50ms. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS P1 IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 50 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 50: The CPU is idle. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS P1 IDLE IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 50 59 P1 I/O Burst P2 I/O Burst P3 I/O Burst CPU Scheduling \u2013 Sample Problem At time 59: P3's I/O completes. It runs for a 5ms quantum of its CPU Burst 2. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS P1 P3 IDLE IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 50 59 64 P1 I/O Burst P3 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem At time 64: P3 runs for its final 3ms of CPU Burst 2. P3 finishes at t = 67ms. P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS P1 P3 P3 IDLE IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 50 59 64 67 P1 I/O Burst P3 I/O Burst P2 I/O Burst CPU Scheduling \u2013 Sample Problem P1 CS P2 CS P3 CS P1 CS P2 CS P3 P3 P2 CS P1 P3 P3 IDLE IDLE 0 5 6 11 12 17 18 23 24 26 27 32 34 41 44 45 50 59 64 67 P3 I/O Burst P1 I/O Burst P2 I/O Burst Turnaround Response CPU Process Arrival Rate Burst Time Completion Time Waiting Time Time Time Utilization P1 0 15 50 50 \u2013 0 = 50 50 \u2013 15 = 35 0 \u2013 0 = 0 15 / 51 = 29.41 P2 0 10 44 44 \u2013 0 = 44 44 \u2013 10 = 34 6 \u2013 0 = 6 10 / 51 = 19.60 P3 0 20 67 67 \u2013 0 = 67 67 \u2013 20 = 47 12 \u2013 0 = 12 20 / 51 = 39.21 Total CPU Burst Time = 15 + 10 + 20 + 6 = 51 Total Elapsed Time = 67 Total CPU utilization = (51 / 67) * 100 = 76.11 Idle CPU time = 67 \u2013 51 = 16 Total Idle time = (16/67) * 100 = 23.88 No. of Context Switch = 06"
  },
  {
    "id": 124,
    "source": "Module 3_Example.txt",
    "text": "6 10 / 51 = 19.60 P3 0 20 67 67 \u2013 0 = 67 67 \u2013 20 = 47 12 \u2013 0 = 12 20 / 51 = 39.21 Total CPU Burst Time = 15 + 10 + 20 + 6 = 51 Total Elapsed Time = 67 Total CPU utilization = (51 / 67) * 100 = 76.11 Idle CPU time = 67 \u2013 51 = 16 Total Idle time = (16/67) * 100 = 23.88 No. of Context Switch = 06"
  },
  {
    "id": 125,
    "source": "Semaphores.txt",
    "text": "Laboratory 3 Synchronization in Operating System Aim & Objectives \u25aa Implement process synchronization using semaphores / monitors. \u25aa Objectives: \u25aa To simulate the working of Semaphores (Binary) \u25aa To simulate the working of Monitors for any one Classical synchronization Problems. Semaphores \u25aa A semaphore is a simple integer variable used as a signaling mechanism to solve synchronization problems. \u25aa It ensures that multiple processes can safely access and modify shared resources without interfering with each other, which could otherwise lead to incorrect results. \u25aa The core issue that semaphores address is the critical section problem. Semaphores \u25aa A \"critical section\" is a piece of code where a process accesses a shared resource (like a shared memory variable, a file, or a database). \u25aa If multiple processes execute this section simultaneously, it can lead to a \"race condition,\" where the final state of the shared resource depends on the unpredictable order in which the processes run, often producing erroneous outcomes. Semaphores - Working \u25aa A semaphore controls access to a shared resource through two atomic (indivisible) operations : \u25aa wait(S): This operation, also known as P(), decrements the semaphore's integer value S. \u25aa If the value becomes less than or equal to zero, the process that called wait() is blocked and put into a waiting queue. \u25aa If the value is positive, the process is allowed to continue into its critical section. Semaphores - Working \u25aa A semaphore controls access to a shared resource through two atomic (indivisible) operations : \u25aa signal(S): This operation, also known as V(), increments the semaphore's value S. \u25aa If there are any processes waiting for the semaphore, one of them is unblocked and allowed to proceed. Semaphores - Types \u25aa There are two main types of semaphores: \u25aa Counting Semaphore: The value can be any non-negative integer. It is used to control access to a resource that has multiple instances. The semaphore is initialized to the number of available resources. \u25aa Binary Semaphore: The value can only be 0 or 1. It functions as a mutex lock, ensuring that only one process can access a resource at a time. It is initialized to 1 (available). Semaphores - pseudocode signal(Semaphore S) wait(Semaphore S) { { S = S - 1 S = S + 1 if (S < 0) if (S <= 0) { { // Block the process // Remove a process from semaphore's add process to semaphore's waiting queue waiting queue sleep() // Process sleeps until signaled wakeup(process) } // else continue, process enters critical section // Wake up one blocked process } } } Semaphores \u2013 pseudocode - Tracing Initial Value of Semaphore S = 1 (Semaphore starts unlocked, one resource available), No Processes waiting Process 1 or Thread 1 Process 2 or Thread 2 calls wait(S) S = 1 - 1 = 0; Since S \u2265 0, P1 or T1 can enter the critical section immediately. No blocking occurs. State now: S=0S=0, 0 resources available (busy), 0 waiting processes. calls wait(S) S"
  },
  {
    "id": 126,
    "source": "Semaphores.txt",
    "text": "process to semaphore's waiting queue waiting queue sleep() // Process sleeps until signaled wakeup(process) } // else continue, process enters critical section // Wake up one blocked process } } } Semaphores \u2013 pseudocode - Tracing Initial Value of Semaphore S = 1 (Semaphore starts unlocked, one resource available), No Processes waiting Process 1 or Thread 1 Process 2 or Thread 2 calls wait(S) S = 1 - 1 = 0; Since S \u2265 0, P1 or T1 can enter the critical section immediately. No blocking occurs. State now: S=0S=0, 0 resources available (busy), 0 waiting processes. calls wait(S) S = 0 \u2013 1 = -1 Since S < 0, P2 or T2 is blocked and added to the waiting queue. State now: S = \u22121, no resources available, 1 process waiting (P2 or T2). Semaphores \u2013 pseudocode - Tracing Initial Value of Semaphore S = 1 (Semaphore starts unlocked, one resource available), No Processes waiting Process 1 or Thread 1 Process 2 or Thread 2 calls signal(S) S = -1 + 1 = 0; Since S \u2264 0, one blocked process (P2 or T2) is removed from waiting queue and resumed. P2 or T2 can now enter the critical section. State now: S = 0, 0 resources available (P2/T2 Eventually P2/T2 calls Signal(S) in critical section), 0 waiting processes. S = 0 + 1 = 1 Since S > 0, no waiting processes to wake. State now: S =1, 1 resource available (semaphore unlocked), 0 waiting processes. Semaphores - pseudocode S = 1: The lock is available. S = 0: The lock is held by one process. S < 0: The lock is held, and one or more other processes are waiting for it. Semaphores - Example Includes the libraries for #include <stdio.h> input/output, POSIX threads, #include <pthread.h> and semaphores. #include <semaphore.h> Semaphores - Example Declares a shared integer int x = 0; variable x initialized to 0. Declares a semaphore mutex to sem_t mutex; control access to x. Semaphores - Example Defines the function for the thread void* increment_thread(void* arg) { that increments x. sem_wait(&mutex); sem_wait(&mutex) waits to acquire x += 5; the semaphore (lock). printf(\"Incremented x to %d\\n\", Adds 5 to x. x); Prints the new value of x. sem_post(&mutex); sem_post(&mutex) releases the return NULL; semaphore (unlock). } Exits the thread function. Semaphores - Example Defines the function for the void* decrement_thread(void* arg) { thread that decrements x. sem_wait(&mutex); x -= 5; printf(\"Decremented x to %d\\n\", Waits to acquire the semaphore, x); subtracts 5 from x, prints it, then releases the semaphore and sem_post(&mutex); exits. return NULL; } Semaphores - Example Main function declares two thread identifiers. int main() { Initializes mutex as a binary semaphore with pthread_t tid1, tid2; initial value 1 (unlocked). &mutex: This is the address of the sem_init(&mutex, 0, 1); semaphore variable to initialize. 0: The pshared parameter. A value of 0 means the semaphore is shared between threads of the same process only (not between different processes). 1: The initial value of"
  },
  {
    "id": 127,
    "source": "Semaphores.txt",
    "text": "x. sem_wait(&mutex); x -= 5; printf(\"Decremented x to %d\\n\", Waits to acquire the semaphore, x); subtracts 5 from x, prints it, then releases the semaphore and sem_post(&mutex); exits. return NULL; } Semaphores - Example Main function declares two thread identifiers. int main() { Initializes mutex as a binary semaphore with pthread_t tid1, tid2; initial value 1 (unlocked). &mutex: This is the address of the sem_init(&mutex, 0, 1); semaphore variable to initialize. 0: The pshared parameter. A value of 0 means the semaphore is shared between threads of the same process only (not between different processes). 1: The initial value of the semaphore. Setting it to 1 means it acts as a binary semaphore (or mutex), allowing only one thread to enter the critical section at a time. Semaphores - Example Creates two threads, one pthread_create(&tid1, NULL, running increment_thread, the increment_thread, NULL); other decrement_thread. pthread_create(&tid2, NULL, decrement_thread, NULL); Semaphores - Example Waits for both threads to pthread_join(tid1, NULL); complete before proceeding. pthread_join(tid2, NULL); sem_destroy(&mutex); Destroys the semaphore to clean up resources. Prints the final value of x after printf(\"Final value of x: %d\\n\", x); both threads finish. return 0; Ends the main program } Create Own Semaphores - Code \u25aa Create a structure that holds // A simple semaphore implementation the integer value, along with a typedef struct { mutex and a condition variable, pthread_mutex_t mutex; which are standard tools for pthread_cond_t condition; building synchronization int value; primitives like semaphores. } semaphore; Create Own Semaphores - Code void semaphore_init(semaphore* \u25aa semaphore_init: Initializes the sem, int initial_value) { semaphore with a starting value. We use 1 for a binary pthread_mutex_init(&(sem- semaphore, meaning one >mutex), NULL); resource is available. pthread_cond_init(&(sem- >condition), NULL); sem->value = initial_value; } Create Own Semaphores - Code // The 'wait' operation \u25aa semaphore_wait: A thread calls void semaphore_wait(semaphore* sem) { this before entering its critical pthread_mutex_lock(&(sem->mutex)); section. It locks the mutex to while (sem->value <= 0) { ensure its check is atomic, // Wait on the condition variable if semaphore value is not positive then checks sem->value. If the pthread_cond_wait(&(sem->condition), value is 0, it waits. Otherwise, &(sem->mutex)); it decrements the value and } sem->value--; proceeds. pthread_mutex_unlock(&(sem->mutex)); Create Own Semaphores - Code // The 'signal' operation \u25aa semaphore_signal: After leaving void semaphore_signal(semaphore* sem) { the critical section, the thread pthread_mutex_lock(&(sem->mutex)); calls this to release the sem->value++; resource. It increments sem- // Signal one waiting thread >value and signals one of the pthread_cond_signal(&(sem- >condition)); waiting threads (if any) to wake pthread_mutex_unlock(&(sem- up. >mutex)); } Create Own Semaphores - Code // A function that threads will run \u25aa main and thread_function: The void* thread_function(void* arg) { semaphore* sem = (semaphore*)arg; main process and a newly printf(\"Thread is trying to acquire the created thread both compete semaphore...\\n\"); semaphore_wait(sem); for the same semaphore. Only // --- Critical Section Start --- printf(\"Thread has acquired the semaphore one can be in its \"critical and is in the critical section.\\n\"); // --- Critical Section End --- section\" at any given time, printf(\"Thread is releasing the preventing overlap"
  },
  {
    "id": 128,
    "source": "Semaphores.txt",
    "text": "the pthread_cond_signal(&(sem- >condition)); waiting threads (if any) to wake pthread_mutex_unlock(&(sem- up. >mutex)); } Create Own Semaphores - Code // A function that threads will run \u25aa main and thread_function: The void* thread_function(void* arg) { semaphore* sem = (semaphore*)arg; main process and a newly printf(\"Thread is trying to acquire the created thread both compete semaphore...\\n\"); semaphore_wait(sem); for the same semaphore. Only // --- Critical Section Start --- printf(\"Thread has acquired the semaphore one can be in its \"critical and is in the critical section.\\n\"); // --- Critical Section End --- section\" at any given time, printf(\"Thread is releasing the preventing overlap and semaphore.\\n\"); semaphore_signal(sem); ensuring synchronized access. return NULL; } Create Own Semaphores - Code int main() { // --- Critical Section End --- semaphore sem; // Initialize as a binary semaphore (value 1) printf(\"Main process is releasing the semaphore_init(&sem, 1); semaphore.\\n\"); pthread_t thread; semaphore_signal(&sem); pthread_create(&thread, NULL, thread_function, (void*)&sem); // Wait for the other thread to finish printf(\"Main process is trying to acquire the semaphore...\\n\"); pthread_join(thread, NULL); semaphore_wait(&sem); // --- Critical Section Start --- printf(\"Main process has acquired the return 0; semaphore and is in the critical section.\\n\"); } Create Own Semaphores - Code \u25aa pthread_mutex_t mutex: A mutex (mutual exclusion \u25aa #include <pthread.h>: This lock). This is used to protect the value field within the includes the POSIX threads semaphore itself, ensuring that only one thread can library. It provides all the modify it at a time. This prevents race conditions inside our semaphore logic. necessary functions and data \u25aa pthread_cond_t condition: A condition variable. This types for creating and allows threads to wait (\"sleep\") efficiently without managing threads, mutexes, wasting CPU time (unlike busy-waiting). When a condition is met (i.e., the semaphore becomes and condition variables (e.g., available), one of the waiting threads can be \"woken pthread_t, pthread_create, up.\" pthread_mutex_t, \u25aa int value: The integer counter of the semaphore. For a pthread_cond_t). binary semaphore, this will be 1 (available) or 0 (in use). For a counting semaphore, it can be any non- negative number. Create Own Semaphores - Code \u25aa pthread_mutex_init(&(sem- \u25aa pthread_mutex_lock(&(sem->mutex)): The >mutex), NULL): Initializes the thread first locks the semaphore's internal mutex inside the semaphore struct. mutex. This ensures that no other thread The NULL argument specifies default can check or change sem->value at the attributes for the mutex. same time. \u25aa pthread_cond_init(&(sem- >condition), NULL): Initializes the \u25aa while (sem->value <= 0): The thread checks condition variable inside the if the semaphore is available. If value is 0 or semaphore struct with default less, the resource is not available. A while attributes. loop is used instead of an if statement to \u25aa sem->value = initial_value;: Sets handle \"spurious wakeups\" (where a thread the semaphore's starting count. For might be woken up even if the condition a binary semaphore, initial_value isn't truly met). will be 1. Create Own Semaphores - Code \u25aa pthread_cond_wait(&(sem- \u25aa sem->value--: If the while loop >condition), &(sem->mutex)): If the condition was false (or after the thread resource is not available, the thread calls"
  },
  {
    "id": 129,
    "source": "Semaphores.txt",
    "text": "variable inside the if the semaphore is available. If value is 0 or semaphore struct with default less, the resource is not available. A while attributes. loop is used instead of an if statement to \u25aa sem->value = initial_value;: Sets handle \"spurious wakeups\" (where a thread the semaphore's starting count. For might be woken up even if the condition a binary semaphore, initial_value isn't truly met). will be 1. Create Own Semaphores - Code \u25aa pthread_cond_wait(&(sem- \u25aa sem->value--: If the while loop >condition), &(sem->mutex)): If the condition was false (or after the thread resource is not available, the thread calls this function to go to sleep. is woken up and the condition becomes Crucially, this function atomically does false), it means the resource is two things: \u25aa It unlocks the mutex. This is vital so available. The thread decrements the that another thread can call semaphore's value to mark it as \"in semaphore_signal to release the resource. use.\" \u25aa It puts the current thread to sleep until it is signaled. \u25aa pthread_mutex_unlock(&(sem- \u25aa When the thread is woken up by a >mutex)): The thread unlocks the pthread_cond_signal, it automatically re-acquires the lock on the mutex before internal mutex, allowing other threads proceeding. to interact with the semaphore. Create Own Semaphores - Code \u25aa pthread_mutex_lock(&(sem->mutex)): The \u25aa semaphore* sem = (semaphore*)arg;: The arg passed thread locks the internal mutex to safely from pthread_create is a void pointer. This line casts it back to its original type, a pointer to a semaphore. modify the semaphore's value. \u25aa semaphore_wait(sem): The thread calls our wait function \u25aa sem->value++: It increments the to request access to the resource. The function will not semaphore's value, indicating that a return until the semaphore is acquired. resource has been freed. \u25aa // --- Critical Section Start ---: This marks the beginning of the code that accesses the shared resource. Only one \u25aa pthread_cond_signal(&(sem->condition)): thread can be in this section at a time. This function \"wakes up\" at least one of the threads that are currently waiting inside \u25aa // --- Critical Section End ---: Marks the end of the pthread_cond_wait. The awakened thread protected code. will then attempt to re-acquire the mutex \u25aa semaphore_signal(sem): After leaving the critical section, and re-check the while loop condition in the thread calls signal to release the semaphore, allowing semaphore_wait. another waiting thread to proceed. \u25aa return NULL;: The function returns a NULL pointer, as it \u25aa pthread_mutex_unlock(&(sem->mutex)): has no specific result to return. The thread unlocks the internal mutex. Create Own Semaphores - Code \u25aa semaphore sem;: Declares a variable of \u25aa &thread: A pointer to store the ID of the new thread. our custom semaphore type. \u25aa NULL: Use default thread attributes. \u25aa semaphore_init(&sem, 1);: Initializes \u25aa thread_function: The function the new thread will our semaphore with a value of 1, execute. making it a binary semaphore that is \u25aa (void*)&sem: A pointer to our semaphore, which is initially available. passed as an argument to thread_function. \u25aa pthread_t thread;: Declares"
  },
  {
    "id": 130,
    "source": "Semaphores.txt",
    "text": "NULL pointer, as it \u25aa pthread_mutex_unlock(&(sem->mutex)): has no specific result to return. The thread unlocks the internal mutex. Create Own Semaphores - Code \u25aa semaphore sem;: Declares a variable of \u25aa &thread: A pointer to store the ID of the new thread. our custom semaphore type. \u25aa NULL: Use default thread attributes. \u25aa semaphore_init(&sem, 1);: Initializes \u25aa thread_function: The function the new thread will our semaphore with a value of 1, execute. making it a binary semaphore that is \u25aa (void*)&sem: A pointer to our semaphore, which is initially available. passed as an argument to thread_function. \u25aa pthread_t thread;: Declares a variable \u25aa The main thread then proceeds to compete for the to hold the thread identifier. same semaphore by calling semaphore_wait(&sem). \u25aa pthread_join(thread, NULL);: The main thread waits \u25aa pthread_create(&thread, NULL, here until the other thread (thread) has finished its thread_function, (void*)&sem);: This execution. This ensures the program doesn't exit function creates and starts a new thread. prematurely. \u25aa return 0;: Indicates that the program has completed successfully. Create Own Semaphores - Code \u25aa The main program starts. \u25aa It creates a semaphore (our key holder) and sets its value to 1 (key available). \u25aa It creates a new thread. Now we have two workers: the main thread and the new thread. \u25aa Both threads will try to run semaphore_wait to get the key. It's a race! \u25aa Whoever gets there first will lock the semaphore, change its value to 0, and enter its \"critical section.\" \u25aa The second thread to arrive will see the value is 0 and will be forced to wait. \u25aa The first thread finishes its work, calls semaphore_signal, changes the value back to 1, and rings the bell. \u25aa The waiting second thread wakes up, sees the value is now 1, and finally gets the key to do its work. \u25aa Once both threads are done, the main program cleans up and exits. Classical Synchronization Problem \u25aa The classical synchronization problems in operating systems are fundamental concurrency issues arising when multiple processes or threads need coordinated access to shared resources. \u25aa These problems illustrate challenges such as avoiding deadlock, ensuring mutual exclusion, and preventing race conditions. 1. Bounded-Buffer (Producer-Consumer) Problem 2. Dining Philosophers Problem 3. Readers-Writers Problem 4. Sleeping Barber Problem Bounded-Buffer (Producer-Consumer) Problem \u25aa This involves a fixed-size buffer shared between two types of processes: \u25aa Producers, who generate data and place it in the buffer, and \u25aa Consumers, who remove data from the buffer. \u25aa Synchronization ensures that producers don't add data when the buffer is full and consumers don't remove data when the buffer is empty. \u25aa The challenge is to coordinate access so that no data is lost or corrupted while ensuring efficient buffer usage. \u25aa Semaphores or mutex locks are typically used for this synchronization. Producer-Consumer \u2013 Semaphore Solution \u25aa Semaphores ensure proper synchronization by controlling access to the buffer slots and mutually exclusive operations on shared data structures. \u25aa Shared Entities: \u25aa A fixed-size buffer to hold produced"
  },
  {
    "id": 131,
    "source": "Semaphores.txt",
    "text": "in the buffer, and \u25aa Consumers, who remove data from the buffer. \u25aa Synchronization ensures that producers don't add data when the buffer is full and consumers don't remove data when the buffer is empty. \u25aa The challenge is to coordinate access so that no data is lost or corrupted while ensuring efficient buffer usage. \u25aa Semaphores or mutex locks are typically used for this synchronization. Producer-Consumer \u2013 Semaphore Solution \u25aa Semaphores ensure proper synchronization by controlling access to the buffer slots and mutually exclusive operations on shared data structures. \u25aa Shared Entities: \u25aa A fixed-size buffer to hold produced data. \u25aa Two indices: in for the next slot to insert by the producer, and out for the next slot to remove by the consumer. \u25aa Three semaphores: \u25aa mutex (binary semaphore) for mutual exclusion to ensure that only one process accesses the buffer or modifies in/out pointers at a time. \u25aa empty (counting semaphore) initialized to the buffer size, representing the number of empty slots. \u25aa full (counting semaphore) initialized to zero, representing the number of filled slots. Bounded-Buffer (Producer-Consumer) Problem Initialize semaphore mutex = 1 // Binary semaphore for mutual exclusion semaphore empty = BUFFER_SIZE // Counting semaphore for empty slots semaphore full = 0 // Counting semaphore for filled slots integer in = 0 // Producer index integer out = 0 // Consumer index buffer[BUFFER_SIZE] // Shared circular buffer Bounded-Buffer (Producer-Consumer) Problem Producer Process: while true do Prepare the data to be inserted into the produce item buffer. Block if the buffer is full (no empty slots), wait(empty) otherwise decrement empty. Lock to enter the critical section so only wait(mutex) one process can modify the buffer. buffer[in] = item Insert the produced item at index in. Move in index to the next position in = (in + 1) mod BUFFER_SIZE circularly to wrap around. signal(mutex) Release the lock (exit critical section). Increment the count of filled slots to notify signal(full) the consumer. Bounded-Buffer (Producer-Consumer) Problem Consumer Process: while true do Block if the buffer is empty (no full slots), wait(Full) otherwise decrement full. Lock to enter the critical section to safely wait(mutex) access the buffer. item = buffer[out] Remove the item at index out. Move out index circularly for the next out = (out + 1) mod BUFFER_SIZE removal. signal(mutex) Release the lock (exit critical section). Increment empty slots count to notify the signal(empty) producer. consume item Process the consumed item. Producer-Consumer \u2013 Semaphore Solution \u25aa Producer Process Flow: \u25aa Wait (decrement) on empty: Wait for at least one empty slot. \u25aa Wait (lock) on mutex: Enter the critical section. \u25aa Insert an item into the buffer at index in. \u25aa Update in index in circular fashion (e.g. (in + 1) % BUFFER_SIZE). \u25aa Signal (unlock) mutex: Leave critical section. \u25aa Signal (increment) full: Increment count of full slots. Producer-Consumer \u2013 Semaphore Solution \u25aa Consumer Process Flow: \u25aa Wait (decrement) on full: Wait for at least one full slot. \u25aa Wait (lock) on mutex: Enter the critical"
  },
  {
    "id": 132,
    "source": "Semaphores.txt",
    "text": "Process the consumed item. Producer-Consumer \u2013 Semaphore Solution \u25aa Producer Process Flow: \u25aa Wait (decrement) on empty: Wait for at least one empty slot. \u25aa Wait (lock) on mutex: Enter the critical section. \u25aa Insert an item into the buffer at index in. \u25aa Update in index in circular fashion (e.g. (in + 1) % BUFFER_SIZE). \u25aa Signal (unlock) mutex: Leave critical section. \u25aa Signal (increment) full: Increment count of full slots. Producer-Consumer \u2013 Semaphore Solution \u25aa Consumer Process Flow: \u25aa Wait (decrement) on full: Wait for at least one full slot. \u25aa Wait (lock) on mutex: Enter the critical section. \u25aa Remove an item from the buffer at index out. \u25aa Update out index in circular fashion (e.g. (out + 1) % BUFFER_SIZE). \u25aa Signal (unlock) mutex: Leave critical section. \u25aa Signal (increment) empty: Increment count of empty slots. Dining Philosophers Problem \u25aa This models a scenario where multiple philosophers sitting around a circular table need two shared chopsticks (resources) to eat. \u25aa Each philosopher must pick up the two adjacent chopsticks, one at a time, to eat, leading to potential deadlock or starvation if all pick up one chopstick simultaneously and wait indefinitely for the other. \u25aa The problem highlights resource allocation and deadlock prevention techniques. Dining Philosophers Problem \u25aa Imagine 5 philosophers sitting around a circular table. \u25aa Between each pair of philosophers, there is one chopstick (fork). \u25aa Each philosopher alternates between thinking and eating. \u25aa To eat, a philosopher needs both the chopsticks on their left and right. \u25aa Since chopsticks are shared, a philosopher can only pick up a chopstick if it is not already being used by a neighbor. \u25aa After eating, the philosopher puts down both chopsticks and returns to thinking. Dining Philosophers Problem \u25aa What Problems to be Avoided: \u25aa Deadlock: If every philosopher picks up the chopstick on their left simultaneously, then all wait forever for the right chopstick to become free. This circular waiting causes a deadlock, and no one can eat. \u25aa Starvation: Some philosophers may wait indefinitely if others continuously get access to the chopsticks, leading to unfair resource allocation. \u25aa Mutual Exclusion: Ensuring that no two adjacent philosophers eat at the same time so that no chopstick is shared simultaneously is critical. Dining Philosophers Problem semaphore chopstick[5]; // One semaphore per chopstick, initialized to 1 semaphore mutex; // Semaphore to limit number of philosophers at the table initialize all chopstick[i] to 1 initialize mutex to 4 // Allow maximum 4 philosophers to try picking chopsticks Dining Philosophers Problem function philosopher(i): while true: think() Each philosopher spends some time thinking (not competing for chopsticks). Dining Philosophers Problem wait(mutex) // Request to enter - limit philosophers to 4 \u25aa Philosopher requests permission to try eating from the \"dining room\". \u25aa Since the mutex semaphore allows only up to 4 philosophers, this prevents all 5 trying to eat at the same time, breaking the circular wait. Dining Philosophers Problem wait(chopstick[i]) // Pick left chopstick wait(chopstick[(i + 1) % 5])"
  },
  {
    "id": 133,
    "source": "Semaphores.txt",
    "text": "table initialize all chopstick[i] to 1 initialize mutex to 4 // Allow maximum 4 philosophers to try picking chopsticks Dining Philosophers Problem function philosopher(i): while true: think() Each philosopher spends some time thinking (not competing for chopsticks). Dining Philosophers Problem wait(mutex) // Request to enter - limit philosophers to 4 \u25aa Philosopher requests permission to try eating from the \"dining room\". \u25aa Since the mutex semaphore allows only up to 4 philosophers, this prevents all 5 trying to eat at the same time, breaking the circular wait. Dining Philosophers Problem wait(chopstick[i]) // Pick left chopstick wait(chopstick[(i + 1) % 5]) // Pick right chopstick Philosopher tries to pick up the left chopstick by waiting on its semaphore. Philosopher then tries to pick up the right chopstick (using modulo for circular seating). If the chopstick is in use, the philosopher waits until it is free. Dining Philosophers Problem eat() Philosopher eats now that both chopsticks are acquired. signal(chopstick[i]) // Put down left chopstick signal(chopstick[(i + 1) % 5]) // Put down right chopstick Philosopher puts down left and right chopsticks, signaling availability to others. Dining Philosophers Problem signal(mutex) // Philosopher leaves the table - allows another to enter Philosopher signals the mutex semaphore to indicate they have finished trying to eat. This allows another philosopher to enter the dining room and attempt to eat, maintaining the limit of 4. Readers-Writers Problem \u25aa This problem deals with a shared database accessed by two types of processes: \u25aa Readers, which can concurrently read the data without conflict, and \u25aa Writers, which require exclusive access to modify the data. \u25aa The challenge is to ensure that multiple readers can access the data simultaneously, but writers have exclusive access, preventing inconsistent reads or writes. \u25aa Synchronization mechanisms manage these constraints to avoid race conditions and starvation. Readers-Writers Problem \u25aa The Readers-Writers Problem involves multiple processes (or threads) that want to access a shared resource (like a file or database): \u25aa Readers can read the resource simultaneously because reading does not change data. \u25aa Writers need exclusive access \u2014 only one writer at a time and no readers should read while writing, to prevent inconsistent or corrupted data. Readers-Writers Problem \u25aa Concerns Addressed Using Semaphores: \u25aa Mutual exclusion for writers to prevent data corruption. \u25aa Allowing multiple readers to read simultaneously. \u25aa Ensuring writers have exclusive access (no readers or other writers). \u25aa Preventing starvation of readers or writers by managing access fairness. Readers-Writers Problem Initialize semaphores and counter mutex semaphore controls mutual exclusion for Semaphore mutex = 1 incrementing/decrementing readCount safely. write semaphore ensures exclusive access for Semaphore write = 1 writers (and blocks readers when held). readCount tracks how many readers are Integer readCount = 0 currently reading. function reader(): wait(mutex): Enter critical section to update wait(mutex) readCount. readCount++: Increment number of active readCount = readCount + 1 readers. If this is the first reader, wait(write) is called to if readCount == 1 lock out writers (block writing). wait(write) signal(mutex): Exit critical section on"
  },
  {
    "id": 134,
    "source": "Semaphores.txt",
    "text": "readers or writers by managing access fairness. Readers-Writers Problem Initialize semaphores and counter mutex semaphore controls mutual exclusion for Semaphore mutex = 1 incrementing/decrementing readCount safely. write semaphore ensures exclusive access for Semaphore write = 1 writers (and blocks readers when held). readCount tracks how many readers are Integer readCount = 0 currently reading. function reader(): wait(mutex): Enter critical section to update wait(mutex) readCount. readCount++: Increment number of active readCount = readCount + 1 readers. If this is the first reader, wait(write) is called to if readCount == 1 lock out writers (block writing). wait(write) signal(mutex): Exit critical section on signal(mutex) readCount. Readers-Writers Problem // Critical section: reading resource // Reader performs reading (critical section). After reading, wait(mutex) is called to enter wait(mutex) critical section again for updating readCount. Semaphore write = 1 readCount--: Decrement number of active readCount = readCount - 1 readers. If this is the last reader leaving, signal(write) if readCount == 0 releases write semaphore to allow writers to signal(write) proceed. signal(mutex) signal(mutex): Exit critical section. Readers-Writers Problem Writer Process: function writer(): Acquire exclusive access \u2014 no other writer or wait(write) reader can enter critical section. // Critical section: writing resource Write to shared resource (critical section). signal(write) Release exclusive access for others. Readers-Writers Problem - Tracing \u25aa Initial State: \u25aa mutex = 1 (protects readCount) \u25aa write = 1 (exclusive access for writers) \u25aa readCount = 0 \u25aa Step 1: First Reader Enters \u25aa Reader calls wait(mutex) \u2192 decrements mutex from 1 to 0. \u25aa Reader increments readCount from 0 to 1. \u25aa Since readCount == 1, reader calls wait(write) \u2192 decrements write from 1 to 0 (blocks writers). \u25aa Reader calls signal(mutex) \u2192 increments mutex back to 1. \u25aa Reader accesses the resource for reading. \u25aa Semaphore values: \u25aa mutex = 1 \u25aa write = 0 \u25aa readCount = 1 Readers-Writers Problem - Tracing \u25aa Step 2: Second Reader Enters (Concurrent Reading Allowed) \u25aa Reader calls wait(mutex) \u2192 mutex goes 1 \u2192 0. \u25aa Reader increments readCount from 1 to 2. \u25aa Since readCount != 1, no call to wait(write). \u25aa Reader calls signal(mutex) \u2192 increments mutex to 1. \u25aa Reader accesses resource simultaneously with the first reader. \u25aa Semaphore values: \u25aa mutex = 1 \u25aa write = 0 \u25aa readCount = 2 Readers-Writers Problem - Tracing \u25aa Step 3: Writer Attempts to Enter \u25aa Writer tries wait(write). \u25aa Since write is 0, writer blocks waiting for the semaphore. \u25aa Writer is blocked here until all readers finish. \u25aa Step 4: First Reader Leaves \u25aa Reader calls wait(mutex) \u2192 mutex 1 \u2192 0. \u25aa Reader decrements readCount from 2 to 1. \u25aa Since readCount != 0, no signal on write. \u25aa Reader calls signal(mutex) \u2192 mutex 0 \u2192 1. \u25aa Semaphore values: \u25aa mutex = 1 \u25aa write = 0 \u25aa readCount = 1 Readers-Writers Problem - Tracing \u25aa Step 5: Last Reader Leaves \u25aa Reader calls wait(mutex) \u2192 mutex 1 \u2192 0. \u25aa Reader decrements readCount from 1 to 0. \u25aa Since readCount"
  },
  {
    "id": 135,
    "source": "Semaphores.txt",
    "text": "blocks waiting for the semaphore. \u25aa Writer is blocked here until all readers finish. \u25aa Step 4: First Reader Leaves \u25aa Reader calls wait(mutex) \u2192 mutex 1 \u2192 0. \u25aa Reader decrements readCount from 2 to 1. \u25aa Since readCount != 0, no signal on write. \u25aa Reader calls signal(mutex) \u2192 mutex 0 \u2192 1. \u25aa Semaphore values: \u25aa mutex = 1 \u25aa write = 0 \u25aa readCount = 1 Readers-Writers Problem - Tracing \u25aa Step 5: Last Reader Leaves \u25aa Reader calls wait(mutex) \u2192 mutex 1 \u2192 0. \u25aa Reader decrements readCount from 1 to 0. \u25aa Since readCount == 0, reader calls signal(write) \u2192 increments write semaphore from 0 to 1, unblocking one waiting writer. \u25aa Reader calls signal(mutex) \u2192 mutex 0 \u2192 1. \u25aa Semaphore values: \u25aa mutex = 1 \u25aa write = 1 \u25aa readCount = 0 Readers-Writers Problem - Tracing \u25aa Step 6: Writer Proceeds \u25aa Writer now successfully performs wait(write) \u2192 decrements write 1 \u2192 0, entering critical section exclusively. \u25aa Writer writes to resource. \u25aa Semaphore values: \u25aa mutex = 1 \u25aa write = 0 (Writer holds access exclusively) \u25aa readCount = 0 Sleeping Barber Problem \u25aa This problem models a barber shop where one barber serves customers. \u25aa If no customers are present, the barber sleeps; \u25aa when a customer arrives, the barber wakes to give a haircut. \u25aa Customers either wait if there are available waiting chairs or leave if the shop is full. \u25aa Synchronization ensures proper coordination of customer arrivals, waiting, and service without loss or deadlock."
  },
  {
    "id": 136,
    "source": "M6_FS.txt",
    "text": "Virtualization and File System Management Module 6 Dr. Naveenkumar J Associate Professor, PRP- 217 - 4 Module 6 - Virtualization File System - Definition \u2751 A file system is a structure and set of rules that dictates how data is stored, organized, and managed on storage devices like hard drives, SSDs, and USB drives. \u2751 Without a file system, the OS would see a storage device as a single, undifferentiated block of data, making it impossible to distinguish between different files. 02-11-2025 2 Module 6 - Virtualization File System - Definition \u2751A file system acts as an index for all the data on a storage device, allowing users and applications to create, read, update, and delete files in an organized manner. 02-11-2025 3 Module 6 - Virtualization File System - Definition \u2751Files \u2751A file is a named collection of related information, such as a document, program, or image, recorded on storage. File systems define conventions for naming files, including length and character limitations \u2751Directories \u2751Also known as folders, directories are used to group files and other directories. This creates a hierarchical structure, with the \"root\" directory at the top. 02-11-2025 4 Module 6 - Virtualization File System - Definition \u2751Partitions \u2751 Before a file system can be used, a storage device is typically divided into one or more partitions. \u2751 Each partition is a distinct region of storage that the OS manages separately, and each can be formatted with a different file system. \u2751 This separation can improve performance, security, and data integrity. 02-11-2025 5 Module 6 - Virtualization File System - Definition \u2751 Metadata \u2751Along with the actual data in a file, the file system stores metadata. \u2751This includes information such as the file's name, size, creation date, access permissions, and its location within the directory structure. 02-11-2025 6 Module 6 - Virtualization File System Interface \u2751 The file system interface defines how the operating system presents files and directories to the user and how applications can interact with them. \u2751 It abstracts the physical properties of storage devices into a logical storage unit 02-11-2025 7 Module 6 - Virtualization File System \u2013 Access Methods \u2751 Access methods determine how the information within a file can be accessed and read. \u2751 Sequential Access \u2751 This is the most common method. \u2751 Information in the file is processed in order, one record after another. \u2751 A read operation advances a file pointer to the next position, and a write operation appends to the end of the file or overwrites subsequent data. \u2751A sequential access file emulates magnetic tape operation, and generally supports a few operations: \u2751Readnext \u2013 read a record and advance the tape to the next position. \u2751writenext-write a record and advance the tape to the next position. \u2751Rewind \u2751Skip n records \u2013 May or may not be supported. N may be limited to positive numbers or may be limited to +/-1. 02-11-2025 8 Module 6 - Virtualization File System \u2013 Access Methods \u2751 Direct Access Also known as"
  },
  {
    "id": 137,
    "source": "M6_FS.txt",
    "text": "file pointer to the next position, and a write operation appends to the end of the file or overwrites subsequent data. \u2751A sequential access file emulates magnetic tape operation, and generally supports a few operations: \u2751Readnext \u2013 read a record and advance the tape to the next position. \u2751writenext-write a record and advance the tape to the next position. \u2751Rewind \u2751Skip n records \u2013 May or may not be supported. N may be limited to positive numbers or may be limited to +/-1. 02-11-2025 8 Module 6 - Virtualization File System \u2013 Access Methods \u2751 Direct Access Also known as relative access, \u2751 This method allows a program to read or write information from a file in any order, without reading from the beginning. \u2751 The file is viewed as a numbered sequence of blocks or records. This is useful for database applications where data needs to be accessed rapidly and in a non-linear fashion. \u2751 Jump to any record and read that record. Operations supported include: \u2751 read n - read record number n. ( Note an argument is now required. ) \u2751 write n - write record number n. ( Note an argument is now required. ) \u2751 jump to record n - could be 0 or the end of file. \u2751 Query current record - used to return back to this record later. \u2751 Sequential access can be easily emulated using direct access. The inverse is complicated and inefficient. 02-11-2025 9 Module 6 - Virtualization File System \u2013 Access Methods \u2751Other Methods \u2751 More complex access methods can be built on top of direct access. \u2751 One common approach is to create an index for the file. \u2751 The index, like an index in a book, contains pointers to various blocks in the file, allowing data to be found quickly without searching the entire file 02-11-2025 10"
  },
  {
    "id": 138,
    "source": "M6_FS_Dir.txt",
    "text": "Virtualization and File System Management Module 6 Dr. Naveenkumar J Associate Professor, PRP- 217 - 4 Module 6 - Virtualization Directory \u2751 A directory is essentially a container that holds information about a collection of files. \u2751 In more advanced structures, it can also contain other directories. 02-11-2025 2 Module 6 - Virtualization Directory Structures Need Logical \u2022 It allows for the logical grouping of related files. For example, you can create a directory to hold all your work-related documents, keeping Organization them separate from personal files. Efficiency \u2022 By organizing files into groups, it becomes faster and easier to find a specific file when you need it. \u2022 Simple directory structures require every file in the system to have a Convenient unique name, which is impractical with many files or users. More advanced structures, like the two-level or tree structure, solve this by Naming allowing different users or different directories to contain files with the same name. \u2022 A directory structure enables files to be shared across different Sharing locations or between users without making multiple copies. This is achieved by creating links to the original file in other directories 02-11-2025 3 Module 6 - Virtualization Directory Structures Type \u2751 Single-Level Directory \u2751 This is the most straightforward directory structure where all files are stored in a single, common directory. Think of it as one large folder for everything. \u2751 The operating system maintains a single list of all files. When a new file is created, it is added to this directory. \u2751 To access a file, the system searches this one directory. 02-11-2025 4 Module 6 - Virtualization Directory Structures Type \u2751 Single-Level Directory - Merits: \u2751 Simplicity It is the easiest structure to understand and implement. \u2751 Easy Access Finding and accessing files is straightforward since everything is in one place. \u2751 Simple File Operations Tasks like creating, deleting, and renaming files are very easy. 02-11-2025 5 Module 6 - Virtualization Directory Structures Type \u2751 Single-Level Directory \u2013 De-merits: \u2751 Naming Conflicts No two files can have the same name. This becomes a significant problem as the number of files increases or when multiple users are on the system. \u2751 Lack of Organization It is difficult to group related files, making the system cluttered and hard to manage as it grows. \u2751 Security Issues There is no way to restrict access to files, as all files are in a shared directory. 02-11-2025 6 Module 6 - Virtualization Directory Structures Type \u2751 Single-Level Directory 02-11-2025 7 Module 6 - Virtualization Directory Structures Type \u2751 Root Directory \u251c \u2500\u2500 file1.txt \u251c \u2500\u2500 file2.doc \u251c \u2500\u2500 program.exe \u251c \u2500\u2500 data.csv \u2514\u2500\u2500 report.pdf 02-11-2025 8 Module 6 - Virtualization Directory Structures Type \u2751 Two-Level Directory \u2751 To overcome the limitations of the single-level structure, the two- level directory creates a separate directory for each user. \u2751 There is a Master File Directory (MFD) that contains pointers to each user's individual User File Directory (UFD). \u2751 When a user logs in, the"
  },
  {
    "id": 139,
    "source": "M6_FS_Dir.txt",
    "text": "in a shared directory. 02-11-2025 6 Module 6 - Virtualization Directory Structures Type \u2751 Single-Level Directory 02-11-2025 7 Module 6 - Virtualization Directory Structures Type \u2751 Root Directory \u251c \u2500\u2500 file1.txt \u251c \u2500\u2500 file2.doc \u251c \u2500\u2500 program.exe \u251c \u2500\u2500 data.csv \u2514\u2500\u2500 report.pdf 02-11-2025 8 Module 6 - Virtualization Directory Structures Type \u2751 Two-Level Directory \u2751 To overcome the limitations of the single-level structure, the two- level directory creates a separate directory for each user. \u2751 There is a Master File Directory (MFD) that contains pointers to each user's individual User File Directory (UFD). \u2751 When a user logs in, the system accesses their specific UFD. \u2751 Any file operations are then confined to that user's directory. This means users can have files with the same name as other users because they exist in different directories. 02-11-2025 9 Module 6 - Virtualization Directory Structures Type \u2751 Two-Level Directory - Merits \u2751 No Naming Conflicts Since each user has their own directory, naming collisions between users are eliminated. \u2751 Improved Organization Files are organized on a per-user basis, which makes searching and management easier for individual users. \u2751 Enhanced Security Users are isolated from each other, preventing them from accessing each other's files by default. 02-11-2025 10 Module 6 - Virtualization Directory Structures Type \u2751 Two-Level Directory - Demerits \u2751 No Collaboration The isolation between users makes it difficult for them to cooperate on tasks or share files. \u2751 Limited Organization for Users: While it separates users, it doesn't help a single user organize their own files into subgroups. A user with many files will still have them all in one large list. 02-11-2025 11 Module 6 - Virtualization Directory Structures Type \u2751 Two-Level Directory 02-11-2025 12 Module 6 - Virtualization Directory Structures Type Master Directory (MFD) \u251c \u2500\u2500 User1 Directory \u251c \u2502 \u2500\u2500 file1.txt \u251c \u2502 \u2500\u2500 program.exe \u2502 \u2514\u2500\u2500 data.csv \u251c \u2500\u2500 User2 Directory \u251c \u2502 \u2500\u2500 file1.txt (allowed - different directory) \u251c \u2502 \u2500\u2500 report.doc \u2502 \u2514\u2500\u2500 notes.txt \u2514\u2500\u2500 User3 Directory \u251c \u2500\u2500 project.py \u2514\u2500\u2500 results.csv 02-11-2025 13 Module 6 - Virtualization Directory Structures Type \u2751 Tree Directory Structure \u2751 This structure extends the two-level directory into a hierarchy, where directories can contain not only files but also other directories (subdirectories). \u2751 This creates a tree-like organization. \u2751 Each user has a home directory and can create a complex tree of subdirectories to organize their files. \u2751 Files are accessed using a path that specifies the route from the root of the tree down to the file. 02-11-2025 14 Module 6 - Virtualization Directory Structures Type \u2751 Tree Directory Structure - Merits \u2751 Highly Scalable and Organized It is very effective for organizing a large number of files by grouping them into logical subdirectories. \u2751 Flexible Searching Files can be located using either an absolute path (from the root directory) or a relative path (from the current directory), making navigation efficient. \u2751 General and Intuitive: This structure is widely used in modern operating systems and is easy for users"
  },
  {
    "id": 140,
    "source": "M6_FS_Dir.txt",
    "text": "accessed using a path that specifies the route from the root of the tree down to the file. 02-11-2025 14 Module 6 - Virtualization Directory Structures Type \u2751 Tree Directory Structure - Merits \u2751 Highly Scalable and Organized It is very effective for organizing a large number of files by grouping them into logical subdirectories. \u2751 Flexible Searching Files can be located using either an absolute path (from the root directory) or a relative path (from the current directory), making navigation efficient. \u2751 General and Intuitive: This structure is widely used in modern operating systems and is easy for users to understand. 02-11-2025 15 Module 6 - Virtualization Directory Structures Type \u2751 Tree Directory Structure \u2013 Demerits \u2751 No Direct File Sharing In its pure form, this structure does not allow for files or directories to be shared between different branches of the tree, which can lead to file duplication. \u2751 Inefficiency in Access Accessing a file deep within the directory tree can be inefficient as it requires traversing multiple directory levels. 02-11-2025 16 Module 6 - Virtualization Directory Structures Type \u2751 Tree Directory Structure 02-11-2025 17 Module 6 - Virtualization Directory Structures Type \u2751 / (Root) \u251c\u2500\u2500 home \u2502 \u251c\u2500\u2500 user1 \u2502 \u2502 \u251c\u2500\u2500 documents \u2502 \u2502 \u2502 \u251c\u2500\u2500 report.pdf \u2502 \u2502 \u2502 \u2514\u2500\u2500 notes.txt \u2502 \u2502 \u2514\u2500\u2500 projects \u2502 \u2502 \u2514\u2500\u2500 code.py \u2502 \u2514\u2500\u2500 user2 \u2502 \u2514\u2500\u2500 data.csv \u251c\u2500\u2500 usr \u2502 \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 lib \u2514\u2500\u2500 var \u2514\u2500\u2500 log 02-11-2025 18 Module 6 - Virtualization Directory Structures Type \u2751 Acyclic Graph Directory Structure \u2751 This structure is an enhancement of the tree structure that allows for sharing. \u2751 A file or directory can have multiple parent directories, enabling it to appear in different locations without being duplicated. This is achieved using links or pointers. \u2751 When users need to share a file, instead of creating a copy, the system creates a link to the original file in the other user's directory. \u2751 Any changes made to the file are reflected everywhere it is linked. 02-11-2025 19 Module 6 - Virtualization Directory Structures Type \u2751 Acyclic Graph Directory Structure \u2013 Merits \u2751 Enables File Sharing It allows for easy collaboration and sharing of files and directories among users, avoiding redundant copies. \u2751 Efficient Searching The presence of multiple paths to the same file can make searching more flexible 02-11-2025 20 Module 6 - Virtualization Directory Structures Type \u2751 Acyclic Graph Directory Structure \u2013 Demerits \u2751Deletion Complexity: Deleting a file becomes complicated. If a file is deleted, it can leave behind \"dangling pointers\" in the directories that were linked to it. \u2751 The system must have a mechanism to handle this, such as only deleting the file when all links to it are removed. \u2751Increased Complexity: This structure is more complex to manage than a simple tree. 02-11-2025 21 Module 6 - Virtualization Directory Structures Type \u2751 Acyclic Graph Directory Structure 02-11-2025 22 Module 6 - Virtualization Directory Structures Type \u2751 Root \u251c \u2500\u2500 UserA \u251c \u2502 \u2500\u2500 project_file.txt"
  },
  {
    "id": 141,
    "source": "M6_FS_Dir.txt",
    "text": "Acyclic Graph Directory Structure \u2013 Demerits \u2751Deletion Complexity: Deleting a file becomes complicated. If a file is deleted, it can leave behind \"dangling pointers\" in the directories that were linked to it. \u2751 The system must have a mechanism to handle this, such as only deleting the file when all links to it are removed. \u2751Increased Complexity: This structure is more complex to manage than a simple tree. 02-11-2025 21 Module 6 - Virtualization Directory Structures Type \u2751 Acyclic Graph Directory Structure 02-11-2025 22 Module 6 - Virtualization Directory Structures Type \u2751 Root \u251c \u2500\u2500 UserA \u251c \u2502 \u2500\u2500 project_file.txt \u2502 \u2514\u2500\u2500 shared_data \u2192 (link to /UserB/data) \u2514\u2500\u2500 UserB \u251c \u2500\u2500 data \u2502 \u2514\u2500\u2500 dataset.csv \u2514\u2500\u2500 project_file.txt \u2192 (link to /UserA/project_file.txt) 02-11-2025 23 Module 6 - Virtualization Directory Structures Type \u2751 General Graph Directory Structure \u2751 This is the most flexible but also the most complex directory structure. It is like the acyclic-graph but allows for cycles. This means a directory can contain a link to one of its parent directories or even to itself. \u2751 The system allows the creation of links without the restriction of avoiding cycles. This offers maximum flexibility for linking files and directories. 02-11-2025 24 Module 6 - Virtualization Directory Structures Type \u2751 General Graph Directory Structure - Merits \u2751 Maximum Flexibility It provides the greatest freedom in how files and directories can be interlinked. 02-11-2025 25 Module 6 - Virtualization Directory Structures Type \u2751 General Graph Directory Structure - Demerits \u2751 Risk of Infinite Loops The presence of cycles can cause algorithms that traverse the directory (like search or file-cleanup utilities) to enter infinite loops. The system must implement cycle detection or garbage collection to manage this. \u2751 High Complexity The complexity of ensuring correctness and preventing problems makes this structure difficult to implement and manage. 02-11-2025 26 Module 6 - Virtualization File System Implementation \u2751 File-system needs to maintain on-disk and in-memory structures \u2751 on-disk for data storage, \u2751 in-memory for data access \u2751 On-disk structure has several control blocks \u2751 Boot control block contains info to boot OS from that volume \u2751 only needed if volume contains OS image, usually first block of volume \u2751 Volume control block (e.g., superblock) contains volume details \u2751 total # of blocks, # of free blocks, block size, free block pointers or array \u2751 Directory structure organizes the directories and files \u2751 file names and layout \u2751 per-file file control block contains many details about the file \u2751 inode number, permissions, size, dates 02-11-2025 27 Module 6 - Virtualization File System Implementation \u2751 File Control Block 02-11-2025 28 Module 6 - Virtualization File System Implementation 02-11-2025 29 Module 6 - Virtualization Directory Implementations 02-11-2025 30 Module 6 - Virtualization Directory Implementations Directory Implementation Linear List Hash how the logical structure of a directory (as a list of files and subdirectories) is physically stored. 02-11-2025 31 Module 6 - Virtualization Directory Implementations \u2751 Linear List \u2751 This is the simplest way to implement a directory. \u2751 It"
  },
  {
    "id": 142,
    "source": "M6_FS_Dir.txt",
    "text": "\u2751 per-file file control block contains many details about the file \u2751 inode number, permissions, size, dates 02-11-2025 27 Module 6 - Virtualization File System Implementation \u2751 File Control Block 02-11-2025 28 Module 6 - Virtualization File System Implementation 02-11-2025 29 Module 6 - Virtualization Directory Implementations 02-11-2025 30 Module 6 - Virtualization Directory Implementations Directory Implementation Linear List Hash how the logical structure of a directory (as a list of files and subdirectories) is physically stored. 02-11-2025 31 Module 6 - Virtualization Directory Implementations \u2751 Linear List \u2751 This is the simplest way to implement a directory. \u2751 It involves creating a list of file names, with each entry in the list pointing to the file's data blocks on the disk. \u2751 When a new file is created, it is added to the end of the list. \u2751 To find a file, the system performs a linear search, checking each entry in the directory one by one. \u2751 To delete a file, the system searches for it and then releases its allocated space. 02-11-2025 32 Module 6 - Virtualization Directory Implementations 02-11-2025 33 Module 6 - Virtualization Directory Implementations \u2751 Merits \u2751 It is simple to program and understand. \u2751Demerits \u2751 It is time-consuming to execute operations because it requires a linear search. This becomes very slow as the directory grows. \u2751 To improve performance, the list can be kept sorted alphabetically, which allows for a binary search, but this makes file creation and deletion more complex. 02-11-2025 34 Module 6 - Virtualization Directory Implementations \u2751 Hash Table \u2751 This method uses a hash table in conjunction with a linear list to speed up file searching. \u2751 A hash function takes the file name and computes a hash value. \u2751 This value is used as an index into the hash table, which then points to the file's entry in the linear list. \u2751 This avoids a lengthy linear search. 02-11-2025 35 Module 6 - Virtualization Directory Implementations 02-11-2025 36 Module 6 - Virtualization Directory Implementations \u2751 Merits \u2751 It significantly decreases directory search time, making it much faster to locate a file. \u2751Demerits \u2751 Collisions It's possible for two different file names to hash to the same location. This must be managed using collision resolution techniques, such as chaining. \u2751 Fixed Size: Hash tables are typically of a fixed size. If the number of files grows too large, the performance degrades, and the table may need to be resized and reorganized, which is a complex operation. 02-11-2025 37 Module 6 - Virtualization File Allocation Methods File Allocation Methods Contiguous File Linked Indexed Allocation File allocation methods determine how an operating system allocates disk blocks for files. The three main methods are contiguous, linked, and indexed allocation. 02-11-2025 38 Module 6 - Virtualization File Allocation Methods \u2751 Contiguous Allocation \u2751 In this method, each file occupies a set of adjacent, contiguous blocks on the disk. \u2751 The directory entry for a file needs to store only two pieces of information:"
  },
  {
    "id": 143,
    "source": "M6_FS_Dir.txt",
    "text": "performance degrades, and the table may need to be resized and reorganized, which is a complex operation. 02-11-2025 37 Module 6 - Virtualization File Allocation Methods File Allocation Methods Contiguous File Linked Indexed Allocation File allocation methods determine how an operating system allocates disk blocks for files. The three main methods are contiguous, linked, and indexed allocation. 02-11-2025 38 Module 6 - Virtualization File Allocation Methods \u2751 Contiguous Allocation \u2751 In this method, each file occupies a set of adjacent, contiguous blocks on the disk. \u2751 The directory entry for a file needs to store only two pieces of information: the starting block address and the total length of the file (in blocks). \u2751 Accessing any part of the file is straightforward because the system can calculate the exact block address from the starting address and the logical position within the file. 02-11-2025 39 Module 6 - Virtualization File Allocation Methods \u2751 Contiguous Allocation 02-11-2025 40 Module 6 - Virtualization File Allocation Methods \u2751Merits \u2751 It offers excellent performance for both sequential and random access because the file's blocks are physically close together, minimizing disk head movement. \u2751 It is simple to implement. \u2751Demerits: \u2751 External Fragmentation: Over time, as files are created and deleted, the free space on the disk becomes broken into small, non-contiguous chunks. This can make it difficult to find a large enough contiguous space for a new file, even if there is enough total free space. \u2751 File Growth: It is difficult for files to grow, as they may run into another file immediately after their allocated space. 02-11-2025 41 Module 6 - Virtualization File Allocation Methods \u2751Linked Allocation \u2751 This method stores each file as a linked list of disk blocks, which can be scattered anywhere on the disk. \u2751 Each block contains not only the file's data but also a pointer to the next block in the file. \u2751 The directory entry only needs to store the starting block address. \u2751 The file is traversed by following the pointers from one block to the next. 02-11-2025 42 Module 6 - Virtualization File Allocation Methods \u2751 Linked Allocation 02-11-2025 43 Module 6 - Virtualization File Allocation Methods \u2751 Linked Allocation \u2751Merits: \u2751It eliminates external fragmentation, as any free block can be used. \u2751Files can grow easily and dynamically as long as there are free blocks available. \u2751Demerits: \u2751It is only efficient for sequential access. Random access is very slow because, to get to a specific block, the system must traverse the chain from the beginning. \u2751 It has a space overhead because each block must reserve space for the pointer. \u2751 It is not very reliable. If a pointer is damaged or lost, the rest of the file becomes inaccessible. \u2751 FAT (File Allocation Table): A variation of this method, used by MS-DOS and early Windows, takes the pointers from all blocks and stores them in a separate table at the beginning of the disk, called the File Allocation Table (FAT). This improves random"
  },
  {
    "id": 144,
    "source": "M6_FS_Dir.txt",
    "text": "Random access is very slow because, to get to a specific block, the system must traverse the chain from the beginning. \u2751 It has a space overhead because each block must reserve space for the pointer. \u2751 It is not very reliable. If a pointer is damaged or lost, the rest of the file becomes inaccessible. \u2751 FAT (File Allocation Table): A variation of this method, used by MS-DOS and early Windows, takes the pointers from all blocks and stores them in a separate table at the beginning of the disk, called the File Allocation Table (FAT). This improves random access times. 02-11-2025 44 Module 6 - Virtualization File Allocation Methods 02-11-2025 45 Module 6 - Virtualization File Allocation Methods \u2751 Indexed Allocation \u2751 This method combines the benefits of the previous two by bringing all the pointers for a file's blocks together into a single location called an index block. \u2751 Each file has its own index block, which is an array of disk block addresses. \u2751 The ith entry in the index block points to the ith block of the file. \u2751 The directory entry contains the address of this index block. 02-11-2025 46 Module 6 - Virtualization File Allocation Methods 02-11-2025 47 Module 6 - Virtualization File Allocation Methods \u2022 Merits: \u2022 It supports direct, random access to any block of the file without performance loss. \u2022 It solves the problem of external fragmentation. \u2022 Demerits: \u2022 It has a space overhead due to the need for the index block. For small files, this can be wasteful. \u2022 For very large files, a single index block may not be enough to hold all the pointers. This is handled by creating multiple, linked index blocks or a multi-level index, which adds complexity. 02-11-2025 48 Module 6 - Virtualization File Allocation Methods \u2751Contiguous Allocation \u2022 Best for: Both sequential and random access. \u2022 Why: All the file\u2019s blocks are together, so reading through (sequential access) or jumping to a specific part (random access) is fast\u2014just calculate the block's position and go straight to it. \u2022 Drawback: Hard for files to grow or shrink; may run into fragmentation. 02-11-2025 49 Module 6 - Virtualization File Allocation Methods \u2751Linked Allocation \u2022 Best for: Sequential access. \u2022 Why: Each file block has a pointer to the next, so you just follow the chain for sequential reads/writes. \u2022 Drawback: For random access, you have to start at the beginning and step through the chain\u2014very slow. 02-11-2025 50 Module 6 - Virtualization File Allocation Methods \u2751Indexed Allocation (including Combined) \u2751 Best for: Both sequential and random access, but with more overhead. \u2751 Why: The file has an index block (or multiple for big files) listing addresses of all its blocks. For random access, get the required block\u2019s address from the index and read it. \u2751 Drawback: Sometimes you need to read two index blocks before you can read the data block\u2014adds complexity and overhead. 02-11-2025 51"
  },
  {
    "id": 145,
    "source": "M6_FS_Dir.txt",
    "text": "have to start at the beginning and step through the chain\u2014very slow. 02-11-2025 50 Module 6 - Virtualization File Allocation Methods \u2751Indexed Allocation (including Combined) \u2751 Best for: Both sequential and random access, but with more overhead. \u2751 Why: The file has an index block (or multiple for big files) listing addresses of all its blocks. For random access, get the required block\u2019s address from the index and read it. \u2751 Drawback: Sometimes you need to read two index blocks before you can read the data block\u2014adds complexity and overhead. 02-11-2025 51"
  },
  {
    "id": 146,
    "source": "Tutorial 1.txt",
    "text": "Q1. Processes P1, P2, P3 and P4 arrive in that order at times 0, 1, 2, and 8 milliseconds respectively, and have execution times of 10, 13, 6, and 9 milliseconds respectively. Shortest Remaining Time First (SRTF) algorithm is used as the CPU scheduling policy. Ignore context switching times. Calculate the average turnaround time, Average Waiting time and Average Response time for four processes in milliseconds? Q2. A computer has two processors, M1 and M2. Four processes P1, P2, P3, P4 with CPU bursts of 20, 16, 25, and 10 milliseconds, respectively, arrive at the same time and these are the only processes in the system. The scheduler uses non-pre-emptive priority scheduling, with priorities decided as follows: \u25aa M1 uses priority of execution for the processes as, P1>P3>P2>P4, i.e., P1 and P4 have highest and lowest priorities, respectively. \u25aa M2 uses priority of execution for the processes as, P2>P3>P4>P1,i.e., P2 and P1 have highest and lowest priorities, respectively. A process Pi is scheduled to a processor Mk, if the processor is free and no other process Pj is waiting with higher priority. At any given point of time, a process can be allocated to any one of the free processors without violating the execution priority rules. Ignore the context switch time. What will be the average waiting time of the processes in milliseconds? Q3. You are an operating systems designer tasked with analyzing the performance of two different CPU scheduling algorithms for a new system. You have been given a set of five processes with their arrival times and required CPU burst times. Process Details: Process Arrival Time (ms) CPU Burst Time (ms) Priority P1 0 12 3 P2 2 7 1 P3 3 5 2 P4 6 8 4 P5 8 4 1 Task: Analyse the performance of the following two scheduling algorithms for the set of processes described above. Q4. Four processes (P1, P2, P3, P4) arrive at time t=0 with the characteristics shown below. Their total CPU burst time is the sum of their Data Processing Time and Computational Time Process Data Processing Computational Ops Ops per ms Priority Size Rate (KB/ms) (millions) (millions/ms) (KB) P1 120 10 30 5 2 P2 90 15 24 8 1 P3 100 10 25 5 3 P4 80 20 12 6 1 Scheduling Rule: The system uses a non-preemptive Shortest Job First (SJF) algorithm. If a tie occurs for the shortest burst time, the tie is resolved by using pre-emptive Priority scheduling (a lower number means higher priority) only among the tied processes. Calculate the average turnaround time and average waiting time for these processes. Assume context switching time is negligible."
  },
  {
    "id": 147,
    "source": "Tutorial 1.txt",
    "text": "shortest burst time, the tie is resolved by using pre-emptive Priority scheduling (a lower number means higher priority) only among the tied processes. Calculate the average turnaround time and average waiting time for these processes. Assume context switching time is negligible."
  }
]